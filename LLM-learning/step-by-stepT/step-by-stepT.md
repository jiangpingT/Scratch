```python
import os
import requests
import pandas as pd
import matplotlib.pyplot as plt
import math
import tiktoken
import torch
import torch.nn as nn
```


```python
# 0，设置超参数Setup Hyperparameters
batch_size = 4  # 每个 batch 中包含的数据样本数量
context_length = 16  # Length of the token chunk each batch
d_model = 64  # The vector size of the token embeddings
num_layers = 8  # Number of transformer blocks
num_heads = 4  # Number of heads in Multi-head attention # 我们的代码中通过 d_model / num_heads = 来获取 head_size
learning_rate = 1e-3  # 学习率
dropout = 0.1 # Dropout rate
max_iters = 500  # Total of training iterations # 控制训练循环的终止条件 #在实际训练中，除了使用 max_iters 作为硬终止条件外，还可能结合早停法（Early Stopping）等技术。早停法会在验证集上的性能不再提升时提前终止训练，而 max_iters 可以作为一个兜底的终止条件，防止训练过程无限期地进行下去
eval_interval = 50  # How often to evaluate the model # 当训练迭代次数达到 eval_interval 的整数倍时，会暂停训练，使用验证集数据对模型进行评估，计算相关评估指标，以监控模型的性能
eval_iters = 20  # How many iterations to average the loss over when evaluating the model # 在每次评估模型时，并不是只计算当前迭代的损失值，而是计算最近 eval_iters 次迭代的损失值的平均值，以此来更稳定、准确地反映模型在一段时间内的性能表现。例如，当 eval_interval 为 50 时，在第 50 次迭代进行模型评估，此时会计算从第 31 次（50 - 20 + 1）迭代到第 50 次迭代这 20 次的损失平均值作为本次评估的损失指标
device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Instead of using the cpu, we'll use the GPU if it's available.

TORCH_SEED = 1337 # 设置随机数种子，随机数种子用于初始化随机数生成器，以便在不同的运行中能够得到相同的随机数序列
torch.manual_seed(TORCH_SEED) # 通过设置固定的随机数种子，可以确保在多次运行代码时，随机数生成的结果是相同的
```




    <torch._C.Generator at 0x33032d430>




```python
# 1，输入准备阶段：准备数据集Prepare the Dataset 
# # download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt
if not os.path.exists('../data/sales_textbook.txt'):
    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'
    with open('sales_textbook.txt', 'w') as f:
        f.write(requests.get(url).text)

with open('../data/sales_textbook.txt', 'r', encoding='utf-8') as f:
    text = f.read()
    
```


```python
# 2，输入准备阶段：分词化Tokenization
# Using TikToken to tokenize the source text
# 使用 tiktoken 库对数据集进行标记化。该库是一种快速且轻量级的分词器，可用于将文本分词化为分词
encoding = tiktoken.get_encoding("cl100k_base") #tiktoken 是 OpenAI 开发的一个用于处理文本和标记（token）之间转换的库，它可以将文本编码为模型能够处理的标记序列，也可以将标记序列解码回原始文本
encoding_text = encoding.encode(text)

# Illustration purpose
print(encoding.encode('Chapter 1: Building Rapport and Capturing'))
print(encoding.decode([26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711])) # "Rapport" is tokenized as two tokens: "Rap"[23097] and "port"[403]

print(f"the first 10 tokens of encoding text: {encoding_text[:10]}")

tokenized_text = torch.tensor(encoding_text, dtype=torch.long, device=device) # Convert tokens into a tensor
max_token_value = tokenized_text.max().item() # the maximum index value in our vocabulary
print(f"the first 10 items in the tensor tokenized_text: {tokenized_text[:10]}")

print(f"Tokenized text size: {len(tokenized_text)}")
print(f"The maximum value in the tensor tokenized_text is: {max_token_value}")
```

    [26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711]
    Chapter 1: Building Rapport and Capturing
    the first 10 tokens of encoding text: [26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711]
    the first 10 items in the tensor tokenized_text: tensor([26072,   220,    16,    25, 17283, 23097,   403,   323, 17013,  1711])
    Tokenized text size: 77919
    The maximum value in the tensor tokenized_text is: 100069



```python
# 3，输入准备阶段：分割训练集和验证集Split train and validation
# 将数据集拆分为训练集和验证集。训练集将用于训练模型，验证集将用于评估模型的性能。
split_idx = int(len(tokenized_text) * 0.9)
train_data = tokenized_text[:split_idx]
val_data = tokenized_text[split_idx:]
```


```python
# 4，输入准备阶段：准备训练数据Prepare data for training batch
# 用于训练和损失计算，训练集将用于训练模型，验证集将用于评估模型的性能
# x_batch 和 y_batch 用于准备模型训练所需的数据批次，其中 y_batch 主要用于模型训练时计算损失，x_batch 主要用于模型训练时计算梯度。
data = train_data
idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,)) # 生成一个包含 batch_size 个随机整数的一维张量 idxs，这些随机整数的范围在 0 到 len(data) - context_length - 1 之间。这些索引通常用于从数据集中随机采样出 batch_size 个样本，每个样本的长度为 context_length，以便用于模型的训练或其他操作。例如，假设 len(data) = 100，context_length = 10，batch_size = 8，那么 idxs 将会是一个包含 8 个在 0 到 89 之间（100 - 10 - 1 = 89）的随机整数的一维张量，如 tensor([12, 35, 6, 47, 23, 78, 19, 56])
print(f"all items in the tensor idxs: {idxs[:]}")

x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs]) # 将 idxs 中的每个索引 idx 作为起点，从数据集中采样出长度为 context_length 的子序列，并将其堆叠成一个二维张量 x_batch。这个张量的大小为 [batch_size, context_length]，表示每个样本的上下文序列。例如，如果 idxs = tensor([12, 35, 6, 47])，那么 x_batch 将会是一个大小为 [4, 10] 的张量，表示每个样本的上下文序列长度为 10，且每个样本的上下文序列分别为 data[12:22], data[35:45], data[6:16], data[47:57]。
print(f"the first 10 items in the tensor x_batch: {x_batch[:10]}")

y_batch = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]) # 在语言模型等任务中，模型的目标是根据输入的一段文本预测下一个词。x_batch 作为模型的输入，包含了当前的文本序列；而 y_batch 包含了对应的下一个位置的文本序列，也就是模型需要预测的目标。在训练过程中，模型会根据 x_batch 做出预测，然后通过计算预测结果与 y_batch 之间的差异（如交叉熵损失）来调整模型的参数，使得模型的预测结果更接近真实的 y_batch。假设 data 是一个包含文本标记的列表，context_length 为 5，idxs 中有一个索引值 idx 为 10。那么 x_batch 对应的一段数据可能是 data[10:15]，而 y_batch 对应的则是 data[11:16] ，y_batch 中每个位置的元素就是模型在看到 x_batch 对应位置元素时应该预测的目标。

print(f"the first 10 items in the tensor y_batch: {y_batch[:10]}")

print(x_batch.shape, y_batch.shape)
```

    all items in the tensor idxs: tensor([35754, 55550, 63572,  1447])
    the first 10 items in the tensor x_batch: tensor([[  279,  6763,  1920,    13,   578,  5845,   311, 13750, 19570,   279,
               907,   323,  7720,   315,  1057,  3956],
            [ 3495, 14955,    11,   477,  5064, 23146,   430,  9788,   279, 66732,
               315,   701, 10209,    13,  3296, 32644],
            [38769, 10742,    11, 20958,   264,  6928, 19451,    11, 11125, 64784,
                11,   323, 56501, 54111,   439,  6975],
            [43496,   872,  8830,   719,  1101,  3727,   279,  6130,  2733,  6755,
               323, 16365,   627, 29831, 19682,  5900]])
    the first 10 items in the tensor y_batch: tensor([[ 6763,  1920,    13,   578,  5845,   311, 13750, 19570,   279,   907,
               323,  7720,   315,  1057,  3956,   477],
            [14955,    11,   477,  5064, 23146,   430,  9788,   279, 66732,   315,
               701, 10209,    13,  3296, 32644,  1521],
            [10742,    11, 20958,   264,  6928, 19451,    11, 11125, 64784,    11,
               323, 56501, 54111,   439,  6975, 10708],
            [  872,  8830,   719,  1101,  3727,   279,  6130,  2733,  6755,   323,
             16365,   627, 29831, 19682,  5900,  1450]])
    torch.Size([4, 16]) torch.Size([4, 16])



```python
# 5，输入准备阶段：将训练数据的张量表示转换为向量表示Convert the tensor representation of the training data into a vector representation
# 定义词嵌入查找表（Token Embedding Look-up Table）是 Transformer 模型中用于将输入文本中的标记（tokens）转换为向量表示的查找表。它将每个标记映射为一个固定大小的向量，这个向量可以作为模型的输入，用于计算注意力机制、前馈神经网络等操作。
# 通过将 x_batch 和 y_batch 作为索引输入到查找表中，可以得到对应的词嵌入向量。 在深度学习模型（如 Transformer）中，将张量变成向量进行计算主要有以下几个原因：
# 适应模型结构：Transformer 等模型中的后续层（如多头注意力机制、前馈神经网络等）通常期望输入具有特定的形状和维度。将输入数据转换为向量形式，能使数据的维度与模型各层的输入要求相匹配，便于模型进行有效的计算和特征提取。例如多头注意力机制中，需要对输入进行线性变换、维度拆分等操作，合适的向量表示能满足这些计算的需求。
# 编码语义信息：通过嵌入层（如这里的token_embedding_lookup_table）将标记（token）转换为向量，能够将离散的标记映射到连续的向量空间中。在这个向量空间里，语义相似的标记在距离上会更接近，从而使模型能够更好地捕捉和利用数据中的语义信息。比如在文本任务中，不同单词的嵌入向量可以反映它们之间的语义关联，这有助于模型理解文本的含义。
# 提高计算效率：向量形式的数据在现代硬件（如 GPU）上能更高效地进行并行计算。GPU 擅长处理大规模的矩阵运算，将张量转换为向量后，模型可以充分利用 GPU 的并行计算能力，加速模型的训练和推理过程，提升计算效率
token_embedding_lookup_table = nn.Embedding(max_token_value+1, d_model)
print("Token Embedding Look-up table: ", token_embedding_lookup_table)

x_batch_embedding = token_embedding_lookup_table(x_batch.data) # [4, 16, 64] [batch_size, context_length, d_model]
y_batch_embedding = token_embedding_lookup_table(y_batch.data)

print(x_batch_embedding.shape, y_batch_embedding.shape)
print(pd.DataFrame(x_batch_embedding[0].detach().cpu().numpy()))
print(pd.DataFrame(y_batch_embedding[0].detach().cpu().numpy()))

```

    Token Embedding Look-up table:  Embedding(100070, 64)
    torch.Size([4, 16, 64]) torch.Size([4, 16, 64])
              0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
    0   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721
    1   0.566486 -1.102276  1.712332 -0.354509  0.550577 -0.707943 -0.743899  0.757765  0.018193  1.392380  ...  0.873562  1.226714  0.794431  0.598629  0.884421  0.032520  1.353617  0.059697  1.172510  0.527427
    2  -0.426478  1.717362 -0.343810 -0.917124 -0.273610  0.695366 -0.849842 -1.301135 -0.162554 -0.252811  ... -0.934960  1.145729 -1.914149 -0.447346  0.597272  1.673483 -1.969475  0.397835 -0.438475 -0.562923
    3   0.709939  1.369311 -0.707588  1.538689 -2.110915  0.441344 -0.005807  0.171597 -0.296632  0.207320  ...  0.071533 -0.735549  0.069967 -2.744750  1.087368 -0.997812  0.714993 -1.357311  1.603957  0.920290
    4  -1.963246  0.298927  0.131364  0.082995  0.153765 -0.821641 -1.220109 -1.088038  1.535371  1.829628  ...  0.531913 -0.567854 -2.390946 -0.086596  0.066017  0.655226  0.624369 -0.763375 -0.692774 -0.007724
    5   0.811149  0.435133  1.131030  0.816734 -1.013971 -0.052429 -0.527541 -0.710573 -0.163887 -1.343154  ... -0.046995 -1.201051 -0.927833  0.322523  0.586139  0.108184 -1.653296  1.918813  0.941642  0.584331
    6   0.317549  2.106441 -0.092210  0.636316 -0.912476 -1.975633 -0.068806  0.201157  0.333519  0.151939  ...  0.200116  0.051824  1.304806  0.517675  0.049345  0.044632  1.346794 -0.321390 -0.478787 -0.166920
    7  -2.025703  1.256391 -0.318619  1.432163  1.644837 -1.910154 -1.001209 -0.976038  1.502204  0.841974  ... -2.339570  0.190784 -0.055200  2.281739 -0.417175 -0.801704 -1.393716  1.863095 -0.393567 -0.131746
    8   2.906978  0.092020 -0.785242  0.609121 -0.790180 -0.026004 -1.156866  0.398454 -0.455395 -0.251288  ...  3.363073 -0.796739  1.757077  1.526690 -0.654219  1.660685  0.965431  0.618787  1.662946  1.768386
    9   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721
    10  0.370826  0.272521 -1.915345  0.032303  0.017929 -1.451397 -1.632907 -1.165714  2.069243  0.550959  ... -1.475737  0.216844  0.722587  1.340471 -0.709199  1.097342  0.231425  0.104482 -1.583871  1.481635
    11 -0.030617  0.305096 -0.199683  1.021860 -0.044290  1.731110 -1.489611  0.928623 -0.587382 -1.500504  ...  0.303483 -0.402740  0.042445 -0.872444  1.356491  1.417310 -1.436294 -0.745807 -0.242414 -0.457403
    12 -0.927026  0.017112 -0.874911  1.184731  0.727968 -0.564862 -1.850788  0.852115 -0.866277 -1.235445  ... -0.060539  0.198801  0.542944 -0.405060  0.361515  1.217013 -0.568322  1.032800  0.174048  1.400139
    13  0.625287 -1.128724 -2.476117  0.281622  0.046729  0.833729 -0.771643 -0.884734  0.910253  0.600033  ... -2.394152 -0.798159 -0.969718  0.399133 -0.768250  0.607608 -1.891320  1.642382 -0.156069 -0.930427
    14 -0.998070  1.024137  0.206201 -2.287179 -0.003692  0.207054  0.138208 -0.992636  0.491568  0.573276  ...  0.146838  0.273048  0.306777 -1.205792 -0.802286 -0.817414 -0.991609  0.227736  0.581187  0.285422
    15  0.512090 -0.403476 -1.283820 -1.035253  0.385313  0.251786  1.729429  1.389589  0.326549 -0.448694  ...  1.361415  1.225376 -0.196476 -1.841317 -0.838676 -0.372250  0.113748 -0.636791  0.750731  0.664510
    
    [16 rows x 64 columns]
              0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
    0   0.566486 -1.102276  1.712332 -0.354509  0.550577 -0.707943 -0.743899  0.757765  0.018193  1.392380  ...  0.873562  1.226714  0.794431  0.598629  0.884421  0.032520  1.353617  0.059697  1.172510  0.527427
    1  -0.426478  1.717362 -0.343810 -0.917124 -0.273610  0.695366 -0.849842 -1.301135 -0.162554 -0.252811  ... -0.934960  1.145729 -1.914149 -0.447346  0.597272  1.673483 -1.969475  0.397835 -0.438475 -0.562923
    2   0.709939  1.369311 -0.707588  1.538689 -2.110915  0.441344 -0.005807  0.171597 -0.296632  0.207320  ...  0.071533 -0.735549  0.069967 -2.744750  1.087368 -0.997812  0.714993 -1.357311  1.603957  0.920290
    3  -1.963246  0.298927  0.131364  0.082995  0.153765 -0.821641 -1.220109 -1.088038  1.535371  1.829628  ...  0.531913 -0.567854 -2.390946 -0.086596  0.066017  0.655226  0.624369 -0.763375 -0.692774 -0.007724
    4   0.811149  0.435133  1.131030  0.816734 -1.013971 -0.052429 -0.527541 -0.710573 -0.163887 -1.343154  ... -0.046995 -1.201051 -0.927833  0.322523  0.586139  0.108184 -1.653296  1.918813  0.941642  0.584331
    5   0.317549  2.106441 -0.092210  0.636316 -0.912476 -1.975633 -0.068806  0.201157  0.333519  0.151939  ...  0.200116  0.051824  1.304806  0.517675  0.049345  0.044632  1.346794 -0.321390 -0.478787 -0.166920
    6  -2.025703  1.256391 -0.318619  1.432163  1.644837 -1.910154 -1.001209 -0.976038  1.502204  0.841974  ... -2.339570  0.190784 -0.055200  2.281739 -0.417175 -0.801704 -1.393716  1.863095 -0.393567 -0.131746
    7   2.906978  0.092020 -0.785242  0.609121 -0.790180 -0.026004 -1.156866  0.398454 -0.455395 -0.251288  ...  3.363073 -0.796739  1.757077  1.526690 -0.654219  1.660685  0.965431  0.618787  1.662946  1.768386
    8   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721
    9   0.370826  0.272521 -1.915345  0.032303  0.017929 -1.451397 -1.632907 -1.165714  2.069243  0.550959  ... -1.475737  0.216844  0.722587  1.340471 -0.709199  1.097342  0.231425  0.104482 -1.583871  1.481635
    10 -0.030617  0.305096 -0.199683  1.021860 -0.044290  1.731110 -1.489611  0.928623 -0.587382 -1.500504  ...  0.303483 -0.402740  0.042445 -0.872444  1.356491  1.417310 -1.436294 -0.745807 -0.242414 -0.457403
    11 -0.927026  0.017112 -0.874911  1.184731  0.727968 -0.564862 -1.850788  0.852115 -0.866277 -1.235445  ... -0.060539  0.198801  0.542944 -0.405060  0.361515  1.217013 -0.568322  1.032800  0.174048  1.400139
    12  0.625287 -1.128724 -2.476117  0.281622  0.046729  0.833729 -0.771643 -0.884734  0.910253  0.600033  ... -2.394152 -0.798159 -0.969718  0.399133 -0.768250  0.607608 -1.891320  1.642382 -0.156069 -0.930427
    13 -0.998070  1.024137  0.206201 -2.287179 -0.003692  0.207054  0.138208 -0.992636  0.491568  0.573276  ...  0.146838  0.273048  0.306777 -1.205792 -0.802286 -0.817414 -0.991609  0.227736  0.581187  0.285422
    14  0.512090 -0.403476 -1.283820 -1.035253  0.385313  0.251786  1.729429  1.389589  0.326549 -0.448694  ...  1.361415  1.225376 -0.196476 -1.841317 -0.838676 -0.372250  0.113748 -0.636791  0.750731  0.664510
    15  0.648122 -0.186573  0.352738 -1.825604  1.270732 -0.969805  0.531069  0.350651 -0.060166  1.503366  ... -0.430661 -0.508977 -0.602847  0.668671  0.910364 -0.553529 -0.558548  1.340093  1.304710  1.511892
    
    [16 rows x 64 columns]



```python
# 6，输入准备阶段：应用位置嵌入Apply Positional Embedding
# 如最初的 “Attention is All You Need” 论文中所述，我们将使用 sine 和 cosine 生成位置嵌入表，然后将这些位置信息添加到输入嵌入标记中。
# 总结位置编码解决的问题：我们希望每个单词都带有一些关于它在句子中的位置的信息。我们希望模型将看起来彼此靠近的单词视为 “close”，将距离较远的单词视为 “distant”。我们希望位置编码表示模型可以学习的模式。
# 这个位置编码矩阵只创建一次，并为每个输入序列重复使用。
# Define Position Encoding look-up table
position_encoding_lookup_table = torch.zeros(context_length, d_model)
position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)
position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)
position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) #add batch dimension

print("Position Encoding Look-up Table: ", position_encoding_lookup_table.shape) # [4, 16, 64] [batch_size, context_length, d_model]
print(pd.DataFrame(position_encoding_lookup_table[0].detach().cpu().numpy()))
```

    Position Encoding Look-up Table:  torch.Size([4, 16, 64])
              0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
    0   0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  ...  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000
    1   0.841471  0.540302  0.681561  0.731761  0.533168  0.846009  0.409309  0.912396  0.310984  0.950415  ...  0.000422  1.000000  0.000316  1.000000  0.000237  1.000000  0.000178  1.000000  0.000133  1.000000
    2   0.909297 -0.416147  0.997480  0.070948  0.902131  0.431463  0.746903  0.664932  0.591127  0.806578  ...  0.000843  1.000000  0.000632  1.000000  0.000474  1.000000  0.000356  1.000000  0.000267  1.000000
    3   0.141120 -0.989992  0.778273 -0.627927  0.993253 -0.115966  0.953635  0.300967  0.812649  0.582754  ...  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000  0.000400  1.000000
    4  -0.756802 -0.653644  0.141539 -0.989933  0.778472 -0.627680  0.993281 -0.115730  0.953581  0.301137  ...  0.001687  0.999999  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000
    5  -0.958924  0.283662 -0.571127 -0.820862  0.323935 -0.946079  0.858896 -0.512150  0.999947 -0.010342  ...  0.002108  0.999998  0.001581  0.999999  0.001186  0.999999  0.000889  1.000000  0.000667  1.000000
    6  -0.279415  0.960170 -0.977396 -0.211416 -0.230368 -0.973104  0.574026 -0.818837  0.947148 -0.320796  ...  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999  0.000800  1.000000
    7   0.656987  0.753902 -0.859313  0.511449 -0.713721 -0.700430  0.188581 -0.982058  0.800422 -0.599437  ...  0.002952  0.999996  0.002214  0.999998  0.001660  0.999999  0.001245  0.999999  0.000933  1.000000
    8   0.989358 -0.145500 -0.280228  0.959933 -0.977262 -0.212036 -0.229904 -0.973213  0.574318 -0.818632  ...  0.003374  0.999994  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999
    9   0.412118 -0.911130  0.449194  0.893434 -0.939824  0.341660 -0.608108 -0.793854  0.291259 -0.956644  ...  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999  0.001200  0.999999
    10 -0.544021 -0.839072  0.937633  0.347628 -0.612937  0.790132 -0.879767 -0.475405 -0.020684 -0.999786  ...  0.004217  0.999991  0.003162  0.999995  0.002371  0.999997  0.001778  0.999998  0.001334  0.999999
    11 -0.999990  0.004426  0.923052 -0.384674 -0.097276  0.995257 -0.997283 -0.073661 -0.330575 -0.943780  ...  0.004639  0.999989  0.003478  0.999994  0.002609  0.999997  0.001956  0.999998  0.001467  0.999999
    12 -0.536573  0.843854  0.413275 -0.910606  0.448343  0.893862 -0.940067  0.340989 -0.607683 -0.794179  ...  0.005060  0.999987  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999
    13  0.420167  0.907447 -0.318216 -0.948018  0.855881  0.517173 -0.718144  0.695895 -0.824528 -0.565821  ...  0.005482  0.999985  0.004111  0.999992  0.003083  0.999995  0.002312  0.999997  0.001734  0.999999
    14  0.990607  0.136737 -0.878990 -0.476839  0.999823 -0.018796 -0.370395  0.928874 -0.959605 -0.281349  ...  0.005904  0.999983  0.004427  0.999990  0.003320  0.999995  0.002490  0.999997  0.001867  0.999998
    15  0.650288 -0.759688 -0.968206  0.250154  0.835838 -0.548975  0.042249  0.999107 -0.999519  0.031022  ...  0.006325  0.999980  0.004743  0.999989  0.003557  0.999994  0.002667  0.999996  0.002000  0.999998
    
    [16 rows x 64 columns]



```python
# Illustration Purpose Only
# 可视化我们的位置嵌入数字并查看模式
# 每条垂直线是 0 到 64 之间的维度;每行代表一个字符。这些值介于 -1 和 1 之间，因为它们来自正弦和余弦函数。颜色较暗表示值更接近 -1，颜色较亮表示值更接近 1。绿色表示介于两者之间的值
def visualize_pe(pe):
    plt.imshow(pe, aspect="auto")
    plt.title("Positional Encoding")
    plt.xlabel("Encoding Dimension")
    plt.ylabel("Position Index")
    plt.colorbar()
    plt.show()

position_encoding_lookup_table2_np = position_encoding_lookup_table[0].cpu().numpy()
visualize_pe(position_encoding_lookup_table2_np)
```


    
![png](step-by-stepT_files/step-by-stepT_8_0.png)
    



```python
# 7，输入准备阶段：将位置信息添加到每个输入嵌入行中，以获得最终输入嵌入矩阵
# 由于两个形状相同的矩阵可以相加，因此我们可以将位置信息添加到每个输入嵌入行中，以获得最终输入嵌入矩阵
# Add positional encoding into the input embedding vector
input_embedding_x = x_batch_embedding + position_encoding_lookup_table # [4, 16, 64] [batch_size, context_length, d_model]
input_embedding_y = y_batch_embedding + position_encoding_lookup_table

print(pd.DataFrame(input_embedding_x[0].detach().cpu().numpy()))
print(pd.DataFrame(input_embedding_y[0].detach().cpu().numpy()))
```

              0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
    0   1.399742 -0.206918  0.327990  0.748870  0.531874  0.751807  0.351937  0.610762  0.129977 -0.207580  ... -0.547752  2.515671  1.223744  1.833670 -0.404194  1.563555  0.591292  0.275255  1.670105  0.758279
    1   1.407957 -0.561973  2.393893  0.377252  1.083745  0.138066 -0.334590  1.670161  0.329176  2.342795  ...  0.873984  2.226714  0.794747  1.598629  0.884658  1.032520  1.353795  1.059697  1.172643  1.527427
    2   0.482819  1.301215  0.653670 -0.846175  0.628521  1.126829 -0.102938 -0.636202  0.428573  0.553768  ... -0.934116  2.145729 -1.913517  0.552654  0.597746  2.673483 -1.969119  1.397835 -0.438208  0.437077
    3   0.851059  0.379319  0.070685  0.910763 -1.117662  0.325378  0.947827  0.472564  0.516017  0.790074  ...  0.072798  0.264451  0.070916 -1.744750  1.088080  0.002188  0.715526 -0.357311  1.604357  1.920290
    4  -2.720048 -0.354717  0.272903 -0.906938  0.932237 -1.449321 -0.226828 -1.203768  2.488952  2.130765  ...  0.533600  0.432145 -2.389682  0.913403  0.066965  1.655225  0.625080  0.236625 -0.692241  0.992275
    5  -0.147775  0.718796  0.559903 -0.004127 -0.690035 -0.998508  0.331355 -1.222723  0.836060 -1.353497  ... -0.044886 -0.201054 -0.926252  1.322522  0.587324  1.108183 -1.652407  2.918813  0.942309  1.584331
    6   0.038134  3.066611 -1.069607  0.424900 -1.142844 -2.948736  0.505219 -0.617680  1.280667 -0.168857  ...  0.202646  1.051821  1.306703  1.517673  0.050767  1.044631  1.347860  0.678610 -0.477987  0.833080
    7  -1.368717  2.010294 -1.177933  1.943613  0.931115 -2.610583 -0.812628 -1.958096  2.302625  0.242536  ... -2.336618  1.190780 -0.052987  3.281737 -0.415515  0.198294 -1.392472  2.863094 -0.392633  0.868254
    8   3.896336 -0.053480 -1.065470  1.569054 -1.767442 -0.238040 -1.386770 -0.574759  0.118923 -1.069920  ...  3.366446  0.203255  1.759607  2.526687 -0.652322  2.660683  0.966853  1.618786  1.664013  2.768385
    9   1.811860 -2.118048  0.777184  0.642304 -0.407949  0.093468 -0.256172 -1.183092  0.421236 -2.164224  ... -0.543957  2.515664  1.226590  1.833666 -0.402060  1.563552  0.592893  0.275254  1.671305  0.758279
    10 -0.173195 -0.566551 -0.977712  0.379931 -0.595008 -0.661265 -2.512674 -1.641119  2.048560 -0.448827  ... -1.471521  1.216835  0.725749  2.340466 -0.706827  2.097339  0.233203  1.104480 -1.582538  2.481634
    11 -1.030607  0.309522  0.723369  0.637185 -0.141566  2.726367 -2.486894  0.854962 -0.917957 -2.444283  ...  0.308122  0.597250  0.045924  0.127550  1.359100  2.417307 -1.434337  0.254191 -0.240947  0.542596
    12 -1.463599  0.860966 -0.461637  0.274124  1.176311  0.329000 -2.790855  1.193104 -1.473960 -2.029624  ... -0.055479  1.198788  0.546739  0.594933  0.364360  2.217009 -0.566188  2.032797  0.175648  2.400138
    13  1.045454 -0.221277 -2.794333 -0.666396  0.902610  1.350902 -1.489786 -0.188839  0.085725  0.034213  ... -2.388670  0.201825 -0.965607  1.399124 -0.765167  1.607603 -1.889009  2.642380 -0.154335  0.069571
    14 -0.007462  1.160874 -0.672790 -2.764018  0.996131  0.188258 -0.232188 -0.063762 -0.468037  0.291926  ...  0.152741  1.273031  0.311204 -0.205802 -0.798966  0.182580 -0.989119  1.227733  0.583054  1.285420
    15  1.162378 -1.163164 -2.252026 -0.785099  1.221151 -0.297189  1.771678  2.388697 -0.672970 -0.417672  ...  1.367740  2.225356 -0.191733 -0.841328 -0.835119  0.627744  0.116416  0.363206  0.752731  1.664508
    
    [16 rows x 64 columns]
              0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
    0   0.566486 -0.102276  1.712332  0.645491  0.550577  0.292057 -0.743899  1.757765  0.018193  2.392380  ...  0.873562  2.226714  0.794431  1.598629  0.884421  1.032520  1.353617  1.059697  1.172510  1.527427
    1   0.414993  2.257664  0.337751 -0.185363  0.259559  1.541375 -0.440533 -0.388739  0.148429  0.697605  ... -0.934538  2.145729 -1.913833  0.552654  0.597509  2.673483 -1.969297  1.397835 -0.438342  0.437077
    2   1.619236  0.953164  0.289892  1.609638 -1.208784  0.872807  0.741096  0.836529  0.294495  1.013898  ...  0.072376  0.264451  0.070600 -1.744750  1.087842  0.002188  0.715348 -0.357311  1.604223  1.920290
    3  -1.822126 -0.691066  0.909637 -0.544932  1.147018 -0.937607 -0.266474 -0.787071  2.348020  2.412382  ...  0.533179  0.432146 -2.389998  0.913404  0.066728  1.655226  0.624902  0.236625 -0.692374  0.992275
    4   0.054347 -0.218510  1.272569 -0.173198 -0.235499 -0.680109  0.465740 -0.826303  0.789694 -1.042017  ... -0.045308 -0.201053 -0.926568  1.322523  0.587087  1.108184 -1.652585  2.918813  0.942176  1.584331
    5  -0.641375  2.390103 -0.663338 -0.184546 -0.588541 -2.921712  0.790090 -0.310992  1.333465  0.141597  ...  0.202224  1.051822  1.306387  1.517674  0.050530  1.044631  1.347683  0.678610 -0.478120  0.833080
    6  -2.305119  2.216562 -1.296015  1.220747  1.414469 -2.883257 -0.427184 -1.794876  2.449352  0.521177  ... -2.337040  1.190781 -0.053303  3.281738 -0.415753  0.198295 -1.392649  2.863094 -0.392766  0.868254
    7   3.563965  0.845922 -1.644555  1.120570 -1.503901 -0.726434 -0.968285 -0.583603  0.345026 -0.850725  ...  3.366024  0.203256  1.759291  2.526688 -0.652559  2.660683  0.966675  1.618786  1.663879  2.768385
    8   2.389100 -1.352418  0.047762  0.708803 -0.445387 -0.460229  0.122033 -1.362451  0.704295 -2.026212  ... -0.544379  2.515666  1.226274  1.833667 -0.402297  1.563553  0.592715  0.275254  1.671171  0.758279
    9   0.782945 -0.638610 -1.466152  0.925738 -0.921894 -1.109737 -2.241015 -1.959568  2.360503 -0.405686  ... -1.471942  1.216836  0.725433  2.340467 -0.707065  2.097339  0.233025  1.104480 -1.582671  2.481634
    10 -0.574638 -0.533976  0.737949  1.369487 -0.657227  2.521242 -2.369378  0.453218 -0.608066 -2.500290  ...  0.307700  0.597252  0.045608  0.127551  1.358862  2.417307 -1.434515  0.254192 -0.241081  0.542596
    11 -1.927016  0.021537  0.048141  0.800057  0.630691  0.430396 -2.848071  0.778454 -1.196852 -2.179225  ... -0.055900  1.198790  0.546423  0.594934  0.364123  2.217009 -0.566366  2.032798  0.175515  2.400138
    12  0.088714 -0.284870 -2.062842 -0.628985  0.495072  1.727591 -1.711710 -0.543744  0.302570 -0.194146  ... -2.389092  0.201828 -0.965923  1.399125 -0.765404  1.607604 -1.889186  2.642380 -0.154468  0.069572
    13 -0.577903  1.931583 -0.112015 -3.235197  0.852189  0.724227 -0.579936 -0.296741 -0.332961  0.007455  ...  0.152320  1.273033  0.310888 -0.205801 -0.799203  0.182581 -0.989297  1.227734  0.582920  1.285421
    14  1.502698 -0.266739 -2.162811 -1.512092  1.385136  0.232990  1.359034  2.318464 -0.633057 -0.730043  ...  1.367319  2.225359 -0.192049 -0.841327 -0.835356  0.627745  0.116238  0.363206  0.752598  1.664509
    15  1.298410 -0.946261 -0.615469 -1.575450  2.106571 -1.518780  0.573318  1.349758 -1.059684  1.534388  ... -0.424336  0.491003 -0.598103  1.668660  0.913921  0.446464 -0.555881  2.340090  1.306710  2.511890
    
    [16 rows x 64 columns]



```python
# 8，特征提取与融合阶段-多头注意力机制：准备 Q、K、V 
# Prepare Query, Key, Value for Multi-head Attention
# 转换为 Q、K、V 的原因：
# 引入不同的表示视角：将输入X通过不同的线性变换得到Q、K、V，可以让模型从不同的角度对输入信息进行编码和处理。Q主要用于引导模型对信息的查询和关注，K用于提供信息的索引和标识，V则携带了实际的信息内容。这样的设计使得模型能够更灵活地捕捉输入数据中的复杂关系。
# 实现注意力机制：Q、K、V是注意力机制的核心组成部分。注意力机制的目的是让模型能够根据不同的任务需求，动态地关注输入序列中的不同部分。通过将输入转换为Q、K、V，模型可以计算出每个位置对于其他位置的注意力权重，从而有针对性地聚合信息，提高模型对重要信息的捕捉能力。
# 提高模型的灵活性和泛化能力：将输入转换为Q、K、V并进行注意力计算，使得模型能够自适应地关注输入数据中的不同部分，根据具体的任务和输入内容动态地调整注意力分布。这种灵活性使得模型能够更好地适应各种不同的语言现象和任务需求，提高了模型的泛化能力，能够在多种自然语言处理任务中取得较好的效果，如文本生成、机器翻译、问答系统等。
# 虽然模型在一定程度上可以解决输入序列中位置之间的相似度问题，但这只是其功能的一部分，更重要的是通过一系列的设计和计算，实现对输入数据的深度理解、信息融合和高效处理，以解决各种复杂的自然语言处理任务和其他相关领域的问题。
# 在 PyTorch 中，view 方法是用于改变张量形状的函数。它可以在不改变张量数据的情况下，将张量重新排列成指定的形状。其语法为 tensor.view(*shape)，其中 *shape 是一个可变参数，表示新的形状。
# 例如，如果有一个形状为 [4, 16, 64] 的张量，想要将其重塑为 [4, 16, 4, 16] 的形状，可以使用 view 方法，如下所示：
# tensor = torch.randn(4, 16, 64)
# reshaped_tensor = tensor.view(4, 16, 4, 16)
# 通过 view 函数对张量 Q、K、V 的形状进行了重新组织，以适应后续的计算需求，特别是在多头注意力机制的计算过程中。
X = input_embedding_x
query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]

print(query.shape)

Wq = nn.Linear(d_model, d_model) # nn.Linear：这是 PyTorch 中 torch.nn 模块里用于创建线性层（也称为全连接层）的类。线性层的作用是对输入数据进行线性变换，其数学公式为 y = x * W + b，其中 x 是输入张量，W 是权重矩阵，b 是偏置向量，y 是输出张量。有两个 d_model 参数。第一个 d_model 表示输入特征的维度，意味着这个线性层期望输入的张量在最后一个维度上有 d_model 个特征。第二个 d_model 表示输出特征的维度，即经过这个线性层变换后，输出张量在最后一个维度上也会有 d_model 个特征。例如，如果 d_model 的值为 64，那么这个线性层的输入和输出特征维度都是 64。
Wk = nn.Linear(d_model, d_model) # Wk 是一个自定义的变量名，用于存储创建的线性层对象。后续在模型的计算过程中，可以通过调用 Wk（例如 output = Wk(input)）来对输入数据应用这个线性变换。
Wv = nn.Linear(d_model, d_model)

Q = Wq(query) #[4, 16, 64]
Q = Q.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]

K = Wk(key) #[4, 16, 64]
K = K.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]

V = Wv(value) #[4, 16, 64]
V = V.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]

# print(torch.round(Q[0] * 100) / 100)
qqq = Q.detach().cpu().numpy()
for qs in qqq:
    for qss in qs:
        print(pd.DataFrame(qss))

print(Q.shape) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]
```

    torch.Size([4, 16, 64])
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.843055 -0.550051 -0.127732 -0.844840 -0.819224  0.063522 -0.187879  1.182711 -1.542636  0.582238  0.123997 -0.023149  0.141312  0.812356 -1.628145  0.312126
    1 -0.350486  0.484634 -0.140094  0.746821  1.481563 -0.266804  0.312059 -0.710153 -0.557109  0.462987 -0.575465  0.824084 -0.910005 -0.060633 -0.851141 -0.696344
    2 -0.140998 -0.055692 -0.718412 -0.797249  0.366570  1.513673  0.577341 -0.707247 -0.275017  1.270264 -0.846118 -1.158900  1.652279  0.261800 -0.884520 -0.846511
    3 -0.244500  0.759894  0.067822  0.388658 -0.178537  1.044808 -0.276759 -0.887757 -0.158977 -0.619547 -0.298364 -0.604880 -1.250719  0.388842 -1.088378  0.081781
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.614938 -0.657467  0.080918 -0.701147 -0.332033 -0.185407 -1.033170  0.447318  0.462455  0.497301 -0.083056 -1.593413  0.454453  0.745469 -1.651096  0.451588
    1 -0.145782  0.062150  0.549227  0.992860  0.473911 -0.393052  0.089704  0.394661 -0.309076 -0.449691 -0.507969  0.825263 -0.800274 -0.936774 -1.369464  0.705076
    2  0.238615  1.076501  0.382592 -0.691137 -1.052643  0.575740  0.474749 -0.176139  0.206100  1.265228 -0.710662  0.764700  1.243479 -0.662066 -1.538389 -0.075417
    3  0.319884  0.242967 -0.233653 -0.966210 -0.693450  0.361393 -1.224569  0.128644 -0.085611  0.866901 -0.032113  0.048993 -0.604088 -1.324803 -1.287755  0.321337
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.477557 -0.665898 -0.062522 -0.844529 -0.971296 -0.579648 -0.250835  0.137787  0.971833  0.627744 -0.350412 -1.107923  1.009746  0.223489 -1.007624  0.238047
    1 -0.157736 -0.379647  0.224449  0.343057  0.675092 -0.883075  0.771069 -0.058893 -0.942641 -0.543024  0.037494 -0.085867 -1.234599  0.592343 -1.158115 -0.058505
    2 -0.451328  0.773131  0.400444 -0.951134 -0.099844  0.243306  0.205239 -0.197144  0.122609 -0.414965 -1.047807  2.015589  0.876089 -0.899640 -1.242613 -1.861980
    3  1.023614  0.791576 -0.105600 -0.760460 -0.164955 -1.103956 -1.514201  0.519381 -0.235128  0.016063 -0.164853 -0.578453  0.694984 -0.830543 -0.427421 -0.185568
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.306344  1.002409 -0.558756 -0.368392 -0.373141 -1.728941  0.253534  0.143261  0.430667  0.641169 -1.484428 -0.382139  0.807904  0.482248 -1.512201  1.232279
    1 -0.550624 -0.022481  0.110000  0.862089  0.557518  0.142512 -0.161860 -0.929796 -0.371089  0.245111  1.033635  0.591592  0.655191  0.273143 -1.534093 -0.017986
    2 -0.343869  0.463462  0.655731 -1.096172  0.000537  1.985811  0.916595  0.886440  0.575298  0.540631 -0.838614 -0.029521  1.687371 -0.517568 -0.002811 -0.565382
    3  0.490431  0.417849  0.480311  0.934107 -0.065339 -0.010033  0.094153 -0.461706  0.424854  0.309975 -0.281256  1.245372  0.686575 -1.852055 -0.234020 -0.428271
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.438334 -0.953410 -0.510782 -0.506376  0.031280 -0.456936  0.935150  0.421769 -0.444609  1.587090  1.122377  0.088356 -0.050521 -0.872715 -0.639425  0.572517
    1  0.295063 -0.300459 -0.087846 -0.854635 -0.029546 -0.896483  0.200802 -1.218089 -0.877742  0.419938 -0.423555  1.188569 -0.862475 -0.562507 -0.539659 -0.153115
    2  0.086299 -1.197778 -0.423674  0.552355  0.725277  0.213812  0.077010  0.029638  0.246063  1.194938 -0.297176  0.751607  0.912519  0.577433 -1.155185  0.272354
    3 -0.901110  0.514844 -0.300310  0.770331 -0.456820 -0.062889 -0.767173  0.156300  0.364622 -0.741814 -0.279991 -0.147554 -0.752332 -0.346314 -0.393168 -0.086922
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.098004 -1.362101 -0.880886 -0.048790 -0.831976 -0.919179  0.458893  0.475047 -0.364164  0.514087 -0.198763  0.427323 -0.647461  0.266048 -0.577928  1.007184
    1 -0.478354 -0.721252  0.784030  0.061834  0.492232 -0.712863 -0.290083 -0.502997  0.520079  0.264797  0.367906  0.900001 -0.784960 -0.213522  0.282126 -0.722758
    2 -0.036579 -0.290290  0.008043 -0.143929  0.665982  0.520363  0.824292  0.072430  0.679129  1.127481 -0.546705  0.412061  1.899873 -0.461428 -0.429431 -0.858572
    3 -1.332145 -0.286588 -0.214228  0.020612 -0.352273 -0.049677 -0.492795 -0.235711  0.168299 -0.326701  0.104917 -0.673956 -0.071659  0.481643  0.049477  0.066592
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.433669 -1.239478 -0.150002 -1.126047  0.151928 -0.831388  0.543729  0.171746 -0.161562  0.301515  0.752866  1.031278  0.243581  0.017311 -1.302813 -0.161804
    1  2.030139  0.848804 -1.016704  0.075851  0.832877 -0.595107 -0.152181 -0.398114  0.323685  0.558464  1.005251  1.297820 -1.275880 -0.522440 -0.065408 -0.632012
    2  0.335642 -0.023684 -1.246166 -0.892868  1.718963  1.081414  0.382986  0.042498  1.659014 -1.029562 -0.731215  0.100014 -0.983408  0.299675 -0.636578 -0.885547
    3 -0.060804  1.340595  0.476056  1.011678 -0.739153  0.736712 -0.095485  1.207922  0.523271 -0.370075  0.505100  1.276299  0.144184  1.645972 -0.691074 -0.852811
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.147221 -1.018187 -0.014175 -1.058182 -0.086709 -0.285634  0.605431  1.288928  0.728841  0.193770  0.235218  0.624430 -1.221081  1.286846 -0.811486 -0.250773
    1 -0.830173 -0.201015  0.375342  1.265234 -0.471125  0.340549 -0.578870 -0.874984 -1.361585 -0.547904  0.919332  0.737287 -0.343580  0.053767  1.055182 -0.393545
    2 -0.943106 -0.257088  0.703252 -1.483492  0.573838  1.792996  0.816353  0.868351  1.144592  0.029599  0.419578 -0.108692  0.989305  0.400592 -0.742554 -1.176377
    3 -0.643927 -0.588140 -0.749730 -0.096672 -0.239110 -0.649397  0.518488  0.144483  1.279444 -1.409347 -0.396471 -0.893813  0.444066  1.756736  0.131318 -1.130873
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.799027  0.856980  0.446163  0.499316 -0.239582  0.088171 -0.536068 -0.025151 -0.556649  0.104557  1.030410 -0.996602 -0.416170 -0.349448 -0.417679  0.926921
    1  0.276474  0.690336 -1.160764  0.339022  1.380022 -2.299778 -0.115019  0.680937  0.577072 -0.210091  0.209844 -0.105638 -0.505048  0.118918 -0.305055 -0.552583
    2  0.049127  0.937139 -0.036767 -1.256461  0.315426  1.488370  1.166160  1.563194  0.747862  0.093654 -0.987244 -1.068065  0.889559  0.637340  1.055367 -0.694108
    3 -0.127101  1.703753 -0.780233 -0.362808 -0.211615 -0.016047 -0.382914  0.765831 -0.267362 -0.231050  1.126519 -0.050697 -0.417823  0.072670  0.054113  0.375692
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.807956 -0.080249  0.028504 -0.205558 -0.510860 -0.623191 -0.371553  1.062170 -2.003340  0.103988  0.828823  0.067705  0.314004  0.260999 -1.272962  0.705449
    1  0.075932  0.159653 -0.375793  0.480969  1.502070 -0.806800  0.496836 -0.671475 -0.481707  0.587015 -0.592886  0.452936 -0.618360 -0.193267 -0.913532 -1.056499
    2 -0.166165 -0.111579 -0.476513 -0.971685  0.430499  1.906988  0.882759 -0.295891  0.490625  1.036929 -0.382306 -1.500847  1.169874  0.514438 -0.322263 -1.096552
    3 -0.605010  1.021955  0.607436  0.416600 -0.263974  1.275973 -0.368588 -0.490732  0.345144 -0.508597  0.101237 -0.505797 -1.383213  0.777961 -0.794481  0.200481
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.975871 -0.355146  0.559124  0.062120 -0.388668 -0.141609 -0.105870  1.215286 -0.553361 -0.017332  1.186850  0.127466 -0.455665 -0.147964 -0.403373 -0.231227
    1 -0.205101 -0.035606 -0.262179  0.460006 -0.181990 -0.047399  1.185728 -0.185939 -0.446449  0.147912 -0.173577  0.529385 -0.016233  0.513281 -0.778501 -0.917174
    2 -1.216010 -1.284461  1.441692 -0.500132  0.130558  1.589988  0.537305 -0.073081  0.368621 -0.432179  0.292155 -0.420973  0.415114  0.966622 -0.029067 -0.783478
    3 -1.276993  0.963069 -0.561037  0.083244 -0.130642 -0.090679 -0.371224 -0.047228  0.877646 -0.731854 -0.649757 -1.325310 -0.018268 -0.085761 -1.308902 -0.753023
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.609152  0.293588  0.681168  0.918692 -0.678428 -0.354196  0.642279  0.126239  0.576536 -0.103225  0.383504 -0.529539 -0.000529 -0.494851 -0.752642  0.999774
    1  0.014018 -0.073415 -0.514388  0.181423  0.368242 -0.733541  1.133217  0.010973  0.346221 -0.306160  0.369413  0.021123 -0.623327  0.922354 -0.711439  0.181036
    2 -1.416832  0.690725 -0.128975  0.545911  0.819613  0.555542  0.133320  0.751006 -0.056599 -0.231622 -1.088230 -0.624539 -0.307156  0.812401  0.886276 -0.646983
    3 -0.565820  0.253911 -0.284727  0.483336 -0.032595 -0.357083  0.346250  0.425751  0.231869 -0.779479  0.215294  0.004534 -0.338554 -0.836733 -1.012783 -1.264050
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.246534  0.311532 -0.668713  0.257104  0.410948 -0.281124  0.246150  0.809893 -0.196731  0.002324  0.671286 -0.184460 -1.098380 -0.228603  0.200689  0.926620
    1  1.258551  0.512766 -0.600787 -0.730703  0.199514 -1.407976  0.178042  0.112705 -1.211821 -1.171478  0.350218  1.014140 -0.667218  0.248370  0.630296 -0.835714
    2 -0.033908  0.473868 -0.262070  0.831669  0.498441 -0.002046  0.850105 -0.393790  0.640280 -0.812363 -1.168075  0.823002  0.174674  0.574397 -0.715130 -1.762390
    3  0.124570 -0.550700 -1.080804 -1.263694 -0.748561 -0.488424 -0.637535  1.093251 -0.212878 -1.900913  0.290618 -0.548380 -0.378159  0.075069 -0.142519 -0.604316
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.045093 -0.017782 -0.036026  0.738200  0.535909  0.540001 -0.284811  0.817065  0.161400 -0.418709  0.505471 -0.807120 -1.493106 -0.354721 -0.677752 -1.364141
    1 -0.553766 -0.643496 -1.064138  0.517799 -0.125831 -0.866458  0.517786 -0.161137  0.205678 -0.741630  0.237892  0.478060  0.579789  0.911892  0.221268 -0.473353
    2 -0.975538  0.020211  0.270379 -0.424409 -0.191103  2.134469  0.342932  0.477962  0.237305 -0.928211 -0.706870 -0.156639 -0.178393  1.178581 -0.449019 -1.222883
    3  0.464261 -0.061850  0.318785  0.414360  1.113341  0.312915 -1.301479  0.484767  0.295944 -1.226489 -0.421380 -1.187196 -1.161356 -0.462495 -0.462336 -0.033009
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.450824 -0.462299  0.040806  0.125190 -0.824344  0.226403 -0.032060  0.119213 -0.317265 -0.176777  0.032324 -0.008321 -1.653131  0.684965 -1.229449 -0.189300
    1  0.128412  0.410048 -0.352405 -0.017320  0.521036 -0.909860  1.139977 -1.093351 -0.397575  0.403441  0.459461  0.752749  0.028293  0.395984  0.351194 -0.202403
    2 -0.208809  0.352684 -1.591495 -0.094741  0.688446  0.350263  0.301608 -0.364587  0.342892 -0.051749 -1.301603 -0.089152 -0.648856  0.080371 -1.173354 -0.958096
    3 -0.127549  0.289107 -0.347003  0.595761  0.312818  0.595343  0.013879  0.676382  0.473617 -0.335544 -0.053533  0.074190 -0.820578  0.144357 -0.719646 -0.173205
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.162300  0.810238 -0.950388  0.148364  0.149367  0.110746  0.224680 -1.256394  0.587622  0.771594 -0.404515 -0.575203 -0.644574  0.833663 -0.296932  0.524873
    1  0.247992  0.218081 -0.906489  0.290906  0.408525  0.238430 -0.589142  0.493921  0.276924 -0.795749  0.255871 -0.493924 -0.131710  1.504040 -0.143763 -1.431793
    2 -0.738435  1.368045 -0.962342  0.105499  0.967353 -0.287253  0.924607 -0.144282 -0.480593  0.760399 -0.268804 -0.655696  0.737238  0.279763 -0.736466 -0.631314
    3  1.751726  0.910328 -0.860379  0.358043 -0.686785 -0.679672 -0.331689 -0.186889 -0.187969  0.584522  1.299885 -0.126924 -0.778028 -0.455524  0.194849  0.312603
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.040591 -1.732430  0.535824 -0.954974 -2.603649  0.957650 -0.246722  0.535815  0.109470  0.249254  0.016516 -0.833666  1.394056  0.508778 -0.962780  0.256198
    1 -0.393435 -1.428576  0.527408  0.083288  0.718811 -1.246769 -0.440604  0.529822 -0.705668 -0.320694  0.520680 -0.457920 -1.183794 -0.457521 -1.093361  1.068063
    2 -0.241522  1.396123  0.328996 -0.781264  0.504539  0.107320  0.134646 -0.644498 -0.511961  0.427737 -1.987921 -0.009629  0.166767 -0.461718 -0.606908 -0.372021
    3  0.210581 -0.098175 -0.696049 -0.555823 -0.827056 -0.590123 -1.126034 -1.177567 -0.730200  0.104929 -0.356334 -0.220717  0.136896 -0.590295 -2.865558 -0.424233
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.411827 -2.483540  0.053123 -0.925263  0.360051  0.623676 -0.124993  0.159468  0.198072  0.594068 -0.474884 -1.280501  0.303108 -0.850220 -0.892375 -0.939363
    1 -0.044534 -0.204753  0.233314  0.749154 -0.218644 -0.693456 -0.457773  1.001401  1.039589 -0.014641 -0.307821  1.190629 -1.070807 -0.807371 -0.881589 -0.527952
    2 -0.699358 -1.333793 -0.699475 -0.129277  0.059782  0.764894 -0.056004 -1.113134 -0.499442  0.918923 -1.280702 -0.491481  1.078909 -0.093876 -0.641482 -0.118734
    3 -0.564800  0.499971 -0.642852  0.397720  0.169384  0.624887 -0.979860 -0.546058 -0.446780 -1.158767 -0.091098 -0.327687 -1.669777 -0.101685 -1.274173 -0.141335
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.068883 -1.115615 -0.357018 -0.015936  0.008346  0.085774 -0.354136 -0.656835  0.423317  0.345923  0.836919 -1.179749 -0.554443 -1.097682  0.860269 -0.158142
    1  0.009750 -0.282828 -0.829414  0.071886  0.169630 -0.810468 -0.203140  0.086831 -0.448596 -0.323520 -0.166213  0.891237  0.018219 -0.639605 -0.662989 -0.733895
    2 -0.472119 -0.585412 -0.768696  0.632276  0.115810  0.220249  0.413234 -0.142372  0.506677  0.865187 -1.043125  0.052915  1.086173  0.775785 -0.687548  0.561888
    3 -0.400620  0.729620 -0.715408  0.108888  0.510726 -0.097342 -0.594547  0.713915 -0.543334  0.048470  0.142754  0.394833 -1.054841  0.061452 -0.944729 -0.458728
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.902081 -1.200709 -0.257382  0.646042 -0.182794 -0.184262 -0.027675  0.217132  0.142002  2.054800 -0.394128 -0.465937  0.745165  0.564093 -1.616877  0.484326
    1 -0.663733 -0.980381  1.089543  1.456469  0.975607 -0.855795  0.499958 -0.500753  0.047210  0.749712 -0.699079  0.198319 -0.639294 -1.052600 -0.938783  0.392807
    2 -0.306832  0.724243  0.561730 -0.751851  0.319922  0.967821  0.163208  0.383438  0.316170  1.546890 -0.366850  0.070544  2.103587 -0.546666 -0.496985 -0.860911
    3 -0.743987 -0.726349  0.046074 -0.221491  0.149525  1.097855 -0.537350 -0.875409  0.714005 -0.008699  0.344594 -0.926214 -1.185889 -1.015362 -1.575311 -0.704234
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.848914 -1.111739  0.006205 -0.039756 -0.933432  0.043748  0.154334  0.911096 -0.581574  0.443663 -0.595020 -0.262369 -0.380455  1.255284 -0.666683  0.853622
    1  0.815517 -0.364623  0.975614 -0.373926 -0.307815 -0.307191 -0.302004 -1.346682 -0.586013  0.702761  0.458446  0.274101 -0.601604  0.523892 -0.964500 -0.109026
    2  0.973533  0.140913  0.455739 -0.230147  0.263246  0.627085  1.259960 -0.178967  1.052730 -0.874989 -0.313048  0.577313  0.932425  0.900780 -1.945323 -1.009919
    3 -0.103243  0.544170 -1.326607 -0.637684  0.144137 -0.298935 -0.438171  0.646421  0.243074 -0.110419 -0.587781 -0.322077 -0.514877 -0.507403 -1.168004  0.435127
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.510558 -0.052268 -1.116502 -0.161902 -0.586372  0.105670  0.028299  0.774327 -0.053025  1.081806 -0.170327 -0.089421  0.502616  0.287768 -1.521247  0.794785
    1 -0.311080 -1.562692  0.837166 -0.634363  0.307649 -1.948408 -0.372313 -0.605613 -0.936221 -0.192276 -0.056637  0.101307 -0.792975 -0.105986 -0.236257  0.015216
    2  0.017085 -0.507896  0.899571 -0.455941 -0.362668  1.391112 -0.107061  0.238046 -0.315525  0.932544 -0.898798  0.875802  0.983320  0.222729 -0.074483 -0.998997
    3 -0.385237 -0.218126 -0.137004 -0.461211 -1.078457 -0.409342 -1.257615 -0.549745 -0.321394 -0.854914  0.355592 -0.779808 -0.950686  0.051154 -0.046016  0.221253
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.889798 -1.164731 -0.700482  0.033665  0.390086  0.031297 -0.490637 -0.085371 -0.487951  0.966926 -0.260996  0.527975 -0.205336  0.379985 -0.240390 -0.151295
    1  0.280330 -0.805784 -0.540771 -0.075365  0.600227 -0.736507  0.075909 -1.635359 -0.503545 -0.522346  0.632772  0.485401 -0.870348 -0.356595 -0.837994 -0.012191
    2 -0.942716  0.034204  0.100116 -0.430781  0.131741  1.203926  0.480484 -0.452285 -0.342294 -0.279016 -0.535710 -0.099177  0.673115  0.186452 -1.986973 -1.628828
    3 -0.553349  1.407444  0.062491  0.135272  0.325641  0.257729 -1.722388  0.102674  0.471423 -0.801807 -0.128275 -1.109145 -0.602943  0.500599 -0.271009 -0.016864
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.093965  0.205285 -0.285254 -1.113492 -1.152514  0.298618  0.117928 -0.059249 -0.927987  0.459643  1.681641 -1.200673 -0.799749  0.008045  0.030887 -0.257713
    1 -0.932471 -0.084573  0.361824  0.234218  0.319488 -1.438632 -0.029715 -0.626279 -0.282511  0.028903  1.048055  0.950885 -0.160759  0.389284  0.627767 -1.276038
    2 -0.475502  0.312370  0.484566 -0.332308  0.955019  1.655013  0.669675  0.759300  1.482135  0.827720 -0.718860 -0.228196  1.390810  0.481911  0.028049 -0.151780
    3 -0.218880  1.172321 -1.219250 -0.444534 -0.235476  0.236778 -0.081137  0.352130 -0.074667 -0.132512  0.780349  0.006259 -0.204236  0.361812 -0.265033 -0.688900
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.830415 -0.279750  0.046583 -0.391788 -0.563857 -0.386821 -0.225548  1.137196 -1.963818  0.181007  0.755685  0.229591  0.377930  0.351241 -1.333537  0.596582
    1  0.185397  0.247017 -0.415759  0.552577  1.578399 -0.754548  0.363617 -0.764385 -0.492507  0.617592 -0.529309  0.495332 -0.758185 -0.161221 -0.736383 -1.138794
    2 -0.105419 -0.304523 -0.553585 -1.196757  0.522040  2.052510  0.764300 -0.362917  0.456191  1.061300 -0.505191 -1.449561  1.084323  0.408596 -0.402934 -1.207800
    3 -0.449628  1.182088  0.634406  0.548405 -0.409534  1.220581 -0.390373 -0.595042  0.331115 -0.611581  0.080876 -0.430741 -1.242464  0.818053 -0.714036  0.302046
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.129658 -0.045262  0.570697 -0.255184  0.144432 -0.933445  0.781099  0.685583 -0.498323 -0.430914  0.485012 -0.771712 -0.414206 -0.440227 -0.081936  0.399950
    1  0.609679 -0.131552  0.411169 -0.882083 -0.518400 -1.112237 -0.255672 -0.659279  0.042839 -0.307857  0.214738  0.506229  0.242895  0.102200 -0.515571  0.586174
    2  1.080462 -0.214256 -1.082755  0.432201 -0.256774  1.216463  1.270056 -0.025288  0.998846  0.798230 -0.634019  0.690348  1.320518  0.368305 -1.035057 -1.378777
    3  0.020668 -0.425955 -0.029217 -0.564785 -0.019478  0.283089 -0.785113  0.626727 -0.789826 -0.741861 -0.760041 -0.227345 -0.559742 -0.552307  0.521503 -0.307743
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.017067 -0.132201  0.005626  0.503344  0.283302 -0.019817 -0.375682  0.887630 -0.013207 -0.690582  1.040519 -1.066493 -0.977408 -0.785576 -0.662742 -0.741300
    1 -0.899417 -1.241653 -0.731693  0.271986 -0.212329 -1.088669  0.407414 -0.006261  0.089814 -0.797364  0.367399  0.209349  0.372228  0.605199 -0.200269 -0.365062
    2 -0.846173  0.287940  0.647478 -0.366624 -0.408595  2.297336  0.545167  0.798921  0.669671 -0.767625 -0.397918 -0.263753 -0.029016  1.440027  0.004349 -1.003745
    3  0.165065 -0.109351  0.492917  0.181230  1.139297  0.234374 -1.204153  0.233503  0.183980 -0.872557 -0.557667 -1.474802 -1.108856 -0.114233 -0.701452  0.033226
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  2.639729  0.432699  0.228840  0.328165 -1.619762 -0.143244 -0.148332  1.446185 -0.136884 -0.225079  1.478135 -0.445745 -0.352616  0.028643 -1.034523  0.454969
    1 -1.186099 -0.324240 -0.791502 -1.016193  1.248315 -0.972243  1.534065 -0.547292 -0.946302  0.431607  0.267453  0.309294 -0.433166  0.201256 -0.104921 -0.838163
    2  0.106800  0.043554  0.900919  0.428170 -0.247493  0.183408  1.895423  0.936508  0.222470 -0.774004 -0.092559  0.208208  0.731011  0.268177 -0.025636 -0.487947
    3  0.084313 -0.803575 -0.570023 -0.064708 -1.060818  0.203365 -0.065024  0.202545  0.112469 -0.145246 -0.513070 -0.679315 -0.260319  0.085924 -1.460548 -1.566682
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.557170 -0.069228 -0.252429  1.265160 -0.494375  0.057511 -0.535007  0.295485  0.095640  0.343272  0.357061 -1.167645 -1.233100  0.834950 -1.384201 -0.601127
    1  0.260277  0.225782 -0.290535  0.660958 -0.063076 -0.393807  0.589620 -0.713500  0.398320 -0.530572 -0.241501  1.107055  0.356750  1.185921 -0.153039  0.215772
    2 -0.330623  0.452588  0.956771 -0.085573  0.729110  1.394057  0.861968  0.413198  0.140418 -1.303610 -0.348097 -0.386850  0.105175  0.568483 -0.466887 -1.195699
    3  0.410845  0.677328 -0.645256  0.401230  0.496405 -0.010768  0.305602  1.510773  0.423297  0.059943  0.245373 -0.200643 -0.773368 -0.625001 -0.626335 -0.814548
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.400794  1.674622 -0.415681 -0.043126 -0.117706 -1.618549  0.164571  0.309570  0.279495  0.378610 -1.406251 -0.275353 -0.007751  0.573838 -1.323963  0.907701
    1 -0.215844  0.564049 -0.409821  0.822482  0.802106 -0.206013  0.197558 -0.683868 -0.381642  0.043122  1.011165  0.687814  1.032968  0.730055 -1.051225 -0.444364
    2 -0.378914  0.294372  0.494542 -1.077896  0.473817  2.164143  1.176582  0.797308  0.450885 -0.162525 -0.898714 -0.042541  0.997564 -0.456949  0.199125 -0.993319
    3  0.643288  0.502156  0.548274  1.268326 -0.210520  0.171024  0.186575  0.253788  0.681656 -0.329138 -0.144567  1.465397  0.942959 -1.870249  0.069844 -0.950228
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.124050  0.371224 -0.305690  1.633266  1.128819 -0.565459  0.768883 -0.629083  0.272960  1.084742 -0.375039 -0.911658 -0.139525  0.008518 -0.125098 -0.617350
    1  0.061303 -0.101059  0.229802  0.550959  0.169034 -0.269751 -0.631112 -0.705359 -0.034888 -0.472547  0.160753  0.327375  0.070442  1.157730 -0.530561 -0.087982
    2  0.609980  0.256873 -0.059367 -0.192184  0.232327  1.274882  1.126307  0.743813 -0.913144  1.074244 -0.373727 -0.221155  2.028465  0.966211 -0.987181 -0.694455
    3  1.318778  0.719581  0.533894  0.456643  0.172207  1.013823 -1.328376  0.407472  0.906544 -0.761099  0.555193  0.784875 -1.695383 -1.376251  0.846673  0.197437
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.346843  0.402914 -0.150677  0.686670 -0.074748 -0.799165 -1.057026 -0.457870  0.001222 -0.029633  0.191230 -1.684285  0.309775 -0.031503 -0.056870  0.114892
    1  0.501270 -0.394868 -0.925444 -0.100002  0.339064 -1.044209  0.320324  0.616051 -0.516770 -0.791340 -0.093174 -0.221809  0.761773  0.443387 -1.108984  0.406432
    2 -0.031256  0.680584  0.657833 -0.708211 -0.352712  1.367602  1.522030  0.917488 -0.668882 -0.727117 -0.187182 -0.723699  0.793422  0.353857  0.172010 -0.420275
    3  0.659493  0.942593  0.302045  0.603299 -0.180087  0.333501 -1.532934  0.614952  0.680199  0.657408  0.167041  0.711188 -0.971582 -0.351090 -1.314679 -0.199633
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.089579 -0.973977  0.785349  0.142430 -0.214122  0.011994 -0.505739  0.275072 -0.760688  0.252204  0.822373 -0.967037 -0.197454  0.037943 -0.570538 -0.011652
    1  0.334160 -0.007269  0.605975  0.523686  0.846121 -0.731225  0.078269  0.136376 -0.141776  0.233214 -0.746309 -0.068632 -0.161884 -0.851085 -1.308490  0.871895
    2  0.362325  0.289150 -0.313092 -0.715971  0.189923  1.530835 -0.059266 -1.019142 -0.295478  1.257605 -0.598340 -1.377333  1.552617  0.794436 -1.117288 -0.343388
    3 -0.733659  0.890256 -0.073768  0.246744 -0.061131  0.390819 -0.930806  0.110050  0.317988 -0.263796  0.176278 -1.281914 -0.835195 -0.446496 -1.792402  0.635731
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.257294 -0.695780  0.121141 -0.177667 -0.508855 -0.030935  0.275189  0.017120 -0.159493  0.637873  0.150516 -0.303546  0.428513 -0.441419  0.376588 -0.236788
    1 -0.910946 -0.198898  0.122654 -0.736171 -0.036079 -0.905127 -0.353397 -0.145026 -0.461541  0.065215  0.192835  0.078685 -0.315833 -1.149468 -1.861958  1.334020
    2 -0.168921  0.189826  0.158124 -0.004286 -0.458063  0.967991  0.559503 -0.480577 -0.439033  0.417869 -1.184282  0.057145  1.950150  0.222633 -1.324048 -0.784906
    3 -0.112746 -0.499071 -0.856497  0.014403 -0.251019 -0.796245 -1.281106 -0.721319 -0.473868 -0.149498 -0.647904 -0.931394 -0.289107 -0.584800 -0.534413 -1.095125
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.068883 -1.115615 -0.357018 -0.015936  0.008346  0.085774 -0.354136 -0.656835  0.423317  0.345923  0.836919 -1.179749 -0.554443 -1.097682  0.860269 -0.158142
    1  0.009750 -0.282828 -0.829414  0.071886  0.169630 -0.810468 -0.203140  0.086831 -0.448596 -0.323520 -0.166213  0.891237  0.018219 -0.639605 -0.662989 -0.733895
    2 -0.472119 -0.585412 -0.768696  0.632276  0.115810  0.220249  0.413234 -0.142372  0.506677  0.865187 -1.043125  0.052915  1.086173  0.775785 -0.687548  0.561888
    3 -0.400620  0.729620 -0.715408  0.108888  0.510726 -0.097342 -0.594547  0.713915 -0.543334  0.048470  0.142754  0.394833 -1.054841  0.061452 -0.944729 -0.458728
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.176404  0.531872  0.409214  0.192468 -1.084885  0.238997 -0.798379  0.188346  0.059492 -0.024330  0.924521 -1.091804 -0.326156  0.291749 -0.732185 -0.054935
    1 -0.615884  0.863978 -0.437723 -0.152925  1.116533 -0.681281  0.415692 -0.764032 -0.687655  0.304295 -0.105089  0.731977  0.626084  0.591653 -0.146139  0.494095
    2 -0.189808  0.788168  0.899602 -1.132650 -0.081587  0.835441  0.785524  0.663442  0.063898 -0.166113 -0.850259  0.151737  1.317178 -0.699426 -0.841369 -0.484020
    3  0.683071  1.052087 -1.006277 -0.075180  0.912621 -0.327235 -1.111313  0.549032  0.110626 -0.140059 -0.477161 -0.342185 -0.293553 -0.850334 -0.414416  0.550237
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.073625  0.150127 -0.907866  0.334299 -0.202801 -0.097351  0.876653 -0.385648  0.045448  2.448797  0.102841  1.521410  0.074046  0.981755 -1.376029 -0.471205
    1  0.400723  0.219498  0.124873 -0.358635 -0.443369 -0.247101 -0.808554 -0.685117  0.247060  0.634415 -0.134543  0.128437 -0.833682  0.816896 -0.164843  0.732479
    2  0.263923  0.820781 -0.206039  0.285594  0.476259  0.501493 -0.342558 -0.732048 -0.340279  1.199124 -0.888102  0.372415  1.133199 -0.030477 -1.117813 -1.242966
    3  0.707586  1.043648 -0.785706 -0.182822 -1.052228 -1.017795 -0.886804 -1.194169  0.279976  0.415883 -0.114251 -1.453649 -0.459333 -0.811049  0.461218  0.328624
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.565246 -0.903285  0.252414  0.429647 -0.713902  1.917740  0.362854  0.713402 -0.210875  0.616729  1.185414  0.169722  0.619079  0.519919 -0.158226 -1.496089
    1  0.888702 -0.035670 -0.126169 -0.364905  0.810807 -0.035866 -0.105929  0.211207 -0.261469 -0.215912 -1.080510  0.501321 -0.848003  0.389684  0.131101 -0.584177
    2 -0.815836  0.537262  0.411533  0.070018  0.632414 -0.549836  0.097125 -0.331562  0.368747 -1.247963 -0.250107  0.091148  0.837426  0.622526 -0.783870 -0.812427
    3  0.127784 -0.015729 -1.144995 -0.323159 -0.314417 -0.153439 -1.000394  0.642610  0.306637  0.099248  0.058967 -0.746932 -0.273949  0.180222 -1.573224 -0.010142
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.255211 -0.157632 -0.574581 -0.166197  0.775349 -0.140706  0.687652  0.807035 -0.184444  0.656217  0.474147 -0.447196  0.060639 -0.188234 -1.242352 -0.329322
    1  0.488382 -0.108822 -0.275919 -0.090759 -0.832167  0.028082 -0.041036 -1.339602  0.134018  0.438956  0.426242  0.013138 -0.717898 -0.010739  0.128523  0.400315
    2  0.567573 -0.723301  0.512000 -0.188115 -0.571319  0.991571  0.793728  0.696590 -0.254946 -1.087185 -0.457050  0.546511  0.099871  0.519373 -1.122454 -1.564487
    3  0.835880 -0.165621  0.295777 -0.285468  0.093285  0.132677 -0.379293  0.561905  0.621888 -0.311707 -0.840512 -0.725356 -0.189057  0.202919  0.051076 -0.613800
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.236179 -1.009632 -0.326470 -0.297060 -0.118641  0.074772  0.023244 -0.203720  0.170052  0.070126  1.334358 -0.704639 -0.528730 -1.149122  0.729896  0.029149
    1  0.322406 -0.267601 -0.963115 -0.098632  0.339756 -1.158210 -0.381949  0.085271 -0.557521 -0.320726  0.213669  0.647582 -0.255545 -0.395816 -0.170296 -1.198567
    2 -0.267999 -0.983763 -0.655108  0.191769  0.576468  0.802856  0.599541 -0.029581  0.807375  0.281877 -0.961682  0.093886  0.208873  0.895717 -0.360856 -0.000831
    3 -0.354376  1.055429 -0.356498  0.454247 -0.083929 -0.238622 -0.499768  0.774746 -0.215797 -0.343473  0.161658  0.368649 -0.487320  0.695373 -0.789305 -0.547663
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.038222 -0.280386 -0.843706 -0.616376  0.294691  0.285665 -0.341028  0.934008 -0.646730 -1.280372  0.236532  0.280084 -0.841592  0.712234 -1.591673  0.064760
    1  0.129691  0.112961 -0.562534  0.463562  0.526540 -0.433082 -1.213341 -0.475485  0.278862 -0.680838  0.434830  1.604740  0.338845  0.895622  0.145595 -0.223971
    2 -0.329862  0.591885 -1.080422 -1.282320  0.325834  1.647089  1.472171 -0.050331  0.198912 -0.014550 -0.356193 -0.371771  0.288920  0.205029  0.371042 -0.932187
    3  0.546774 -0.186697  0.209333  0.978451  0.718924  0.990333  0.238876  0.500508 -0.173352 -1.233035 -0.107134 -0.610185 -0.074154  0.489221  0.113718  0.280216
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.306278 -0.150934  0.773143  0.595103 -0.325866 -0.395447 -0.256868  0.796102 -0.280283 -0.303306  1.391956 -1.493502 -0.704255 -0.555533 -1.038595 -0.122333
    1 -0.061770 -0.297525 -0.457170 -0.168391  0.302301 -1.931828  0.410538  0.788897  0.600575 -0.890034 -0.427509 -0.468240  0.590850  1.146330 -0.039833 -0.277840
    2 -0.456495  1.298474  0.484908 -0.122877  0.398217  1.352437  1.198192  0.608493  0.803740  0.279618 -0.096277  0.157943 -0.848379  0.831901  0.467421  0.236334
    3 -0.263925  1.021767  0.642150  0.543245  0.314341 -0.524004 -0.234829  1.024676  0.541816 -1.119100  0.150054  0.552932 -0.928475 -0.283212 -0.447764 -0.602209
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.169938 -0.497667 -0.325872  0.179360  0.028744 -0.469345 -0.364128 -0.387989  0.060916 -0.149403  1.493312 -1.122818 -0.762886 -1.368208  0.920177  0.245207
    1  0.088814 -0.410940 -0.922852 -0.260886  0.199503 -1.318331 -0.001653  0.331418 -0.532446 -0.429146  0.038133  0.582173  0.127124 -0.449546 -0.571451 -1.034238
    2 -0.412180 -0.528039 -0.503837  0.738685  0.370084  0.470210  0.903698  0.125189  0.880732  0.189188 -0.687172 -0.031385  0.387593  1.141662 -0.148142  0.251607
    3 -0.714183  0.683091 -0.422763  0.152439  0.290620 -0.064837 -0.431068  1.105868 -0.195865 -0.142729  0.210017  0.234452 -0.808928  0.553445 -0.942682 -0.842847
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.609152  0.293588  0.681168  0.918692 -0.678428 -0.354196  0.642279  0.126239  0.576536 -0.103225  0.383504 -0.529539 -0.000529 -0.494851 -0.752642  0.999774
    1  0.014018 -0.073415 -0.514388  0.181423  0.368242 -0.733541  1.133217  0.010973  0.346221 -0.306160  0.369413  0.021123 -0.623327  0.922354 -0.711439  0.181036
    2 -1.416832  0.690725 -0.128975  0.545911  0.819613  0.555542  0.133320  0.751006 -0.056599 -0.231622 -1.088230 -0.624539 -0.307156  0.812401  0.886276 -0.646983
    3 -0.565820  0.253911 -0.284727  0.483336 -0.032595 -0.357083  0.346250  0.425751  0.231869 -0.779479  0.215294  0.004534 -0.338554 -0.836733 -1.012783 -1.264050
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.089952  0.067533  0.085976 -0.181908 -0.242819 -0.559609 -0.495977 -0.046075 -0.098181 -0.457995  0.522625 -1.265378 -1.320826 -0.209618  0.640950  0.411471
    1  1.201720  0.347224 -0.412666 -0.487666 -0.788717 -0.983542  0.471513 -0.094739 -1.259613 -0.879070  0.363518  0.810040  1.047161  0.236667 -0.032963 -0.061229
    2  0.425411 -0.076583  0.659215  0.843612  0.921593  1.298184  0.342010  0.924739  1.574549 -0.853615 -0.380778  0.607227 -0.442868  1.298039  0.039828  0.212155
    3 -0.287385  0.249837 -0.788417 -0.484166  0.454116 -0.390626 -0.147465  1.692854 -0.124048 -0.319251 -0.351036  0.625752  0.184101 -0.575827 -0.779026 -0.565858
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.329653  0.529794 -0.331515  0.677400 -0.131367 -0.555976  0.278185 -0.807618 -0.061015  1.134797 -0.002194  0.735074 -0.201729 -0.046618 -0.306439  0.227947
    1  1.058406  0.809351 -1.344075 -0.246845  0.175689 -0.245918  0.350410 -0.492653 -0.494349 -0.492613  0.239605  0.506383  0.320137  0.692919 -0.615415  0.188230
    2  0.185587  1.260174  0.250561 -0.299587  1.242047 -0.277770  0.612673  0.279593 -0.020054 -0.857408 -0.524378 -0.238635  0.308315  0.027083  0.397161 -0.839262
    3  1.383053  0.896572 -0.590994  0.132179 -0.037870 -0.235834 -0.011826  0.426326  0.076197  0.507106 -0.080371 -0.052627  0.483994 -0.217879  0.242060 -0.172769
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.584967  0.036679 -0.297522  0.097387  0.485003 -0.238667  0.350071 -0.361201 -0.267589  0.658243  0.040019 -0.734725 -0.959120 -0.350231  0.195392  0.406448
    1  0.852087 -0.366939 -0.282480  0.265795 -0.002569 -0.844916 -0.104893 -0.690379 -0.224831 -0.746555  1.011040  1.010993 -0.554456  0.348798  0.947264 -0.397369
    2 -0.200723 -0.304258 -0.723998  0.231694  0.869864  1.757347  0.229796  0.930889  0.621773  0.163052 -0.344292 -0.835321 -0.158639  0.378007 -0.990519 -1.116215
    3 -0.100365  0.223015  0.344705 -0.073661 -0.639542  0.149556 -0.734440  0.478714 -0.072013 -0.749257  1.757566  0.081617 -1.066923 -0.352199  0.226350 -0.415477
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.431029 -0.389901 -0.247779  1.057589 -0.214162 -0.377291 -0.831225 -0.075418 -0.452601  0.135656  0.525034 -0.443614 -1.325111  0.056101  0.228472 -0.178676
    1  1.723565  0.278511 -0.016062 -1.201930  0.733769 -1.793392 -0.558585  0.130216 -0.274069  0.038803 -0.863897  0.813440  0.919791  0.494655  0.573373  0.027652
    2  0.879813 -0.209875  0.213537  0.656168  0.473445  0.980684 -0.236595 -0.546739  0.569917  1.428658 -0.737947  0.451539  0.750646  0.683225 -0.826336  0.000961
    3  0.036668  0.604981 -0.078725 -0.471873  0.906546  0.596679 -0.927565  0.293336 -0.211209 -0.397249  0.382137  0.309855 -1.381001 -0.803773 -0.138390  0.917402
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.362992 -0.324788 -0.199897 -0.665556 -0.714832  0.774445  0.666566  0.487252  0.055709  1.294657 -0.629741 -0.634791 -0.727247  0.827464 -0.410294  0.111628
    1 -0.726221 -1.016504  0.375666 -0.016474 -0.293093 -0.453114  0.620295 -0.496051 -0.858718 -0.680479  0.327625  0.234066 -0.865707  0.594118 -1.148261  0.229093
    2  0.585169  0.930731 -0.295074  0.362957  0.147258  0.483375  1.154191 -0.452694 -1.047820 -0.006229 -0.810283 -0.388121  0.672026  0.566222 -1.201632 -0.011228
    3  0.786654  0.380318 -0.605672  0.383441 -0.101051  0.076134 -0.676884 -0.484011  0.056946 -0.020415 -0.457473 -0.753593 -0.350078 -0.780334 -1.821025 -0.593597
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.176822 -2.470878 -0.147898 -0.274015  0.651451  0.796163 -0.673210  0.159244  0.584264  0.825909  0.290970 -0.608795 -0.352897 -0.225098 -0.972970 -0.860101
    1  1.448978 -0.258498 -1.089631  0.271407  0.727970 -0.072183  0.364827  0.583581 -0.054674  0.277727 -0.891726  1.263470 -0.313823  0.171239 -0.670323 -0.375938
    2 -0.203743  0.770183  0.648911 -0.115609 -0.092174  1.593996  0.051193 -0.894547  0.432104 -0.469846 -0.743074  0.249705  0.485517  0.563877 -0.916904 -0.435618
    3  0.720368  0.267263  0.131167  0.399877  0.984353  0.196040 -0.693219  0.515868  0.182996 -0.152200 -0.281154 -0.016874 -0.803759 -0.564830 -0.649379  0.401523
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.308425 -0.539138  0.259385  0.296139 -0.440926  0.288650 -0.458033 -0.505212  0.679706  0.771421  0.926825 -0.857782 -0.079759 -0.741500 -1.539278 -0.015836
    1 -0.062495  0.018222 -0.693836  0.729388  1.335306 -0.897933 -0.791998 -0.767458 -0.063126  0.372156 -0.413532  0.537594  0.143167 -0.032629 -0.408854  0.525611
    2 -0.563814  0.754811  0.150471 -0.752292 -0.196089  0.811713  0.596193 -0.068530  0.573009  1.598698 -0.534681  0.204389  1.164141  0.366169 -0.638497  0.336121
    3  0.018690  0.668563  0.362883 -0.175717  0.777268  0.265388  0.305293  0.003245  0.814834  0.119982  0.105408  0.698817 -0.878940 -1.288623 -0.465074 -0.137698
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.633797 -0.141075  0.139497  0.358901 -0.697078 -0.262024  0.213127  0.320573 -0.346077  1.143742  0.120111 -0.583446  0.061218  0.039754  0.213620 -0.518166
    1  0.244168 -0.542625  0.169464 -0.562253 -0.065920 -1.249914  0.501507 -0.605107 -0.280301  0.933891 -0.708358  0.235844 -0.427313  0.163051 -0.979141  0.109747
    2  0.581454  0.309372 -0.450555  0.661134 -0.098451  0.071689  0.438015 -0.642685  0.419394  1.336107 -0.757630  0.287455  2.246995 -0.247059 -0.576627 -0.452991
    3  0.024407  0.043233 -1.071366 -0.464481 -0.056488  0.196217 -1.077691 -0.171896 -0.610767 -0.282540  0.311062 -1.076181 -0.833077 -0.622530 -0.472522 -0.361622
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.471197  0.088107 -0.383323 -0.223165 -0.469665 -2.372031  0.272508  0.037552  1.050081  1.027021 -0.132061 -0.094782  1.077781  0.878793 -0.239834 -0.230372
    1  0.029179 -0.375825  0.352634  0.852334  0.187841 -0.277456  0.070253  0.059308 -0.135633  0.185454 -0.571119  0.115326 -0.333283 -0.260872 -0.540465  0.015298
    2 -0.107907  0.848008 -0.207849  0.075016 -0.292608 -0.287172  0.107562  1.009907  1.871122  0.219246  0.006427  1.198960  1.336066  0.160271 -1.012163 -1.876254
    3  0.513124  0.250150 -0.362078 -0.456548 -0.820738 -0.722619 -0.163761  0.326078 -0.196407  0.575446  0.202406 -0.401170  0.376073 -0.906169  0.381597 -0.071292
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.467124 -0.984563 -0.257064 -2.412939 -0.639123 -0.345935  0.859396  1.475097 -0.012862  0.375938  0.619530 -0.325561  0.118446  0.458732 -1.207592  0.147781
    1  1.088158  0.810464  0.807675  0.654824  0.703983 -0.603323  0.043721 -0.270394 -0.900159  0.019439 -0.293723  1.303475 -0.680663  0.589663  0.252987 -0.994002
    2  0.292755 -0.045952 -0.128179 -0.426168  1.002135  1.233764  0.672043 -0.698935  0.949020 -0.219086  0.124726  0.918181  1.181413 -0.125518 -0.416058 -1.771814
    3 -0.242158  0.389674 -0.669892 -0.280750 -0.688672  0.363253  0.630064  0.664664  0.136879 -1.421761 -0.212463 -0.382077  0.715042  0.261851 -0.037907  0.047845
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.059226 -0.558207 -0.249727 -0.602460 -0.761082 -0.116131  0.261580  1.301103 -1.791569  0.495411  0.351794  0.675023  0.378143  0.823263 -1.746921  0.506247
    1  0.274707  0.351110 -0.267733  0.665271  1.453197 -0.418145  0.193962 -1.205970 -0.500388  0.785514 -0.261973  0.550507 -1.111312 -0.008571 -0.504501 -0.966200
    2 -0.096769 -0.546517 -0.722104 -1.146778  0.559480  1.752102  0.669178 -0.494542  0.156485  0.845991 -0.670795 -1.131252  0.897950  0.253173 -1.034849 -1.355884
    3 -0.536298  1.018879  0.436880  0.469634 -0.633801  0.954112 -0.318206 -0.859970  0.359073 -0.614701 -0.042932 -0.458950 -1.168430  0.884767 -0.871933  0.298743
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.610655 -1.370669 -0.277214 -0.888282 -0.160174  0.249849  0.726395 -0.309036  0.245068 -0.293404  0.884923 -1.118040 -0.858950 -0.627054 -1.436908 -0.067506
    1  0.547155 -0.337403 -0.414298 -0.212033 -0.145155  0.207373  0.381346  0.100462  0.709092  0.317103 -0.289168  0.348626 -0.340906  1.027436  0.504305 -0.969035
    2 -0.273024 -0.651273 -0.300382 -0.134794 -0.397924  1.016576  0.910840 -0.153315  0.340497 -0.124381 -0.059417 -0.479784  0.712562  0.236867 -1.093192 -0.390130
    3  0.162120  0.305357  0.109529 -0.471995  0.701465 -0.630411  0.013097  0.355996  0.202033  0.387095 -0.159123 -0.428632 -1.473286  0.255073 -0.325924  0.773818
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.858824 -0.177800 -0.352326 -0.744648 -0.964367 -0.167892 -0.971617  1.283839  0.319020  0.239411  0.495465 -0.076289  0.106109  0.246583 -0.913282 -0.194536
    1  0.298962  0.576147  0.073836 -0.247572  0.533223 -1.038875  0.314859 -0.377403 -0.484762 -0.297288 -0.050916  1.660669  0.520465 -0.142441 -0.137322 -1.251222
    2 -0.033523 -0.829220 -0.304602 -0.110416 -0.150082  1.594558  0.274587  0.676428  1.243627  0.277391 -0.565361 -0.421763 -0.036186 -0.461395  0.704446 -0.372949
    3  0.148561  0.996812 -0.108721  0.710904 -1.185101  1.112566 -0.759846  0.024740 -0.183061 -0.134591 -0.500687 -0.171026 -0.172278 -0.232858 -0.280900  0.070325
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.488704 -0.725228  0.173900 -0.830414  0.213070 -0.718127 -0.444598  0.672499 -1.035795  0.015452  0.295015 -0.910450 -0.307357  0.305210 -0.243458  0.940930
    1  1.042728 -0.129669  0.313524 -1.872151 -0.012815 -0.607495  0.371844 -0.861372 -0.658208 -0.640721  0.625611  0.016249 -0.624924 -0.071195  0.438702 -0.223898
    2  0.565551  0.743168  0.227358  0.633667  0.500689  1.503283  0.044266  0.352887  0.546992  0.214653  0.130992 -1.011247 -0.438571 -0.165588 -0.744111 -0.907085
    3  0.286262  0.421306  0.353233 -0.335495 -0.862758 -0.010216 -1.786810  0.390619  0.424009 -0.530579  0.090104 -1.364448 -0.198620 -0.126026  0.187004 -0.110236
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.683050  0.255799  0.749018  0.855960 -0.666782 -0.468342  0.535272  0.089472  0.529222 -0.168698  0.622150 -0.635523  0.240684 -0.716005 -0.665860  1.135421
    1 -0.067731 -0.270443 -0.465293  0.120904  0.368781 -0.819422  1.016821  0.149845  0.338048 -0.305068  0.320750 -0.081365 -0.627034  0.785185 -0.829301  0.136954
    2 -1.385310  0.756914  0.017838  0.404852  0.797507  0.749105  0.096549  0.821897  0.128030 -0.020867 -0.998237 -0.762568 -0.179816  0.874281  1.155173 -0.573110
    3 -0.496411  0.383706 -0.159284  0.536126 -0.037892 -0.341835  0.271312  0.318583  0.227475 -0.699326  0.258262 -0.061990 -0.343047 -0.732064 -0.997765 -1.098356
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.590556 -0.126417 -0.008332 -0.129837 -0.218032  0.165846 -0.376837  0.704665 -0.395220 -0.282819 -0.006145 -0.243167 -1.332027  1.710084 -0.399444  0.197928
    1  0.804329  0.749667  0.707012 -1.197384  0.487677 -1.247303  0.620496 -0.566080 -0.653210  0.731915 -0.119785 -0.021137  0.388589  0.057226 -0.883790 -0.808471
    2  0.709311  0.215274 -0.333312  1.049628 -0.046225 -0.605354  1.996748  0.004057  0.490174  0.132027 -0.014141  1.074840  0.392585 -0.311970 -1.730481 -0.528037
    3 -0.374563 -0.796446 -1.307556 -0.275224 -0.090963  1.311742  0.028888  1.471484  0.982600 -0.028525 -1.000560 -0.926102 -0.157814 -0.307164  0.156203 -0.537111
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  0.401719  0.425182 -0.292917 -0.701397 -0.326698 -0.279441 -0.798718  0.513086  0.320014  0.617820 -0.086840 -0.417280 -1.574415 -0.074147  0.181661 -0.622625
    1  0.125515  0.108487 -0.512970 -0.430991 -0.051894 -2.067564  1.478227 -0.140694 -0.479147 -0.896122  0.062240  0.174992 -0.011013 -0.770407 -0.188411 -0.085206
    2  0.193214  0.273297 -0.967667  0.689172  0.477827  0.713157  0.815838  0.416669  0.088426  0.044879 -0.690439 -0.935450  0.204611  0.335932  0.014221  0.127856
    3 -0.343215  0.089352 -0.411377 -0.590583 -0.172385  0.425989 -1.009057  0.996904 -0.125823 -0.892946  0.006623  0.012985 -1.091260 -0.097457 -0.360760 -0.812595
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0  1.215814 -0.422020  0.019028  0.565088 -0.494546  0.823564 -0.480298  1.053372 -0.271921  0.653942  0.552066 -0.897783 -1.794641  0.332980 -1.265209  1.206126
    1  0.164707 -0.261773  0.114363  0.238533  0.831699 -0.905116  0.107201 -0.855621 -0.653122 -0.311885 -0.033228  0.752663 -0.481584  0.115888 -0.548276 -0.652191
    2  0.416088  0.351545  0.118157 -0.563940  0.611703  0.765792  1.143817 -0.260738  0.213900  0.278640 -0.615235  0.565911  0.454788 -0.064289 -0.472896  0.166102
    3 -0.358502  0.241573 -0.537251 -0.294055  0.697770 -0.024773 -0.013605  1.368784  0.674618 -0.099743 -0.501064 -0.418571  0.113558 -0.607647 -1.413939 -0.550027
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.030005  0.560489  0.692207  0.802441 -1.049383 -1.130618 -0.233082 -0.138613  0.441630  0.025435 -0.203886 -0.245355 -0.877590  0.185146  0.144022 -0.151590
    1  0.886263  0.015151 -0.640962 -0.522613  0.534007 -2.012024  0.963597 -0.149933 -0.269668  0.051612 -0.241905  0.215621  0.793468  0.193036 -0.299267 -0.301879
    2 -0.777996 -1.183819 -0.050823  0.102093  0.829035  1.152996  0.128101  1.013317  1.131896  0.165107 -0.910277 -0.281369  0.560279  0.269381 -0.152854 -1.395591
    3 -0.647185  1.049913 -0.477628  0.617693 -0.168791  0.307628 -0.439353  1.122450 -0.519324 -0.404857  0.109275  1.548271 -1.384960 -0.262460  0.355068  0.332892
             0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15
    0 -0.070381 -0.865201 -1.882663 -0.408366 -0.598919  0.541644  0.638400  0.586414  0.963336  0.899848 -0.811803 -0.428355  0.283824  0.715828 -0.689767 -1.133214
    1  0.163508  0.148884 -0.098002  0.161132 -0.572743 -0.657556 -0.488363 -0.237395 -0.239877  0.452628 -0.079612 -0.221905 -0.779132  0.257819 -0.168416 -1.584344
    2  0.159060 -0.572794 -0.154949  0.059427  0.620815 -0.265221 -0.102914 -0.490620 -1.777336  0.243808 -1.639493  0.448959  0.437344  0.562454 -0.772507  0.010446
    3  0.485404 -0.012398 -1.315031  0.238375 -1.477421 -0.787531 -1.435814 -0.116352  0.708661 -0.874855  0.670505 -0.339437 -0.194216 -0.594283 -0.826917 -0.267774
    torch.Size([4, 16, 4, 16])



```python
# 9，特征提取与融合阶段-多头注意力机制：重塑 Q、K、V 
# 将 Q、K、V 重塑为 [batch_size、num_heads、context_length、head_size] 以进行进一步计算
# 并行计算与高效性：模型的设计还考虑了计算效率。通过将Q、K、V表示为矩阵，并利用矩阵乘法等操作，可以在 GPU 等并行计算设备上进行高效的并行计算。这使得模型能够快速处理大规模的数据，大大提高了训练和推理的速度，使得在实际应用中处理大量文本数据成为可能。
# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]
# The reason is that treat each batch with "num_heads" as its first dimension.
Q = Q.transpose(1, 2) # [4, 4, 16, 16]
K = K.transpose(1, 2) # [4, 4, 16, 16]
V = V.transpose(1, 2) # [4, 4, 16, 16]
```


```python
# 10，特征提取与融合阶段-多头注意力机制：计算注意力分数和缩放
# Calculate the attention score and scale
# Q 和 K 进行乘法的原因：
# 计算注意力得分：如前面所说，Q和K的乘法是为了计算注意力得分。具体来说，Q和K转置相乘的结果表示了Q中每个元素与K中每个元素之间的相似度或关联程度。在计算过程中，Q和K的维度通常是匹配的，这样矩阵乘法可以有效地在所有元素之间进行成对的计算，得到一个注意力得分矩阵。这个矩阵中的每个元素反映了相应Q和K元素之间的相关性，为后续确定注意力权重提供了基础。
# 捕捉长序列依赖关系：Transformer 等大模型设计的一个重要目标是解决长序列数据中的依赖关系问题。在自然语言处理等任务中，一个句子或文档中的不同位置之间可能存在长期的语义依赖关系。通过注意力机制，模型可以直接计算序列中任意两个位置之间的关联，而不受序列长度的限制，从而有效地捕捉长序列中的依赖关系，提高对文本等长序列数据的理解和处理能力。
# 信息筛选与融合：通过Q和K的乘法，模型可以根据计算出的注意力得分来筛选和融合V中的信息。高得分表示对应的Q和K元素之间具有较强的关联，因此在聚合V的信息时，与这些高得分相关的V元素会被赋予更高的权重，从而实现对重要信息的筛选和融合，使得模型能够生成更准确、更有针对性的输出。
attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model // num_heads) # [4, 4, 16, 16] # Scale

# Illustration only
plt.imshow(attention_score[1, 1].detach().cpu().numpy(), "Accent", aspect="auto")
plt.title("Attention(Q @ K)") #plot attention in the first head of the first batch
plt.xlabel(encoding.decode(x_batch[0].tolist()))
plt.ylabel(encoding.decode(x_batch[0].tolist()))
plt.colorbar()
pd.DataFrame(attention_score[0][0].detach().cpu().numpy())
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.827912</td>
      <td>0.872087</td>
      <td>0.805252</td>
      <td>0.638937</td>
      <td>0.403635</td>
      <td>-0.220579</td>
      <td>0.074281</td>
      <td>0.066307</td>
      <td>0.161097</td>
      <td>0.776864</td>
      <td>-0.015753</td>
      <td>0.059171</td>
      <td>-0.167397</td>
      <td>-0.519080</td>
      <td>-0.113654</td>
      <td>0.370162</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.123528</td>
      <td>0.464093</td>
      <td>0.514921</td>
      <td>-0.184883</td>
      <td>0.292599</td>
      <td>-0.127226</td>
      <td>0.205937</td>
      <td>0.207778</td>
      <td>0.602376</td>
      <td>1.016719</td>
      <td>0.158707</td>
      <td>0.074483</td>
      <td>0.526718</td>
      <td>0.460966</td>
      <td>0.633765</td>
      <td>0.534267</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.040624</td>
      <td>0.605709</td>
      <td>0.119166</td>
      <td>-0.133348</td>
      <td>-0.169015</td>
      <td>-0.146091</td>
      <td>0.444743</td>
      <td>0.050206</td>
      <td>0.774002</td>
      <td>0.830742</td>
      <td>-0.216136</td>
      <td>-0.111672</td>
      <td>0.751038</td>
      <td>0.496729</td>
      <td>0.747770</td>
      <td>0.609336</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.900045</td>
      <td>0.596818</td>
      <td>0.359724</td>
      <td>0.836026</td>
      <td>0.074438</td>
      <td>0.652813</td>
      <td>1.057635</td>
      <td>-0.026944</td>
      <td>1.124048</td>
      <td>0.971823</td>
      <td>-0.467323</td>
      <td>-0.247706</td>
      <td>0.002623</td>
      <td>-0.360194</td>
      <td>0.017813</td>
      <td>0.965252</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.061119</td>
      <td>1.158660</td>
      <td>0.633724</td>
      <td>-0.320907</td>
      <td>0.383561</td>
      <td>-0.448183</td>
      <td>-0.238576</td>
      <td>-0.078154</td>
      <td>-0.033891</td>
      <td>0.852194</td>
      <td>0.056048</td>
      <td>0.533376</td>
      <td>0.968321</td>
      <td>0.675573</td>
      <td>0.506570</td>
      <td>0.651593</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.071059</td>
      <td>0.273445</td>
      <td>0.044782</td>
      <td>0.250272</td>
      <td>-0.114290</td>
      <td>0.121239</td>
      <td>0.472959</td>
      <td>0.086112</td>
      <td>-0.077087</td>
      <td>-0.158122</td>
      <td>-0.416886</td>
      <td>0.212225</td>
      <td>0.220158</td>
      <td>-0.492304</td>
      <td>0.175022</td>
      <td>0.505993</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.897961</td>
      <td>1.322749</td>
      <td>0.553525</td>
      <td>0.079147</td>
      <td>0.502846</td>
      <td>-0.506932</td>
      <td>0.223247</td>
      <td>-0.400829</td>
      <td>0.203914</td>
      <td>0.658943</td>
      <td>-0.018270</td>
      <td>0.620022</td>
      <td>0.600703</td>
      <td>0.066074</td>
      <td>0.316328</td>
      <td>0.332746</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.025299</td>
      <td>0.779429</td>
      <td>0.627483</td>
      <td>-0.354743</td>
      <td>0.218848</td>
      <td>-0.055096</td>
      <td>0.297431</td>
      <td>-0.119230</td>
      <td>-0.025749</td>
      <td>0.031660</td>
      <td>0.079259</td>
      <td>0.274235</td>
      <td>0.095197</td>
      <td>-0.625557</td>
      <td>-0.000797</td>
      <td>0.226548</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.529123</td>
      <td>-0.196815</td>
      <td>0.232964</td>
      <td>0.031045</td>
      <td>0.214278</td>
      <td>-0.328799</td>
      <td>-0.685732</td>
      <td>0.060671</td>
      <td>0.237945</td>
      <td>0.642808</td>
      <td>0.532076</td>
      <td>0.191681</td>
      <td>-0.066689</td>
      <td>0.383821</td>
      <td>-0.080174</td>
      <td>0.330104</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.935247</td>
      <td>0.690427</td>
      <td>0.511019</td>
      <td>1.063499</td>
      <td>0.504674</td>
      <td>-0.227777</td>
      <td>-0.035515</td>
      <td>-0.006277</td>
      <td>0.174739</td>
      <td>0.899135</td>
      <td>0.184269</td>
      <td>0.238904</td>
      <td>-0.084467</td>
      <td>-0.414827</td>
      <td>-0.173580</td>
      <td>0.431657</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.467166</td>
      <td>0.532110</td>
      <td>0.238954</td>
      <td>0.085494</td>
      <td>0.207894</td>
      <td>-0.739952</td>
      <td>-0.371635</td>
      <td>-0.087244</td>
      <td>-0.217037</td>
      <td>0.353716</td>
      <td>0.276079</td>
      <td>0.202890</td>
      <td>0.124148</td>
      <td>-0.118437</td>
      <td>-0.063674</td>
      <td>0.186508</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.016316</td>
      <td>-0.238568</td>
      <td>-0.610145</td>
      <td>-0.418894</td>
      <td>-0.609520</td>
      <td>-0.072247</td>
      <td>-0.179780</td>
      <td>-0.085679</td>
      <td>0.529486</td>
      <td>0.309438</td>
      <td>0.002346</td>
      <td>-0.154982</td>
      <td>0.340535</td>
      <td>0.199168</td>
      <td>-0.240800</td>
      <td>0.511496</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.210837</td>
      <td>0.166079</td>
      <td>0.430884</td>
      <td>0.002882</td>
      <td>0.447536</td>
      <td>0.287130</td>
      <td>-0.069169</td>
      <td>0.463648</td>
      <td>-0.251515</td>
      <td>0.225306</td>
      <td>0.302280</td>
      <td>0.252891</td>
      <td>0.182380</td>
      <td>-0.017386</td>
      <td>-0.036945</td>
      <td>0.483606</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-0.131177</td>
      <td>0.249064</td>
      <td>0.690766</td>
      <td>-0.918174</td>
      <td>0.273946</td>
      <td>-0.226493</td>
      <td>-0.467489</td>
      <td>0.787193</td>
      <td>-0.527138</td>
      <td>-0.107953</td>
      <td>0.469084</td>
      <td>0.095217</td>
      <td>0.443433</td>
      <td>0.514454</td>
      <td>0.195352</td>
      <td>0.647130</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.268342</td>
      <td>0.041401</td>
      <td>0.530629</td>
      <td>-0.301306</td>
      <td>0.030230</td>
      <td>-0.376372</td>
      <td>-0.326081</td>
      <td>0.092524</td>
      <td>-0.112595</td>
      <td>-0.274940</td>
      <td>0.107115</td>
      <td>0.171046</td>
      <td>-0.394139</td>
      <td>-0.418468</td>
      <td>-0.190215</td>
      <td>0.442426</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-0.219957</td>
      <td>-0.423728</td>
      <td>0.599078</td>
      <td>-0.366324</td>
      <td>0.452232</td>
      <td>0.868501</td>
      <td>-0.051142</td>
      <td>0.200568</td>
      <td>0.335000</td>
      <td>0.050063</td>
      <td>0.305444</td>
      <td>0.225228</td>
      <td>-0.176068</td>
      <td>0.332432</td>
      <td>0.156352</td>
      <td>0.471813</td>
    </tr>
  </tbody>
</table>
</div>




    
![png](step-by-stepT_files/step-by-stepT_12_1.png)
    



```python
# 11，特征提取与融合阶段-多头注意力机制：掩码 Mask
# Apply Mask to attention scores
# 防止信息泄露（用于自注意力且输入为序列时）：在处理序列数据（如自然语言文本）的自注意力机制中，我们通常不希望一个位置的信息在计算时依赖于它后面位置的信息。例如在文本生成任务中，在生成第 i 个词时，模型只能基于前面 i-1 个词的信息，而不能提前看到后面的词。通过掩码操作可以实现这一点。
attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf'))#[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]

# Illustration only
plt.imshow(attention_score[1, 1].detach().cpu().numpy(), "Accent", aspect="auto")
plt.title("Attention(Q,K)")
plt.xlabel(encoding.decode(x_batch[0].tolist()))
plt.ylabel(encoding.decode(x_batch[0].tolist()))
plt.colorbar()
pd.DataFrame(attention_score[0][0].detach().cpu().numpy())
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.827912</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.123528</td>
      <td>0.464093</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.040624</td>
      <td>0.605709</td>
      <td>0.119166</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.900045</td>
      <td>0.596818</td>
      <td>0.359724</td>
      <td>0.836026</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.061119</td>
      <td>1.158660</td>
      <td>0.633724</td>
      <td>-0.320907</td>
      <td>0.383561</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.071059</td>
      <td>0.273445</td>
      <td>0.044782</td>
      <td>0.250272</td>
      <td>-0.114290</td>
      <td>0.121239</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.897961</td>
      <td>1.322749</td>
      <td>0.553525</td>
      <td>0.079147</td>
      <td>0.502846</td>
      <td>-0.506932</td>
      <td>0.223247</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.025299</td>
      <td>0.779429</td>
      <td>0.627483</td>
      <td>-0.354743</td>
      <td>0.218848</td>
      <td>-0.055096</td>
      <td>0.297431</td>
      <td>-0.119230</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.529123</td>
      <td>-0.196815</td>
      <td>0.232964</td>
      <td>0.031045</td>
      <td>0.214278</td>
      <td>-0.328799</td>
      <td>-0.685732</td>
      <td>0.060671</td>
      <td>0.237945</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.935247</td>
      <td>0.690427</td>
      <td>0.511019</td>
      <td>1.063499</td>
      <td>0.504674</td>
      <td>-0.227777</td>
      <td>-0.035515</td>
      <td>-0.006277</td>
      <td>0.174739</td>
      <td>0.899135</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.467166</td>
      <td>0.532110</td>
      <td>0.238954</td>
      <td>0.085494</td>
      <td>0.207894</td>
      <td>-0.739952</td>
      <td>-0.371635</td>
      <td>-0.087244</td>
      <td>-0.217037</td>
      <td>0.353716</td>
      <td>0.276079</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.016316</td>
      <td>-0.238568</td>
      <td>-0.610145</td>
      <td>-0.418894</td>
      <td>-0.609520</td>
      <td>-0.072247</td>
      <td>-0.179780</td>
      <td>-0.085679</td>
      <td>0.529486</td>
      <td>0.309438</td>
      <td>0.002346</td>
      <td>-0.154982</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.210837</td>
      <td>0.166079</td>
      <td>0.430884</td>
      <td>0.002882</td>
      <td>0.447536</td>
      <td>0.287130</td>
      <td>-0.069169</td>
      <td>0.463648</td>
      <td>-0.251515</td>
      <td>0.225306</td>
      <td>0.302280</td>
      <td>0.252891</td>
      <td>0.182380</td>
      <td>-inf</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-0.131177</td>
      <td>0.249064</td>
      <td>0.690766</td>
      <td>-0.918174</td>
      <td>0.273946</td>
      <td>-0.226493</td>
      <td>-0.467489</td>
      <td>0.787193</td>
      <td>-0.527138</td>
      <td>-0.107953</td>
      <td>0.469084</td>
      <td>0.095217</td>
      <td>0.443433</td>
      <td>0.514454</td>
      <td>-inf</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.268342</td>
      <td>0.041401</td>
      <td>0.530629</td>
      <td>-0.301306</td>
      <td>0.030230</td>
      <td>-0.376372</td>
      <td>-0.326081</td>
      <td>0.092524</td>
      <td>-0.112595</td>
      <td>-0.274940</td>
      <td>0.107115</td>
      <td>0.171046</td>
      <td>-0.394139</td>
      <td>-0.418468</td>
      <td>-0.190215</td>
      <td>-inf</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-0.219957</td>
      <td>-0.423728</td>
      <td>0.599078</td>
      <td>-0.366324</td>
      <td>0.452232</td>
      <td>0.868501</td>
      <td>-0.051142</td>
      <td>0.200568</td>
      <td>0.335000</td>
      <td>0.050063</td>
      <td>0.305444</td>
      <td>0.225228</td>
      <td>-0.176068</td>
      <td>0.332432</td>
      <td>0.156352</td>
      <td>0.471813</td>
    </tr>
  </tbody>
</table>
</div>




    
![png](step-by-stepT_files/step-by-stepT_13_1.png)
    



```python
# 12，特征提取与融合阶段-多头注意力机制：指数归一化 Softmax
# Softmax the attention score
# 将得分转换为概率分布：softmax 函数可以将原始的注意力得分转换为一个概率分布，使得每个位置的注意力权重都在 0 到 1 之间，且所有位置的注意力权重之和为 1。这样就可以明确地表示每个位置在当前注意力计算中的相对重要性。例如，在处理文本时，经过 softmax 后，每个词对应的注意力权重表示了该词在当前语境下对于生成特定输出（如生成下一个词或理解句子语义）的重要程度比例。
# 便于模型学习和优化：将注意力得分转换为概率分布后，更符合模型的学习和优化过程。模型可以通过调整参数来最小化损失函数，而基于概率分布的表示使得模型能够更有效地学习到不同位置的重要性，从而更好地适应各种任务和输入数据。
# 突出重要信息：softmax 函数具有指数运算的特性，会将较大的得分放大，较小的得分缩小。这意味着在原始的注意力得分中，相对较大的得分经过 softmax 后会变得更加突出，对应的位置会获得更高的注意力权重，从而使模型能够更关注输入中的重要信息。例如，在处理图像时，对于包含物体关键特征的区域，其对应的注意力得分经过 softmax 后会得到较高的权重，模型就能更聚焦于这些区域进行特征提取和识别。
attention_score = torch.softmax(attention_score, dim=-1) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]
pd.DataFrame(attention_score[0][0].detach().cpu().numpy())
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.659133</td>
      <td>0.340867</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.488936</td>
      <td>0.316498</td>
      <td>0.194566</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.306845</td>
      <td>0.226584</td>
      <td>0.178756</td>
      <td>0.287816</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.284609</td>
      <td>0.313769</td>
      <td>0.185624</td>
      <td>0.071457</td>
      <td>0.144541</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.159313</td>
      <td>0.195051</td>
      <td>0.155182</td>
      <td>0.190583</td>
      <td>0.132360</td>
      <td>0.167512</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.195804</td>
      <td>0.299437</td>
      <td>0.138751</td>
      <td>0.086341</td>
      <td>0.131894</td>
      <td>0.048049</td>
      <td>0.099724</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.100501</td>
      <td>0.213640</td>
      <td>0.183524</td>
      <td>0.068726</td>
      <td>0.121962</td>
      <td>0.092737</td>
      <td>0.131933</td>
      <td>0.086976</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.176701</td>
      <td>0.085500</td>
      <td>0.131407</td>
      <td>0.107381</td>
      <td>0.128974</td>
      <td>0.074929</td>
      <td>0.052437</td>
      <td>0.110609</td>
      <td>0.132063</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.148425</td>
      <td>0.116194</td>
      <td>0.097110</td>
      <td>0.168735</td>
      <td>0.096496</td>
      <td>0.046389</td>
      <td>0.056222</td>
      <td>0.057891</td>
      <td>0.069378</td>
      <td>0.143160</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.127447</td>
      <td>0.135999</td>
      <td>0.101442</td>
      <td>0.087011</td>
      <td>0.098340</td>
      <td>0.038114</td>
      <td>0.055086</td>
      <td>0.073207</td>
      <td>0.064296</td>
      <td>0.113778</td>
      <td>0.105279</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.091145</td>
      <td>0.070638</td>
      <td>0.048715</td>
      <td>0.058983</td>
      <td>0.048746</td>
      <td>0.083420</td>
      <td>0.074915</td>
      <td>0.082307</td>
      <td>0.152265</td>
      <td>0.122190</td>
      <td>0.089881</td>
      <td>0.076796</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.075991</td>
      <td>0.072665</td>
      <td>0.094695</td>
      <td>0.061723</td>
      <td>0.096286</td>
      <td>0.082016</td>
      <td>0.057433</td>
      <td>0.097850</td>
      <td>0.047859</td>
      <td>0.077099</td>
      <td>0.083268</td>
      <td>0.079255</td>
      <td>0.073859</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.051873</td>
      <td>0.075872</td>
      <td>0.118007</td>
      <td>0.023613</td>
      <td>0.077783</td>
      <td>0.047157</td>
      <td>0.037058</td>
      <td>0.129953</td>
      <td>0.034912</td>
      <td>0.053092</td>
      <td>0.094544</td>
      <td>0.065053</td>
      <td>0.092150</td>
      <td>0.098932</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.055033</td>
      <td>0.075014</td>
      <td>0.122352</td>
      <td>0.053248</td>
      <td>0.074181</td>
      <td>0.049398</td>
      <td>0.051945</td>
      <td>0.078949</td>
      <td>0.064308</td>
      <td>0.054671</td>
      <td>0.080109</td>
      <td>0.085398</td>
      <td>0.048528</td>
      <td>0.047361</td>
      <td>0.059505</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.039804</td>
      <td>0.032466</td>
      <td>0.090289</td>
      <td>0.034385</td>
      <td>0.077958</td>
      <td>0.118207</td>
      <td>0.047124</td>
      <td>0.060613</td>
      <td>0.069334</td>
      <td>0.052143</td>
      <td>0.067315</td>
      <td>0.062126</td>
      <td>0.041590</td>
      <td>0.069156</td>
      <td>0.057991</td>
      <td>0.079499</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 13，特征提取与融合阶段-多头注意力机制：计算 V 注意力
# Calculate the V attention output
# 结合注意力权重与值信息：通过前面的 q*k 计算得到的是注意力得分，经过 softmax 后得到的是归一化的注意力权重，它表示了输入序列中各个位置对于当前计算的重要性程度。而 V 代表了输入数据的特征表示（值向量）。将注意力权重与 V 相乘，能够根据每个位置的重要性程度对其对应的特征进行加权求和，从而得到综合考虑了注意力分布的输出表示。这样可以使模型在生成输出时，更加关注重要位置的特征信息，有效地融合了注意力机制和原始特征信息。
# 实现信息的选择性聚合：不同的位置可能对生成输出具有不同的贡献，通过注意力权重与 V 的相乘，模型能够有选择地聚合来自不同位置的信息。那些具有较高注意力权重的位置对应的 V 中的特征会被更多地保留和传递到输出中，而注意力权重较低的位置对应的特征则会被相对弱化。这种选择性聚合能够帮助模型更好地捕捉输入数据中的关键信息，过滤掉不重要的信息，从而提高模型的性能和泛化能力。
# 生成上下文相关的表示：在自然语言处理等任务中，例如文本生成或机器翻译，每个位置的输出都应该依赖于整个输入序列的上下文信息。通过将注意力权重与 V 相乘，可以根据当前位置与其他位置的注意力关系，动态地生成与上下文相关的表示。这样模型能够更好地理解文本中的语义关系，生成更符合上下文逻辑的输出。例如，在生成一个句子中的某个单词时，模型会根据该单词与句子中其他单词的注意力权重，结合其他单词的特征信息来生成更准确的表示，从而提高生成文本的质量。
print(attention_score.shape) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]
print(V.shape) #[4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]
A = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]
print(A.shape)
```

    torch.Size([4, 4, 16, 16])
    torch.Size([4, 4, 16, 16])
    torch.Size([4, 4, 16, 16])



```python
# 14，特征提取与融合阶段-多头注意力机制：连接多头的注意力
# Concatenate the multi-head attention output
# 多头注意力机制中，将每个头的输出结果进行连接，会把输入分别通过不同的线性层得到多个头的查询（Q）、键（K）、值（V），然后分别计算每个头的注意力输出。在经过注意力计算、加权求和等操作得到每个头的输出结果后，就会进行 “Concatenate” 操作。
# 假设我们有 num_heads 个注意力头，每个头输出的张量形状为 [batch_size, sequence_length, head_dim]（batch_size 是批次大小，sequence_length 是序列长度，head_dim 是每个头的维度）。在连接这些头的输出时，是沿着最后一个维度（即 head_dim 所在维度）进行拼接。拼接后得到的张量形状变为 [batch_size, sequence_length, num_heads * head_dim] 。
A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]
A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]
A.shape
```




    torch.Size([4, 16, 64])




```python
# 15，特征提取与融合阶段-多头注意力机制：定义多头注意力的最终输出（定义输出权重矩阵）
# Define the output weight matrix
# 在计算注意力机制的输出时乘以 Wo（一个线性变换矩阵）而不是直接使用 A（注意力机制的中间输出）用于后续计算，主要有以下几个原因：
# 特征变换与融合：
# A 的输出是经过注意力计算后得到的结果，但它可能还没有被映射到适合后续任务或模型结构的特征空间中。通过乘以 Wo，可以对 A 进行线性变换，将其特征映射到更有利于后续计算和模型学习的空间中。
# 这种线性变换可以融合不同维度的信息。d_model 通常是模型的隐藏层维度，在注意力机制中，不同位置和不同头的信息可能具有不同的特征表示。Wo 可以将这些信息进行加权组合和变换，使得输出能够更好地捕捉到各种语义和句法关系，为后续的任务（如文本生成、分类等）提供更有价值的特征。
# 模型灵活性与拟合能力：
# 引入 Wo 增加了模型的参数数量和灵活性。模型可以通过调整 Wo 的参数来更好地适应不同的任务和数据特点，提高模型的拟合能力。
# 不同的任务可能需要不同的特征表示和变换方式。Wo 可以学习到特定任务所需的特征映射，使得模型能够更准确地完成各种自然语言处理任务，如在情感分类任务中，Wo 可以将注意力输出变换为能够准确反映情感倾向的特征表示。
# 与模型其他部分的兼容性：
# 模型的后续部分可能对输入有特定的要求，例如特定的维度、数据分布等。乘以 Wo 可以将注意力输出调整为与后续层兼容的形式，确保模型能够顺利进行前向传播和反向传播。
# 在 Transformer 等模型架构中，通常会有多个不同的模块和层，Wo 有助于将注意力机制的输出与其他模块（如前馈神经网络层、层归一化层等）进行无缝连接，使整个模型能够协同工作，有效地学习和处理自然语言数据。
Wo = nn.Linear(d_model, d_model)
output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]
print(output.shape)
pd.DataFrame(output[0].detach().cpu().numpy())
```

    torch.Size([4, 16, 64])





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>54</th>
      <th>55</th>
      <th>56</th>
      <th>57</th>
      <th>58</th>
      <th>59</th>
      <th>60</th>
      <th>61</th>
      <th>62</th>
      <th>63</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.064506</td>
      <td>0.451883</td>
      <td>0.126058</td>
      <td>-0.308508</td>
      <td>0.429776</td>
      <td>-0.321017</td>
      <td>0.028085</td>
      <td>0.494516</td>
      <td>0.014582</td>
      <td>-0.300572</td>
      <td>...</td>
      <td>0.401453</td>
      <td>0.703710</td>
      <td>-0.791798</td>
      <td>0.423244</td>
      <td>0.758770</td>
      <td>0.434047</td>
      <td>-0.165581</td>
      <td>-0.510981</td>
      <td>-0.012390</td>
      <td>0.565687</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.319784</td>
      <td>0.270716</td>
      <td>-0.050118</td>
      <td>-0.361192</td>
      <td>0.337563</td>
      <td>-0.242993</td>
      <td>0.135025</td>
      <td>0.104678</td>
      <td>0.128337</td>
      <td>-0.151392</td>
      <td>...</td>
      <td>0.253589</td>
      <td>0.731760</td>
      <td>-0.598175</td>
      <td>0.584505</td>
      <td>0.856153</td>
      <td>0.657069</td>
      <td>-0.114799</td>
      <td>-0.277092</td>
      <td>0.109785</td>
      <td>0.568027</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.142772</td>
      <td>0.471190</td>
      <td>-0.135125</td>
      <td>-0.391241</td>
      <td>0.240273</td>
      <td>-0.426438</td>
      <td>0.191585</td>
      <td>0.249496</td>
      <td>0.114119</td>
      <td>-0.148854</td>
      <td>...</td>
      <td>0.188330</td>
      <td>0.707296</td>
      <td>-0.513143</td>
      <td>0.675920</td>
      <td>0.843663</td>
      <td>0.519398</td>
      <td>0.241321</td>
      <td>-0.254675</td>
      <td>-0.067157</td>
      <td>0.583508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.112123</td>
      <td>0.188070</td>
      <td>-0.195295</td>
      <td>-0.499538</td>
      <td>0.074381</td>
      <td>-0.364622</td>
      <td>0.126506</td>
      <td>0.199517</td>
      <td>0.166654</td>
      <td>-0.116992</td>
      <td>...</td>
      <td>0.050666</td>
      <td>0.629354</td>
      <td>-0.457674</td>
      <td>0.544949</td>
      <td>0.920907</td>
      <td>0.465222</td>
      <td>0.169629</td>
      <td>-0.187357</td>
      <td>0.040276</td>
      <td>0.363001</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.020715</td>
      <td>0.262572</td>
      <td>-0.107539</td>
      <td>-0.486144</td>
      <td>0.098776</td>
      <td>-0.386885</td>
      <td>0.132087</td>
      <td>0.430532</td>
      <td>0.070903</td>
      <td>-0.235981</td>
      <td>...</td>
      <td>0.107817</td>
      <td>0.448574</td>
      <td>-0.550493</td>
      <td>0.264393</td>
      <td>0.794209</td>
      <td>0.431990</td>
      <td>0.282289</td>
      <td>-0.231657</td>
      <td>-0.063665</td>
      <td>0.233252</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.008326</td>
      <td>0.399232</td>
      <td>-0.110658</td>
      <td>-0.406787</td>
      <td>0.053074</td>
      <td>-0.316901</td>
      <td>0.138022</td>
      <td>0.376356</td>
      <td>0.234606</td>
      <td>-0.272379</td>
      <td>...</td>
      <td>0.189062</td>
      <td>0.355173</td>
      <td>-0.501581</td>
      <td>0.171572</td>
      <td>0.708473</td>
      <td>0.260176</td>
      <td>0.336404</td>
      <td>-0.285479</td>
      <td>-0.020590</td>
      <td>0.198434</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.124724</td>
      <td>0.291068</td>
      <td>-0.078329</td>
      <td>-0.373831</td>
      <td>0.082521</td>
      <td>-0.331801</td>
      <td>0.086711</td>
      <td>0.314733</td>
      <td>0.143774</td>
      <td>-0.316908</td>
      <td>...</td>
      <td>0.183282</td>
      <td>0.388413</td>
      <td>-0.533431</td>
      <td>0.296272</td>
      <td>0.681265</td>
      <td>0.266746</td>
      <td>0.207132</td>
      <td>-0.252256</td>
      <td>-0.067646</td>
      <td>0.238893</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.095166</td>
      <td>0.354915</td>
      <td>-0.023955</td>
      <td>-0.282569</td>
      <td>0.061036</td>
      <td>-0.429683</td>
      <td>0.220804</td>
      <td>0.376843</td>
      <td>0.187381</td>
      <td>-0.233587</td>
      <td>...</td>
      <td>0.265134</td>
      <td>0.267361</td>
      <td>-0.426334</td>
      <td>0.250288</td>
      <td>0.574936</td>
      <td>0.035810</td>
      <td>0.334888</td>
      <td>-0.203362</td>
      <td>-0.035591</td>
      <td>0.152928</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.214826</td>
      <td>0.306409</td>
      <td>-0.111830</td>
      <td>-0.316602</td>
      <td>0.210473</td>
      <td>-0.285486</td>
      <td>0.234773</td>
      <td>0.146622</td>
      <td>0.178420</td>
      <td>-0.231031</td>
      <td>...</td>
      <td>0.293020</td>
      <td>0.345010</td>
      <td>-0.355211</td>
      <td>0.474156</td>
      <td>0.595328</td>
      <td>0.373845</td>
      <td>0.198910</td>
      <td>-0.092789</td>
      <td>0.023336</td>
      <td>0.254176</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.150454</td>
      <td>0.300733</td>
      <td>-0.084071</td>
      <td>-0.347423</td>
      <td>0.197580</td>
      <td>-0.288124</td>
      <td>0.055312</td>
      <td>0.199533</td>
      <td>0.170684</td>
      <td>-0.402754</td>
      <td>...</td>
      <td>0.265382</td>
      <td>0.349139</td>
      <td>-0.415729</td>
      <td>0.308192</td>
      <td>0.549358</td>
      <td>0.390737</td>
      <td>0.178057</td>
      <td>-0.118175</td>
      <td>-0.048890</td>
      <td>0.205317</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.183160</td>
      <td>0.410611</td>
      <td>-0.001230</td>
      <td>-0.289932</td>
      <td>0.188518</td>
      <td>-0.295212</td>
      <td>0.119197</td>
      <td>0.188429</td>
      <td>0.064878</td>
      <td>-0.301170</td>
      <td>...</td>
      <td>0.334207</td>
      <td>0.335139</td>
      <td>-0.470068</td>
      <td>0.417071</td>
      <td>0.521629</td>
      <td>0.421272</td>
      <td>0.187883</td>
      <td>-0.144798</td>
      <td>0.070235</td>
      <td>0.236351</td>
    </tr>
    <tr>
      <th>11</th>
      <td>-0.172794</td>
      <td>0.400183</td>
      <td>-0.066224</td>
      <td>-0.222050</td>
      <td>0.287892</td>
      <td>-0.254785</td>
      <td>0.145653</td>
      <td>0.185306</td>
      <td>0.057948</td>
      <td>-0.253772</td>
      <td>...</td>
      <td>0.320497</td>
      <td>0.302853</td>
      <td>-0.354345</td>
      <td>0.398626</td>
      <td>0.416117</td>
      <td>0.424110</td>
      <td>0.158920</td>
      <td>-0.182346</td>
      <td>0.025160</td>
      <td>0.289050</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-0.138616</td>
      <td>0.470225</td>
      <td>-0.054341</td>
      <td>-0.155410</td>
      <td>0.200397</td>
      <td>-0.344928</td>
      <td>0.082469</td>
      <td>0.206519</td>
      <td>0.046299</td>
      <td>-0.360907</td>
      <td>...</td>
      <td>0.346053</td>
      <td>0.227064</td>
      <td>-0.358268</td>
      <td>0.445714</td>
      <td>0.284947</td>
      <td>0.392696</td>
      <td>0.249379</td>
      <td>-0.200285</td>
      <td>-0.064398</td>
      <td>0.272158</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-0.136451</td>
      <td>0.428663</td>
      <td>-0.058041</td>
      <td>-0.134002</td>
      <td>0.139564</td>
      <td>-0.306150</td>
      <td>0.185001</td>
      <td>0.164044</td>
      <td>0.008917</td>
      <td>-0.261845</td>
      <td>...</td>
      <td>0.334006</td>
      <td>0.066729</td>
      <td>-0.433017</td>
      <td>0.403648</td>
      <td>0.305393</td>
      <td>0.318253</td>
      <td>0.263525</td>
      <td>-0.088667</td>
      <td>-0.013552</td>
      <td>0.174142</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.107749</td>
      <td>0.335819</td>
      <td>-0.118002</td>
      <td>-0.153278</td>
      <td>0.118403</td>
      <td>-0.280735</td>
      <td>0.077118</td>
      <td>0.139404</td>
      <td>0.003165</td>
      <td>-0.345072</td>
      <td>...</td>
      <td>0.206117</td>
      <td>0.047577</td>
      <td>-0.357331</td>
      <td>0.362311</td>
      <td>0.195641</td>
      <td>0.355531</td>
      <td>0.281613</td>
      <td>-0.095845</td>
      <td>-0.178509</td>
      <td>0.160478</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-0.090324</td>
      <td>0.319917</td>
      <td>-0.137331</td>
      <td>-0.184051</td>
      <td>0.176805</td>
      <td>-0.253172</td>
      <td>0.153882</td>
      <td>0.214823</td>
      <td>0.040408</td>
      <td>-0.244891</td>
      <td>...</td>
      <td>0.212659</td>
      <td>0.037718</td>
      <td>-0.324449</td>
      <td>0.442058</td>
      <td>0.257269</td>
      <td>0.335858</td>
      <td>0.358707</td>
      <td>-0.101721</td>
      <td>-0.151703</td>
      <td>0.225689</td>
    </tr>
  </tbody>
</table>
<p>16 rows × 64 columns</p>
</div>




```python
# 16，特征提取与融合阶段-残差连接和层归一化：残差连接
# 在 Transformer 等模型中使用残差连接（output = output + X）主要有以下几个重要原因：
# 解决梯度消失问题：在深层神经网络中，随着网络层数的增加，梯度在反向传播过程中可能会逐渐变小，导致前面的层难以学习到有效的参数，这就是梯度消失问题。残差连接提供了一条直接的梯度传播路径，使得梯度可以更容易地从后面的层传递到前面的层，从而缓解梯度消失问题，让模型能够更有效地训练深层网络。
# 加快训练速度：由于残差连接有助于梯度的传播，模型在训练过程中可以更快地收敛，减少训练时间和计算资源的消耗。它使得模型能够更容易地学习到恒等映射，即当模型已经学习到较好的特征表示时，残差连接可以让模型直接传递这些特征，而不需要重新学习，从而加快了训练速度。
# 提高模型性能：通过残差连接，模型可以更好地利用输入信息，保留原始输入中的重要特征。这有助于模型学习到更复杂的函数，提高模型的表达能力和泛化能力，从而在各种自然语言处理任务中取得更好的性能，如机器翻译、文本分类、问答系统等。
# 防止过拟合：残差连接可以在一定程度上增加模型的鲁棒性，减少过拟合的风险。因为它允许模型在训练过程中更容易地保留一些原始信息，避免模型过度拟合训练数据中的噪声和细节，使得模型能够更好地适应新的未知数据。#
# Add residual connection
output = output + X
```


```python
# 17，特征提取与融合阶段--残差连接和层归一化：层归一化
# Add Layer Normalization
# 在 Transformer 等模型中，LayerNorm（层归一化）具有重要意义，主要体现在以下几个方面：
# 加速模型收敛：在神经网络训练过程中，每层输入的分布会随着训练的进行而发生变化，这会导致模型训练难度增加，收敛速度变慢，这种现象被称为 “内部协变量偏移”。层归一化通过对每个样本的特征维度进行归一化，将其均值调整为 0，方差调整为 1，使得每层的输入分布保持相对稳定，从而减少了内部协变量偏移的影响，加快了模型的收敛速度。
# 提高模型泛化能力：层归一化可以使模型对输入数据的变化更加鲁棒，减少模型对特定数据分布的依赖。它能够在一定程度上避免模型过拟合，提高模型在不同数据集上的泛化能力。因为归一化操作使得模型在训练过程中能够看到更广泛的数据分布，从而学习到更具通用性的特征表示。
# 缓解梯度消失和爆炸问题：通过归一化输入数据，层归一化可以使数据的分布更加合理，避免某些特征维度的值过大或过小，从而使得梯度在反向传播过程中更加稳定，缓解了梯度消失和爆炸问题。这有助于模型更好地学习到深层的特征，提高模型的性能。
# 允许使用更大的学习率：由于层归一化能够稳定输入数据的分布，使得模型在训练过程中对学习率的变化更加不敏感，因此可以使用更大的学习率进行训练，进一步加快模型的收敛速度，减少训练时间和计算资源的消耗。
# 对于 d_model 维度的层归一化，它是针对模型中每个样本的 d_model 维特征向量进行操作的，确保这些特征向量在每个维度上都具有相似的分布，从而为后续的计算提供稳定、标准化的输入，有助于提高整个模型的性能和训练效率。
layer_norm = nn.LayerNorm(d_model)
output_layernorm = layer_norm(output)
```


```python
# 18，特征提取与融合阶段-前馈神经网络
# Define Feed Forward Network
# 在 Transformer 等模型中设置前馈神经网络（Feed Forward Network，FFN）有以下重要原因：
# 增强模型的表达能力：
# 前馈神经网络可以对输入进行非线性变换，能够学习到更复杂的函数关系。尽管注意力机制能够有效地捕捉文本中的长期依赖关系，但它本身主要是对输入进行加权求和等线性操作，对于一些复杂的语义和句法关系的建模能力有限。前馈神经网络通过引入非线性激活函数（如 ReLU 等），可以将注意力机制输出的特征进行进一步的加工和组合，从而增强模型对各种语言现象的表达能力，例如可以更好地处理句子中的嵌套结构、语义角色标注等复杂任务。
# 融合局部信息：
# 注意力机制通常关注的是全局的依赖关系，而前馈神经网络可以在局部范围内对特征进行融合和处理。它可以将每个位置的特征与周围的局部特征进行组合，捕捉到一些局部的上下文信息，这对于理解文本中的一些局部语义和语法结构非常有帮助。例如，在处理一个短语或一个句子片段时，前馈神经网络可以利用局部的词序和词汇信息来更好地理解其含义，从而与注意力机制捕捉的全局信息相互补充，使模型对文本的理解更加全面和准确。
output = nn.Linear(d_model, d_model * 4)(output_layernorm)
output = nn.ReLU()(output)
output = nn.Linear(d_model * 4, d_model)(output)
output = torch.dropout(output, p=dropout, train=True)

```


```python
# 19，特征提取与融合阶段-残差连接和层归一化：添加最后一个残差连接和层规一化
# Add residual connection & layerNorm (last time in a Transformer block)
# 在 Transformer 等模型中，添加最后一个残差连接和层规范化有以下重要作用：
# 残差连接的作用：
# 信息传递与保留：残差连接能够将前一层的输入直接传递到后面的层，确保在模型的深层结构中，原始输入信息不会在传播过程中丢失或被过度变换。在经过多个层的复杂计算后，原始输入中的一些关键信息可能会被弱化，而残差连接提供了一条 “捷径”，让这些信息可以直接参与到后续的计算中，有助于模型更好地利用输入信息，提高模型的性能。
# 缓解梯度消失和爆炸：与前面提到的残差连接作用类似，最后一个残差连接也有助于在反向传播过程中更好地传递梯度。当模型层数较多时，如果没有残差连接，梯度在传播过程中可能会逐渐消失或爆炸，导致模型难以训练。残差连接提供了直接的梯度传播路径，使得梯度能够更顺畅地从输出层传递到输入层，从而缓解了梯度消失和爆炸问题，让模型能够更有效地进行训练。
# 层规范化的作用：
# 数据标准化：层规范化对输入数据进行标准化处理，将其均值调整为 0，方差调整为 1。这有助于将数据分布在一个合理的范围内，避免数据的某些维度过大或过小，使得模型对不同特征维度的敏感度更加一致。在经过多个层的计算后，数据的分布可能会变得不稳定，层规范化可以将数据重新拉回到一个相对稳定的分布状态，为后续的计算提供更稳定的输入。
# 提高模型泛化能力：通过对数据进行规范化，层规范化可以减少模型对数据分布变化的敏感性，提高模型的鲁棒性和泛化能力。它使得模型在面对不同的输入数据时，能够更加稳定地输出结果，不容易受到数据微小变化的影响。同时，层规范化也有助于防止模型过拟合，因为它可以减少数据中的噪声和异常值对模型训练的干扰，让模型学习到更具一般性的特征表示。
# 添加最后一个残差连接和层规范化可以进一步优化模型的性能，提高模型的稳定性和泛化能力，使得模型能够更好地处理各种自然语言处理任务。
output = output + output_layernorm
# Add Layer Normalization
layer_norm = nn.LayerNorm(d_model)
output = layer_norm(output)
print(output.shape)
```

    torch.Size([4, 16, 64])



```python
# 20，特征提取与融合阶段：重复多头注意力机制、残差连接和层归一化、前馈神经网络
# Until here, we finished a Transformer block,
# We actually should pack the above Transformer block code into a call and 
# repeat the steps for num_layers times
# but this jupyter notebook is purely for illustration purpose, so we'll skip it:
# for _ in range(num_layers):
#   do loop for each transformer block
```


```python
# 21，输出生成阶段：应用最后一个线性层来得到我们的 logits
# Apply the final linear layer to get the logits
# 在自然语言处理模型中，通过nn.Linear(d_model, max_token_value)(output)得到logits主要有以下原因：
# 维度变换与适配：
# d_model 通常是模型中特征向量的维度，而 max_token_value 表示词汇表的大小或可能的输出类别数量。线性变换 nn.Linear(d_model, max_token_value) 将模型输出的 d_model 维特征向量映射到 max_token_value 维的空间中，使得模型的输出维度与词汇表或输出类别的数量相匹配，为后续的分类或生成任务做准备。例如，在文本生成任务中，需要为每个可能的生成 token 计算一个得分，通过这种线性变换就可以将模型的特征表示转换为与词汇表大小一致的维度，每个维度对应一个 token 的得分。
# 生成预测分数：
# 得到的 logits 可以看作是对每个类别（token）的未归一化预测分数。这些分数反映了模型对于输入数据属于每个类别的可能性大小。在分类任务中，可以根据这些分数进行进一步的处理，如使用 softmax 函数将其转换为概率分布，从而得到每个类别被预测的概率。在生成任务中，也可以根据 logits 来选择生成的 token，例如选择得分最高的 token 作为生成结果，或者按照一定的概率分布进行采样来生成 token。
# 模型训练与优化：
# 在训练过程中，logits 是计算损失函数的重要输入。通过将模型预测的 logits 与真实标签进行比较，可以计算出损失值，如交叉熵损失等，然后利用反向传播算法来更新模型的参数，使得模型能够不断调整预测结果，以最小化损失函数，从而提高模型的准确性和性能。
# logits 的计算是模型中非常重要的一步，它将模型的特征表示转换为可用于分类、生成和模型训练的预测分数，是连接模型内部计算和实际任务需求的关键环节。
logits = nn.Linear(d_model, max_token_value+1)(output)
pd.DataFrame(logits[0].detach().cpu().numpy())
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>100060</th>
      <th>100061</th>
      <th>100062</th>
      <th>100063</th>
      <th>100064</th>
      <th>100065</th>
      <th>100066</th>
      <th>100067</th>
      <th>100068</th>
      <th>100069</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.151764</td>
      <td>-0.138867</td>
      <td>-0.942958</td>
      <td>-0.355769</td>
      <td>0.120958</td>
      <td>-0.019493</td>
      <td>-0.985039</td>
      <td>-0.595256</td>
      <td>-0.274413</td>
      <td>-0.415326</td>
      <td>...</td>
      <td>0.035935</td>
      <td>0.195056</td>
      <td>0.714029</td>
      <td>0.057430</td>
      <td>0.845246</td>
      <td>-0.640783</td>
      <td>0.091546</td>
      <td>-0.408806</td>
      <td>-0.191840</td>
      <td>-0.015684</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.152344</td>
      <td>0.521729</td>
      <td>-1.121775</td>
      <td>-0.537063</td>
      <td>-0.250971</td>
      <td>-0.099949</td>
      <td>-0.275567</td>
      <td>-1.067943</td>
      <td>0.374539</td>
      <td>-0.539804</td>
      <td>...</td>
      <td>0.624783</td>
      <td>0.048131</td>
      <td>-0.694008</td>
      <td>-0.895768</td>
      <td>0.732693</td>
      <td>-0.710512</td>
      <td>0.545159</td>
      <td>0.595501</td>
      <td>-0.359785</td>
      <td>0.808579</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.163774</td>
      <td>0.329005</td>
      <td>-0.335850</td>
      <td>-0.324920</td>
      <td>-0.111924</td>
      <td>-0.604700</td>
      <td>0.194806</td>
      <td>-0.727899</td>
      <td>0.285325</td>
      <td>-1.056179</td>
      <td>...</td>
      <td>0.188514</td>
      <td>0.313076</td>
      <td>-0.325933</td>
      <td>-0.482292</td>
      <td>0.652267</td>
      <td>-0.295093</td>
      <td>-0.194476</td>
      <td>-0.445205</td>
      <td>-0.628694</td>
      <td>0.259318</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.059175</td>
      <td>0.317208</td>
      <td>0.151746</td>
      <td>0.198112</td>
      <td>-0.475986</td>
      <td>0.184620</td>
      <td>-0.923758</td>
      <td>-0.826276</td>
      <td>-1.389871</td>
      <td>-1.488968</td>
      <td>...</td>
      <td>-0.829640</td>
      <td>0.911820</td>
      <td>-0.049544</td>
      <td>-0.765915</td>
      <td>1.190440</td>
      <td>-0.410919</td>
      <td>0.457360</td>
      <td>1.183785</td>
      <td>-0.142738</td>
      <td>0.731109</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.203493</td>
      <td>-0.297255</td>
      <td>-0.255880</td>
      <td>-0.881310</td>
      <td>-0.029223</td>
      <td>-0.673268</td>
      <td>0.062755</td>
      <td>-0.512842</td>
      <td>0.194643</td>
      <td>-0.216917</td>
      <td>...</td>
      <td>-0.166174</td>
      <td>0.046733</td>
      <td>-0.812436</td>
      <td>1.617657</td>
      <td>0.356186</td>
      <td>-0.563219</td>
      <td>1.082209</td>
      <td>-1.476452</td>
      <td>-1.098764</td>
      <td>0.508759</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.532292</td>
      <td>-0.151126</td>
      <td>-1.302914</td>
      <td>0.088134</td>
      <td>-0.593736</td>
      <td>-0.514214</td>
      <td>-0.746215</td>
      <td>-0.234466</td>
      <td>0.356030</td>
      <td>-0.717981</td>
      <td>...</td>
      <td>-0.649706</td>
      <td>0.552178</td>
      <td>0.513879</td>
      <td>0.171119</td>
      <td>-0.232306</td>
      <td>-0.202599</td>
      <td>1.271086</td>
      <td>-0.397143</td>
      <td>0.241443</td>
      <td>0.621223</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.061069</td>
      <td>0.061822</td>
      <td>-0.780232</td>
      <td>0.033775</td>
      <td>0.493742</td>
      <td>-0.188903</td>
      <td>-1.241413</td>
      <td>-0.740801</td>
      <td>-0.637103</td>
      <td>-0.528444</td>
      <td>...</td>
      <td>-0.468431</td>
      <td>0.478140</td>
      <td>0.426217</td>
      <td>-0.507422</td>
      <td>0.130201</td>
      <td>-0.550591</td>
      <td>0.389341</td>
      <td>-0.341393</td>
      <td>0.475995</td>
      <td>-0.193076</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.656042</td>
      <td>0.297650</td>
      <td>0.133958</td>
      <td>0.173233</td>
      <td>-0.281520</td>
      <td>-0.073458</td>
      <td>-0.582138</td>
      <td>0.420799</td>
      <td>0.526054</td>
      <td>0.233135</td>
      <td>...</td>
      <td>0.244255</td>
      <td>0.253873</td>
      <td>-0.222831</td>
      <td>0.014127</td>
      <td>0.250771</td>
      <td>-0.162268</td>
      <td>1.170713</td>
      <td>0.707391</td>
      <td>0.074375</td>
      <td>0.437575</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.241402</td>
      <td>-0.129030</td>
      <td>-0.521567</td>
      <td>0.174383</td>
      <td>0.592734</td>
      <td>-0.216198</td>
      <td>-0.862083</td>
      <td>0.077348</td>
      <td>-0.410971</td>
      <td>-0.838186</td>
      <td>...</td>
      <td>-1.227130</td>
      <td>-0.084385</td>
      <td>0.661095</td>
      <td>-0.146697</td>
      <td>0.385814</td>
      <td>-0.346445</td>
      <td>0.140659</td>
      <td>-0.355870</td>
      <td>1.617523</td>
      <td>0.017401</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.148204</td>
      <td>-0.088879</td>
      <td>-0.644473</td>
      <td>-0.505158</td>
      <td>0.598318</td>
      <td>-0.625066</td>
      <td>-0.978184</td>
      <td>-0.081975</td>
      <td>-0.418040</td>
      <td>-0.484153</td>
      <td>...</td>
      <td>0.025702</td>
      <td>0.224775</td>
      <td>0.739608</td>
      <td>0.510721</td>
      <td>0.704796</td>
      <td>-0.762244</td>
      <td>0.193956</td>
      <td>-0.841684</td>
      <td>-0.158762</td>
      <td>-0.145742</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.042756</td>
      <td>-0.544063</td>
      <td>0.361202</td>
      <td>-0.876234</td>
      <td>0.244055</td>
      <td>-0.727155</td>
      <td>-0.126128</td>
      <td>0.082008</td>
      <td>0.921980</td>
      <td>-0.250840</td>
      <td>...</td>
      <td>-0.182311</td>
      <td>0.611349</td>
      <td>-0.155162</td>
      <td>1.257192</td>
      <td>-0.003148</td>
      <td>-0.914570</td>
      <td>0.766338</td>
      <td>-0.837840</td>
      <td>0.133556</td>
      <td>-0.149257</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.055674</td>
      <td>-0.363830</td>
      <td>0.030649</td>
      <td>-0.543012</td>
      <td>-0.004213</td>
      <td>-0.537952</td>
      <td>0.784556</td>
      <td>-0.032552</td>
      <td>-0.054321</td>
      <td>-0.995863</td>
      <td>...</td>
      <td>-0.480730</td>
      <td>0.305459</td>
      <td>-0.434508</td>
      <td>-0.684696</td>
      <td>-0.703986</td>
      <td>0.175240</td>
      <td>0.132505</td>
      <td>-1.186731</td>
      <td>0.130202</td>
      <td>0.385154</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.501703</td>
      <td>-1.228627</td>
      <td>-1.299516</td>
      <td>-0.611446</td>
      <td>1.106983</td>
      <td>-0.995603</td>
      <td>-0.320499</td>
      <td>-1.062071</td>
      <td>0.479815</td>
      <td>0.738366</td>
      <td>...</td>
      <td>-0.332079</td>
      <td>0.881173</td>
      <td>0.094492</td>
      <td>-0.284838</td>
      <td>-1.137060</td>
      <td>-0.699440</td>
      <td>0.539316</td>
      <td>-1.508222</td>
      <td>0.802151</td>
      <td>-0.538308</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.690691</td>
      <td>-0.339400</td>
      <td>-0.283244</td>
      <td>-0.483147</td>
      <td>0.985240</td>
      <td>-0.703993</td>
      <td>-0.188205</td>
      <td>0.258862</td>
      <td>0.471739</td>
      <td>0.193974</td>
      <td>...</td>
      <td>-0.219407</td>
      <td>0.678628</td>
      <td>-0.752596</td>
      <td>0.156606</td>
      <td>-0.730574</td>
      <td>-0.291162</td>
      <td>0.482393</td>
      <td>-1.234015</td>
      <td>0.199265</td>
      <td>0.306926</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.766174</td>
      <td>-1.270677</td>
      <td>-0.908114</td>
      <td>-0.192428</td>
      <td>1.097895</td>
      <td>-0.788984</td>
      <td>-0.176648</td>
      <td>0.563787</td>
      <td>0.124659</td>
      <td>-0.347454</td>
      <td>...</td>
      <td>-0.040482</td>
      <td>0.429994</td>
      <td>0.560528</td>
      <td>0.345786</td>
      <td>-0.212589</td>
      <td>-1.009258</td>
      <td>-0.413055</td>
      <td>-1.563507</td>
      <td>-0.458435</td>
      <td>-0.656041</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.046377</td>
      <td>-0.572218</td>
      <td>-1.237437</td>
      <td>-0.710016</td>
      <td>0.403219</td>
      <td>-0.400054</td>
      <td>0.304650</td>
      <td>-0.584162</td>
      <td>0.465965</td>
      <td>-0.446406</td>
      <td>...</td>
      <td>1.424351</td>
      <td>-0.253294</td>
      <td>0.256185</td>
      <td>-0.494801</td>
      <td>-0.198137</td>
      <td>-0.118498</td>
      <td>-0.296561</td>
      <td>-0.421304</td>
      <td>-0.035740</td>
      <td>-0.558273</td>
    </tr>
  </tbody>
</table>
<p>16 rows × 100070 columns</p>
</div>




```python
# 22，输出生成阶段：softmax logits 以获得每个 token 的概率
# Get the probabilities 
# 我们在这里得到的是一个巨大的矩阵，形状为 [16， 100069]，它是整个词汇表中每个标记的概率
# 在自然语言处理任务中，使用torch.softmax(logits, dim=-1)将logits转换为概率分布有以下几个重要原因：
# 表示预测的不确定性：
# 提供概率信息：通过softmax函数得到的概率分布可以明确地表示模型对每个可能结果的预测概率。例如在文本分类任务中，每个类别都有一个对应的概率值，这让我们不仅能知道模型预测的最可能类别，还能了解到其他类别的可能性大小，从而更全面地评估模型的预测结果，量化模型对于不同输出的置信程度。
# 处理多类别问题：在许多自然语言处理任务中，输出往往是多类别分类或多选项生成问题。softmax函数将logits转换为一个概率分布，使得所有可能的类别概率之和为 1，这样就可以方便地处理多个类别之间的关系，模型可以根据这个概率分布来做出决策，例如选择概率最高的类别作为最终输出，或者按照概率进行采样来生成文本等。
# 便于计算损失函数：
# 在模型训练过程中，通常使用基于概率的损失函数，如交叉熵损失。将logits转换为概率分布后，可以很方便地与真实标签的概率分布（通常是独热编码形式）进行比较，计算出损失值，以此来衡量模型预测结果与真实结果之间的差异，进而通过反向传播算法更新模型参数，使模型的预测结果逐渐接近真实结果，优化模型的性能。
# 符合概率模型的假设：
#许多自然语言处理模型在理论上是基于概率模型构建的，将输出表示为概率分布符合这些模型的数学基础和假设。例如，在语言生成任务中，我们希望模型生成的文本是基于一定的概率分布，符合语言的自然规律，即某些词语或句子在特定语境下出现的概率更高，通过softmax得到的概率分布可以更好地模拟这种自然语言的概率特性。
# torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss
# but for illustration purpose, we'll use torch.softmax here
probabilities = torch.softmax(logits, dim=-1)
pd.DataFrame(probabilities[0].detach().cpu().numpy())

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>100060</th>
      <th>100061</th>
      <th>100062</th>
      <th>100063</th>
      <th>100064</th>
      <th>100065</th>
      <th>100066</th>
      <th>100067</th>
      <th>100068</th>
      <th>100069</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000007</td>
      <td>0.000007</td>
      <td>0.000003</td>
      <td>0.000006</td>
      <td>0.000010</td>
      <td>0.000008</td>
      <td>0.000003</td>
      <td>0.000005</td>
      <td>0.000006</td>
      <td>0.000006</td>
      <td>...</td>
      <td>0.000009</td>
      <td>0.000010</td>
      <td>0.000017</td>
      <td>0.000009</td>
      <td>0.000020</td>
      <td>0.000004</td>
      <td>0.000009</td>
      <td>0.000006</td>
      <td>0.000007</td>
      <td>0.000008</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000007</td>
      <td>0.000014</td>
      <td>0.000003</td>
      <td>0.000005</td>
      <td>0.000007</td>
      <td>0.000008</td>
      <td>0.000006</td>
      <td>0.000003</td>
      <td>0.000012</td>
      <td>0.000005</td>
      <td>...</td>
      <td>0.000016</td>
      <td>0.000009</td>
      <td>0.000004</td>
      <td>0.000003</td>
      <td>0.000018</td>
      <td>0.000004</td>
      <td>0.000015</td>
      <td>0.000015</td>
      <td>0.000006</td>
      <td>0.000019</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000007</td>
      <td>0.000012</td>
      <td>0.000006</td>
      <td>0.000006</td>
      <td>0.000008</td>
      <td>0.000005</td>
      <td>0.000010</td>
      <td>0.000004</td>
      <td>0.000011</td>
      <td>0.000003</td>
      <td>...</td>
      <td>0.000010</td>
      <td>0.000012</td>
      <td>0.000006</td>
      <td>0.000005</td>
      <td>0.000016</td>
      <td>0.000006</td>
      <td>0.000007</td>
      <td>0.000005</td>
      <td>0.000004</td>
      <td>0.000011</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000008</td>
      <td>0.000012</td>
      <td>0.000010</td>
      <td>0.000010</td>
      <td>0.000005</td>
      <td>0.000010</td>
      <td>0.000003</td>
      <td>0.000004</td>
      <td>0.000002</td>
      <td>0.000002</td>
      <td>...</td>
      <td>0.000004</td>
      <td>0.000021</td>
      <td>0.000008</td>
      <td>0.000004</td>
      <td>0.000028</td>
      <td>0.000006</td>
      <td>0.000013</td>
      <td>0.000028</td>
      <td>0.000007</td>
      <td>0.000018</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000007</td>
      <td>0.000006</td>
      <td>0.000007</td>
      <td>0.000004</td>
      <td>0.000008</td>
      <td>0.000004</td>
      <td>0.000009</td>
      <td>0.000005</td>
      <td>0.000010</td>
      <td>0.000007</td>
      <td>...</td>
      <td>0.000007</td>
      <td>0.000009</td>
      <td>0.000004</td>
      <td>0.000043</td>
      <td>0.000012</td>
      <td>0.000005</td>
      <td>0.000025</td>
      <td>0.000002</td>
      <td>0.000003</td>
      <td>0.000014</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000005</td>
      <td>0.000007</td>
      <td>0.000002</td>
      <td>0.000009</td>
      <td>0.000005</td>
      <td>0.000005</td>
      <td>0.000004</td>
      <td>0.000007</td>
      <td>0.000012</td>
      <td>0.000004</td>
      <td>...</td>
      <td>0.000004</td>
      <td>0.000015</td>
      <td>0.000014</td>
      <td>0.000010</td>
      <td>0.000007</td>
      <td>0.000007</td>
      <td>0.000030</td>
      <td>0.000006</td>
      <td>0.000011</td>
      <td>0.000016</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000009</td>
      <td>0.000009</td>
      <td>0.000004</td>
      <td>0.000009</td>
      <td>0.000014</td>
      <td>0.000007</td>
      <td>0.000002</td>
      <td>0.000004</td>
      <td>0.000004</td>
      <td>0.000005</td>
      <td>...</td>
      <td>0.000005</td>
      <td>0.000014</td>
      <td>0.000013</td>
      <td>0.000005</td>
      <td>0.000010</td>
      <td>0.000005</td>
      <td>0.000012</td>
      <td>0.000006</td>
      <td>0.000014</td>
      <td>0.000007</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.000016</td>
      <td>0.000011</td>
      <td>0.000010</td>
      <td>0.000010</td>
      <td>0.000006</td>
      <td>0.000008</td>
      <td>0.000005</td>
      <td>0.000013</td>
      <td>0.000014</td>
      <td>0.000011</td>
      <td>...</td>
      <td>0.000011</td>
      <td>0.000011</td>
      <td>0.000007</td>
      <td>0.000009</td>
      <td>0.000011</td>
      <td>0.000007</td>
      <td>0.000027</td>
      <td>0.000017</td>
      <td>0.000009</td>
      <td>0.000013</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.000011</td>
      <td>0.000007</td>
      <td>0.000005</td>
      <td>0.000010</td>
      <td>0.000015</td>
      <td>0.000007</td>
      <td>0.000004</td>
      <td>0.000009</td>
      <td>0.000006</td>
      <td>0.000004</td>
      <td>...</td>
      <td>0.000002</td>
      <td>0.000008</td>
      <td>0.000016</td>
      <td>0.000007</td>
      <td>0.000012</td>
      <td>0.000006</td>
      <td>0.000010</td>
      <td>0.000006</td>
      <td>0.000042</td>
      <td>0.000009</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.000010</td>
      <td>0.000008</td>
      <td>0.000004</td>
      <td>0.000005</td>
      <td>0.000015</td>
      <td>0.000005</td>
      <td>0.000003</td>
      <td>0.000008</td>
      <td>0.000006</td>
      <td>0.000005</td>
      <td>...</td>
      <td>0.000009</td>
      <td>0.000011</td>
      <td>0.000018</td>
      <td>0.000014</td>
      <td>0.000017</td>
      <td>0.000004</td>
      <td>0.000010</td>
      <td>0.000004</td>
      <td>0.000007</td>
      <td>0.000007</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.000009</td>
      <td>0.000005</td>
      <td>0.000012</td>
      <td>0.000004</td>
      <td>0.000011</td>
      <td>0.000004</td>
      <td>0.000007</td>
      <td>0.000009</td>
      <td>0.000021</td>
      <td>0.000007</td>
      <td>...</td>
      <td>0.000007</td>
      <td>0.000016</td>
      <td>0.000007</td>
      <td>0.000030</td>
      <td>0.000008</td>
      <td>0.000003</td>
      <td>0.000018</td>
      <td>0.000004</td>
      <td>0.000010</td>
      <td>0.000007</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.000009</td>
      <td>0.000006</td>
      <td>0.000009</td>
      <td>0.000005</td>
      <td>0.000008</td>
      <td>0.000005</td>
      <td>0.000019</td>
      <td>0.000008</td>
      <td>0.000008</td>
      <td>0.000003</td>
      <td>...</td>
      <td>0.000005</td>
      <td>0.000011</td>
      <td>0.000005</td>
      <td>0.000004</td>
      <td>0.000004</td>
      <td>0.000010</td>
      <td>0.000010</td>
      <td>0.000003</td>
      <td>0.000010</td>
      <td>0.000012</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.000014</td>
      <td>0.000002</td>
      <td>0.000002</td>
      <td>0.000005</td>
      <td>0.000026</td>
      <td>0.000003</td>
      <td>0.000006</td>
      <td>0.000003</td>
      <td>0.000014</td>
      <td>0.000018</td>
      <td>...</td>
      <td>0.000006</td>
      <td>0.000020</td>
      <td>0.000009</td>
      <td>0.000006</td>
      <td>0.000003</td>
      <td>0.000004</td>
      <td>0.000014</td>
      <td>0.000002</td>
      <td>0.000019</td>
      <td>0.000005</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.000017</td>
      <td>0.000006</td>
      <td>0.000006</td>
      <td>0.000005</td>
      <td>0.000023</td>
      <td>0.000004</td>
      <td>0.000007</td>
      <td>0.000011</td>
      <td>0.000014</td>
      <td>0.000010</td>
      <td>...</td>
      <td>0.000007</td>
      <td>0.000017</td>
      <td>0.000004</td>
      <td>0.000010</td>
      <td>0.000004</td>
      <td>0.000006</td>
      <td>0.000014</td>
      <td>0.000002</td>
      <td>0.000010</td>
      <td>0.000012</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.000004</td>
      <td>0.000002</td>
      <td>0.000003</td>
      <td>0.000007</td>
      <td>0.000025</td>
      <td>0.000004</td>
      <td>0.000007</td>
      <td>0.000015</td>
      <td>0.000010</td>
      <td>0.000006</td>
      <td>...</td>
      <td>0.000008</td>
      <td>0.000013</td>
      <td>0.000015</td>
      <td>0.000012</td>
      <td>0.000007</td>
      <td>0.000003</td>
      <td>0.000006</td>
      <td>0.000002</td>
      <td>0.000005</td>
      <td>0.000004</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.000009</td>
      <td>0.000005</td>
      <td>0.000002</td>
      <td>0.000004</td>
      <td>0.000013</td>
      <td>0.000006</td>
      <td>0.000011</td>
      <td>0.000005</td>
      <td>0.000013</td>
      <td>0.000005</td>
      <td>...</td>
      <td>0.000035</td>
      <td>0.000007</td>
      <td>0.000011</td>
      <td>0.000005</td>
      <td>0.000007</td>
      <td>0.000008</td>
      <td>0.000006</td>
      <td>0.000006</td>
      <td>0.000008</td>
      <td>0.000005</td>
    </tr>
  </tbody>
</table>
<p>16 rows × 100070 columns</p>
</div>




```python
# Let's see the predicted token and it's original English word
predicted_index = torch.argmax(logits[0,0]).item()
encoding.decode([predicted_index])
```




    ' Lane'




```python
# Let's see the original input sentence
encoding.decode(x_batch[0].tolist())
```




    ' the sales process. The ability to effectively communicate the value and benefits of our products'




```python
# Looks like the predicted token "Catholics" is not the correct prediction token to the original sentence, because we only did one training loop and barely trained nothing.
# But this is the basic idea of how the Transformer works.
# We'll continue to train the model in the next notebook and wrap the above code into a class.
# https://waylandzhang.github.io/en/let-s-code-llm.html
# 在实践中，多个 transformer 块将堆叠在一起以执行一个解码交易。在训练期间，输出 Token 将与 Ground Truth Token 进行比较以计算损失。然后对超参数中定义的 max_iters 时间重复该过程。
```
