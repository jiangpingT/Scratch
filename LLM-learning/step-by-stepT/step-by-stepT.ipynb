{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b197111f3dd1d21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:23:38.643505Z",
     "start_time": "2024-02-09T04:23:10.263131Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adec81e8b065abf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:23:51.966821Z",
     "start_time": "2024-02-09T04:23:51.948834Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x33032d430>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0，设置超参数Setup Hyperparameters\n",
    "batch_size = 4  # 每个 batch 中包含的数据样本数量\n",
    "context_length = 16  # Length of the token chunk each batch\n",
    "d_model = 64  # The vector size of the token embeddings\n",
    "num_layers = 8  # Number of transformer blocks\n",
    "num_heads = 4  # Number of heads in Multi-head attention # 我们的代码中通过 d_model / num_heads = 来获取 head_size\n",
    "learning_rate = 1e-3  # 学习率\n",
    "dropout = 0.1 # Dropout rate\n",
    "max_iters = 500  # Total of training iterations # 控制训练循环的终止条件 #在实际训练中，除了使用 max_iters 作为硬终止条件外，还可能结合早停法（Early Stopping）等技术。早停法会在验证集上的性能不再提升时提前终止训练，而 max_iters 可以作为一个兜底的终止条件，防止训练过程无限期地进行下去\n",
    "eval_interval = 50  # How often to evaluate the model # 当训练迭代次数达到 eval_interval 的整数倍时，会暂停训练，使用验证集数据对模型进行评估，计算相关评估指标，以监控模型的性能\n",
    "eval_iters = 20  # How many iterations to average the loss over when evaluating the model # 在每次评估模型时，并不是只计算当前迭代的损失值，而是计算最近 eval_iters 次迭代的损失值的平均值，以此来更稳定、准确地反映模型在一段时间内的性能表现。例如，当 eval_interval 为 50 时，在第 50 次迭代进行模型评估，此时会计算从第 31 次（50 - 20 + 1）迭代到第 50 次迭代这 20 次的损失平均值作为本次评估的损失指标\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Instead of using the cpu, we'll use the GPU if it's available.\n",
    "\n",
    "TORCH_SEED = 1337 # 设置随机数种子，随机数种子用于初始化随机数生成器，以便在不同的运行中能够得到相同的随机数序列\n",
    "torch.manual_seed(TORCH_SEED) # 通过设置固定的随机数种子，可以确保在多次运行代码时，随机数生成的结果是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:23:53.539657Z",
     "start_time": "2024-02-09T04:23:53.516333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1，输入准备阶段：准备数据集Prepare the Dataset \n",
    "# # download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt\n",
    "if not os.path.exists('../data/sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('../data/sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8eb984bfcb7a06b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:24:44.270382Z",
     "start_time": "2024-02-09T04:24:44.140668Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711]\n",
      "Chapter 1: Building Rapport and Capturing\n",
      "the first 10 tokens of encoding text: [26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711]\n",
      "the first 10 items in the tensor tokenized_text: tensor([26072,   220,    16,    25, 17283, 23097,   403,   323, 17013,  1711])\n",
      "Tokenized text size: 77919\n",
      "The maximum value in the tensor tokenized_text is: 100069\n"
     ]
    }
   ],
   "source": [
    "# 2，输入准备阶段：分词化Tokenization\n",
    "# Using TikToken to tokenize the source text\n",
    "# 使用 tiktoken 库对数据集进行标记化。该库是一种快速且轻量级的分词器，可用于将文本分词化为分词\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\") #tiktoken 是 OpenAI 开发的一个用于处理文本和标记（token）之间转换的库，它可以将文本编码为模型能够处理的标记序列，也可以将标记序列解码回原始文本\n",
    "encoding_text = encoding.encode(text)\n",
    "\n",
    "# Illustration purpose\n",
    "print(encoding.encode('Chapter 1: Building Rapport and Capturing'))\n",
    "print(encoding.decode([26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711])) # \"Rapport\" is tokenized as two tokens: \"Rap\"[23097] and \"port\"[403]\n",
    "\n",
    "print(f\"the first 10 tokens of encoding text: {encoding_text[:10]}\")\n",
    "\n",
    "tokenized_text = torch.tensor(encoding_text, dtype=torch.long, device=device) # Convert tokens into a tensor\n",
    "max_token_value = tokenized_text.max().item() # the maximum index value in our vocabulary\n",
    "print(f\"the first 10 items in the tensor tokenized_text: {tokenized_text[:10]}\")\n",
    "\n",
    "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
    "print(f\"The maximum value in the tensor tokenized_text is: {max_token_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bd965d977d210d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:24:52.889469Z",
     "start_time": "2024-02-09T04:24:52.878327Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3，输入准备阶段：分割训练集和验证集Split train and validation\n",
    "# 将数据集拆分为训练集和验证集。训练集将用于训练模型，验证集将用于评估模型的性能。\n",
    "split_idx = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd4d699c1f25bbe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:25:08.155569Z",
     "start_time": "2024-02-09T04:25:08.138702Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all items in the tensor idxs: tensor([35754, 55550, 63572,  1447])\n",
      "the first 10 items in the tensor x_batch: tensor([[  279,  6763,  1920,    13,   578,  5845,   311, 13750, 19570,   279,\n",
      "           907,   323,  7720,   315,  1057,  3956],\n",
      "        [ 3495, 14955,    11,   477,  5064, 23146,   430,  9788,   279, 66732,\n",
      "           315,   701, 10209,    13,  3296, 32644],\n",
      "        [38769, 10742,    11, 20958,   264,  6928, 19451,    11, 11125, 64784,\n",
      "            11,   323, 56501, 54111,   439,  6975],\n",
      "        [43496,   872,  8830,   719,  1101,  3727,   279,  6130,  2733,  6755,\n",
      "           323, 16365,   627, 29831, 19682,  5900]])\n",
      "the first 10 items in the tensor y_batch: tensor([[ 6763,  1920,    13,   578,  5845,   311, 13750, 19570,   279,   907,\n",
      "           323,  7720,   315,  1057,  3956,   477],\n",
      "        [14955,    11,   477,  5064, 23146,   430,  9788,   279, 66732,   315,\n",
      "           701, 10209,    13,  3296, 32644,  1521],\n",
      "        [10742,    11, 20958,   264,  6928, 19451,    11, 11125, 64784,    11,\n",
      "           323, 56501, 54111,   439,  6975, 10708],\n",
      "        [  872,  8830,   719,  1101,  3727,   279,  6130,  2733,  6755,   323,\n",
      "         16365,   627, 29831, 19682,  5900,  1450]])\n",
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# 4，输入准备阶段：准备训练数据Prepare data for training batch\n",
    "# 用于训练和损失计算，训练集将用于训练模型，验证集将用于评估模型的性能\n",
    "# x_batch 和 y_batch 用于准备模型训练所需的数据批次，其中 y_batch 主要用于模型训练时计算损失，x_batch 主要用于模型训练时计算梯度。\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,)) # 生成一个包含 batch_size 个随机整数的一维张量 idxs，这些随机整数的范围在 0 到 len(data) - context_length - 1 之间。这些索引通常用于从数据集中随机采样出 batch_size 个样本，每个样本的长度为 context_length，以便用于模型的训练或其他操作。例如，假设 len(data) = 100，context_length = 10，batch_size = 8，那么 idxs 将会是一个包含 8 个在 0 到 89 之间（100 - 10 - 1 = 89）的随机整数的一维张量，如 tensor([12, 35, 6, 47, 23, 78, 19, 56])\n",
    "print(f\"all items in the tensor idxs: {idxs[:]}\")\n",
    "\n",
    "x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs]) # 将 idxs 中的每个索引 idx 作为起点，从数据集中采样出长度为 context_length 的子序列，并将其堆叠成一个二维张量 x_batch。这个张量的大小为 [batch_size, context_length]，表示每个样本的上下文序列。例如，如果 idxs = tensor([12, 35, 6, 47])，那么 x_batch 将会是一个大小为 [4, 10] 的张量，表示每个样本的上下文序列长度为 10，且每个样本的上下文序列分别为 data[12:22], data[35:45], data[6:16], data[47:57]。\n",
    "print(f\"the first 10 items in the tensor x_batch: {x_batch[:10]}\")\n",
    "\n",
    "y_batch = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]) # 在语言模型等任务中，模型的目标是根据输入的一段文本预测下一个词。x_batch 作为模型的输入，包含了当前的文本序列；而 y_batch 包含了对应的下一个位置的文本序列，也就是模型需要预测的目标。在训练过程中，模型会根据 x_batch 做出预测，然后通过计算预测结果与 y_batch 之间的差异（如交叉熵损失）来调整模型的参数，使得模型的预测结果更接近真实的 y_batch。假设 data 是一个包含文本标记的列表，context_length 为 5，idxs 中有一个索引值 idx 为 10。那么 x_batch 对应的一段数据可能是 data[10:15]，而 y_batch 对应的则是 data[11:16] ，y_batch 中每个位置的元素就是模型在看到 x_batch 对应位置元素时应该预测的目标。\n",
    "\n",
    "print(f\"the first 10 items in the tensor y_batch: {y_batch[:10]}\")\n",
    "\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ade5a79e8c689ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:25:48.830907Z",
     "start_time": "2024-02-09T04:25:48.749815Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Look-up table:  Embedding(100070, 64)\n",
      "torch.Size([4, 16, 64]) torch.Size([4, 16, 64])\n",
      "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
      "0   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721\n",
      "1   0.566486 -1.102276  1.712332 -0.354509  0.550577 -0.707943 -0.743899  0.757765  0.018193  1.392380  ...  0.873562  1.226714  0.794431  0.598629  0.884421  0.032520  1.353617  0.059697  1.172510  0.527427\n",
      "2  -0.426478  1.717362 -0.343810 -0.917124 -0.273610  0.695366 -0.849842 -1.301135 -0.162554 -0.252811  ... -0.934960  1.145729 -1.914149 -0.447346  0.597272  1.673483 -1.969475  0.397835 -0.438475 -0.562923\n",
      "3   0.709939  1.369311 -0.707588  1.538689 -2.110915  0.441344 -0.005807  0.171597 -0.296632  0.207320  ...  0.071533 -0.735549  0.069967 -2.744750  1.087368 -0.997812  0.714993 -1.357311  1.603957  0.920290\n",
      "4  -1.963246  0.298927  0.131364  0.082995  0.153765 -0.821641 -1.220109 -1.088038  1.535371  1.829628  ...  0.531913 -0.567854 -2.390946 -0.086596  0.066017  0.655226  0.624369 -0.763375 -0.692774 -0.007724\n",
      "5   0.811149  0.435133  1.131030  0.816734 -1.013971 -0.052429 -0.527541 -0.710573 -0.163887 -1.343154  ... -0.046995 -1.201051 -0.927833  0.322523  0.586139  0.108184 -1.653296  1.918813  0.941642  0.584331\n",
      "6   0.317549  2.106441 -0.092210  0.636316 -0.912476 -1.975633 -0.068806  0.201157  0.333519  0.151939  ...  0.200116  0.051824  1.304806  0.517675  0.049345  0.044632  1.346794 -0.321390 -0.478787 -0.166920\n",
      "7  -2.025703  1.256391 -0.318619  1.432163  1.644837 -1.910154 -1.001209 -0.976038  1.502204  0.841974  ... -2.339570  0.190784 -0.055200  2.281739 -0.417175 -0.801704 -1.393716  1.863095 -0.393567 -0.131746\n",
      "8   2.906978  0.092020 -0.785242  0.609121 -0.790180 -0.026004 -1.156866  0.398454 -0.455395 -0.251288  ...  3.363073 -0.796739  1.757077  1.526690 -0.654219  1.660685  0.965431  0.618787  1.662946  1.768386\n",
      "9   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721\n",
      "10  0.370826  0.272521 -1.915345  0.032303  0.017929 -1.451397 -1.632907 -1.165714  2.069243  0.550959  ... -1.475737  0.216844  0.722587  1.340471 -0.709199  1.097342  0.231425  0.104482 -1.583871  1.481635\n",
      "11 -0.030617  0.305096 -0.199683  1.021860 -0.044290  1.731110 -1.489611  0.928623 -0.587382 -1.500504  ...  0.303483 -0.402740  0.042445 -0.872444  1.356491  1.417310 -1.436294 -0.745807 -0.242414 -0.457403\n",
      "12 -0.927026  0.017112 -0.874911  1.184731  0.727968 -0.564862 -1.850788  0.852115 -0.866277 -1.235445  ... -0.060539  0.198801  0.542944 -0.405060  0.361515  1.217013 -0.568322  1.032800  0.174048  1.400139\n",
      "13  0.625287 -1.128724 -2.476117  0.281622  0.046729  0.833729 -0.771643 -0.884734  0.910253  0.600033  ... -2.394152 -0.798159 -0.969718  0.399133 -0.768250  0.607608 -1.891320  1.642382 -0.156069 -0.930427\n",
      "14 -0.998070  1.024137  0.206201 -2.287179 -0.003692  0.207054  0.138208 -0.992636  0.491568  0.573276  ...  0.146838  0.273048  0.306777 -1.205792 -0.802286 -0.817414 -0.991609  0.227736  0.581187  0.285422\n",
      "15  0.512090 -0.403476 -1.283820 -1.035253  0.385313  0.251786  1.729429  1.389589  0.326549 -0.448694  ...  1.361415  1.225376 -0.196476 -1.841317 -0.838676 -0.372250  0.113748 -0.636791  0.750731  0.664510\n",
      "\n",
      "[16 rows x 64 columns]\n",
      "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
      "0   0.566486 -1.102276  1.712332 -0.354509  0.550577 -0.707943 -0.743899  0.757765  0.018193  1.392380  ...  0.873562  1.226714  0.794431  0.598629  0.884421  0.032520  1.353617  0.059697  1.172510  0.527427\n",
      "1  -0.426478  1.717362 -0.343810 -0.917124 -0.273610  0.695366 -0.849842 -1.301135 -0.162554 -0.252811  ... -0.934960  1.145729 -1.914149 -0.447346  0.597272  1.673483 -1.969475  0.397835 -0.438475 -0.562923\n",
      "2   0.709939  1.369311 -0.707588  1.538689 -2.110915  0.441344 -0.005807  0.171597 -0.296632  0.207320  ...  0.071533 -0.735549  0.069967 -2.744750  1.087368 -0.997812  0.714993 -1.357311  1.603957  0.920290\n",
      "3  -1.963246  0.298927  0.131364  0.082995  0.153765 -0.821641 -1.220109 -1.088038  1.535371  1.829628  ...  0.531913 -0.567854 -2.390946 -0.086596  0.066017  0.655226  0.624369 -0.763375 -0.692774 -0.007724\n",
      "4   0.811149  0.435133  1.131030  0.816734 -1.013971 -0.052429 -0.527541 -0.710573 -0.163887 -1.343154  ... -0.046995 -1.201051 -0.927833  0.322523  0.586139  0.108184 -1.653296  1.918813  0.941642  0.584331\n",
      "5   0.317549  2.106441 -0.092210  0.636316 -0.912476 -1.975633 -0.068806  0.201157  0.333519  0.151939  ...  0.200116  0.051824  1.304806  0.517675  0.049345  0.044632  1.346794 -0.321390 -0.478787 -0.166920\n",
      "6  -2.025703  1.256391 -0.318619  1.432163  1.644837 -1.910154 -1.001209 -0.976038  1.502204  0.841974  ... -2.339570  0.190784 -0.055200  2.281739 -0.417175 -0.801704 -1.393716  1.863095 -0.393567 -0.131746\n",
      "7   2.906978  0.092020 -0.785242  0.609121 -0.790180 -0.026004 -1.156866  0.398454 -0.455395 -0.251288  ...  3.363073 -0.796739  1.757077  1.526690 -0.654219  1.660685  0.965431  0.618787  1.662946  1.768386\n",
      "8   1.399742 -1.206918  0.327990 -0.251130  0.531874 -0.248193  0.351937 -0.389238  0.129977 -1.207580  ... -0.547752  1.515671  1.223744  0.833670 -0.404194  0.563554  0.591292 -0.724745  1.670105 -0.241721\n",
      "9   0.370826  0.272521 -1.915345  0.032303  0.017929 -1.451397 -1.632907 -1.165714  2.069243  0.550959  ... -1.475737  0.216844  0.722587  1.340471 -0.709199  1.097342  0.231425  0.104482 -1.583871  1.481635\n",
      "10 -0.030617  0.305096 -0.199683  1.021860 -0.044290  1.731110 -1.489611  0.928623 -0.587382 -1.500504  ...  0.303483 -0.402740  0.042445 -0.872444  1.356491  1.417310 -1.436294 -0.745807 -0.242414 -0.457403\n",
      "11 -0.927026  0.017112 -0.874911  1.184731  0.727968 -0.564862 -1.850788  0.852115 -0.866277 -1.235445  ... -0.060539  0.198801  0.542944 -0.405060  0.361515  1.217013 -0.568322  1.032800  0.174048  1.400139\n",
      "12  0.625287 -1.128724 -2.476117  0.281622  0.046729  0.833729 -0.771643 -0.884734  0.910253  0.600033  ... -2.394152 -0.798159 -0.969718  0.399133 -0.768250  0.607608 -1.891320  1.642382 -0.156069 -0.930427\n",
      "13 -0.998070  1.024137  0.206201 -2.287179 -0.003692  0.207054  0.138208 -0.992636  0.491568  0.573276  ...  0.146838  0.273048  0.306777 -1.205792 -0.802286 -0.817414 -0.991609  0.227736  0.581187  0.285422\n",
      "14  0.512090 -0.403476 -1.283820 -1.035253  0.385313  0.251786  1.729429  1.389589  0.326549 -0.448694  ...  1.361415  1.225376 -0.196476 -1.841317 -0.838676 -0.372250  0.113748 -0.636791  0.750731  0.664510\n",
      "15  0.648122 -0.186573  0.352738 -1.825604  1.270732 -0.969805  0.531069  0.350651 -0.060166  1.503366  ... -0.430661 -0.508977 -0.602847  0.668671  0.910364 -0.553529 -0.558548  1.340093  1.304710  1.511892\n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5，输入准备阶段：将训练数据的张量表示转换为向量表示Convert the tensor representation of the training data into a vector representation\n",
    "# 定义词嵌入查找表（Token Embedding Look-up Table）是 Transformer 模型中用于将输入文本中的标记（tokens）转换为向量表示的查找表。它将每个标记映射为一个固定大小的向量，这个向量可以作为模型的输入，用于计算注意力机制、前馈神经网络等操作。\n",
    "# 通过将 x_batch 和 y_batch 作为索引输入到查找表中，可以得到对应的词嵌入向量。 在深度学习模型（如 Transformer）中，将张量变成向量进行计算主要有以下几个原因：\n",
    "# 适应模型结构：Transformer 等模型中的后续层（如多头注意力机制、前馈神经网络等）通常期望输入具有特定的形状和维度。将输入数据转换为向量形式，能使数据的维度与模型各层的输入要求相匹配，便于模型进行有效的计算和特征提取。例如多头注意力机制中，需要对输入进行线性变换、维度拆分等操作，合适的向量表示能满足这些计算的需求。\n",
    "# 编码语义信息：通过嵌入层（如这里的token_embedding_lookup_table）将标记（token）转换为向量，能够将离散的标记映射到连续的向量空间中。在这个向量空间里，语义相似的标记在距离上会更接近，从而使模型能够更好地捕捉和利用数据中的语义信息。比如在文本任务中，不同单词的嵌入向量可以反映它们之间的语义关联，这有助于模型理解文本的含义。\n",
    "# 提高计算效率：向量形式的数据在现代硬件（如 GPU）上能更高效地进行并行计算。GPU 擅长处理大规模的矩阵运算，将张量转换为向量后，模型可以充分利用 GPU 的并行计算能力，加速模型的训练和推理过程，提升计算效率\n",
    "token_embedding_lookup_table = nn.Embedding(max_token_value+1, d_model)\n",
    "print(\"Token Embedding Look-up table: \", token_embedding_lookup_table)\n",
    "\n",
    "x_batch_embedding = token_embedding_lookup_table(x_batch.data) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "y_batch_embedding = token_embedding_lookup_table(y_batch.data)\n",
    "\n",
    "print(x_batch_embedding.shape, y_batch_embedding.shape)\n",
    "print(pd.DataFrame(x_batch_embedding[0].detach().cpu().numpy()))\n",
    "print(pd.DataFrame(y_batch_embedding[0].detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17f145bd89a13b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:26:57.181192Z",
     "start_time": "2024-02-09T04:26:57.135243Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Encoding Look-up Table:  torch.Size([4, 16, 64])\n",
      "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
      "0   0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  ...  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000\n",
      "1   0.841471  0.540302  0.681561  0.731761  0.533168  0.846009  0.409309  0.912396  0.310984  0.950415  ...  0.000422  1.000000  0.000316  1.000000  0.000237  1.000000  0.000178  1.000000  0.000133  1.000000\n",
      "2   0.909297 -0.416147  0.997480  0.070948  0.902131  0.431463  0.746903  0.664932  0.591127  0.806578  ...  0.000843  1.000000  0.000632  1.000000  0.000474  1.000000  0.000356  1.000000  0.000267  1.000000\n",
      "3   0.141120 -0.989992  0.778273 -0.627927  0.993253 -0.115966  0.953635  0.300967  0.812649  0.582754  ...  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000  0.000400  1.000000\n",
      "4  -0.756802 -0.653644  0.141539 -0.989933  0.778472 -0.627680  0.993281 -0.115730  0.953581  0.301137  ...  0.001687  0.999999  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000\n",
      "5  -0.958924  0.283662 -0.571127 -0.820862  0.323935 -0.946079  0.858896 -0.512150  0.999947 -0.010342  ...  0.002108  0.999998  0.001581  0.999999  0.001186  0.999999  0.000889  1.000000  0.000667  1.000000\n",
      "6  -0.279415  0.960170 -0.977396 -0.211416 -0.230368 -0.973104  0.574026 -0.818837  0.947148 -0.320796  ...  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999  0.000800  1.000000\n",
      "7   0.656987  0.753902 -0.859313  0.511449 -0.713721 -0.700430  0.188581 -0.982058  0.800422 -0.599437  ...  0.002952  0.999996  0.002214  0.999998  0.001660  0.999999  0.001245  0.999999  0.000933  1.000000\n",
      "8   0.989358 -0.145500 -0.280228  0.959933 -0.977262 -0.212036 -0.229904 -0.973213  0.574318 -0.818632  ...  0.003374  0.999994  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999\n",
      "9   0.412118 -0.911130  0.449194  0.893434 -0.939824  0.341660 -0.608108 -0.793854  0.291259 -0.956644  ...  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999  0.001200  0.999999\n",
      "10 -0.544021 -0.839072  0.937633  0.347628 -0.612937  0.790132 -0.879767 -0.475405 -0.020684 -0.999786  ...  0.004217  0.999991  0.003162  0.999995  0.002371  0.999997  0.001778  0.999998  0.001334  0.999999\n",
      "11 -0.999990  0.004426  0.923052 -0.384674 -0.097276  0.995257 -0.997283 -0.073661 -0.330575 -0.943780  ...  0.004639  0.999989  0.003478  0.999994  0.002609  0.999997  0.001956  0.999998  0.001467  0.999999\n",
      "12 -0.536573  0.843854  0.413275 -0.910606  0.448343  0.893862 -0.940067  0.340989 -0.607683 -0.794179  ...  0.005060  0.999987  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999\n",
      "13  0.420167  0.907447 -0.318216 -0.948018  0.855881  0.517173 -0.718144  0.695895 -0.824528 -0.565821  ...  0.005482  0.999985  0.004111  0.999992  0.003083  0.999995  0.002312  0.999997  0.001734  0.999999\n",
      "14  0.990607  0.136737 -0.878990 -0.476839  0.999823 -0.018796 -0.370395  0.928874 -0.959605 -0.281349  ...  0.005904  0.999983  0.004427  0.999990  0.003320  0.999995  0.002490  0.999997  0.001867  0.999998\n",
      "15  0.650288 -0.759688 -0.968206  0.250154  0.835838 -0.548975  0.042249  0.999107 -0.999519  0.031022  ...  0.006325  0.999980  0.004743  0.999989  0.003557  0.999994  0.002667  0.999996  0.002000  0.999998\n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# 6，输入准备阶段：应用位置嵌入Apply Positional Embedding\n",
    "# 如最初的 “Attention is All You Need” 论文中所述，我们将使用 sine 和 cosine 生成位置嵌入表，然后将这些位置信息添加到输入嵌入标记中。\n",
    "# 总结位置编码解决的问题：我们希望每个单词都带有一些关于它在句子中的位置的信息。我们希望模型将看起来彼此靠近的单词视为 “close”，将距离较远的单词视为 “distant”。我们希望位置编码表示模型可以学习的模式。\n",
    "# 这个位置编码矩阵只创建一次，并为每个输入序列重复使用。\n",
    "# Define Position Encoding look-up table\n",
    "position_encoding_lookup_table = torch.zeros(context_length, d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) #add batch dimension\n",
    "\n",
    "print(\"Position Encoding Look-up Table: \", position_encoding_lookup_table.shape) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(pd.DataFrame(position_encoding_lookup_table[0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa43476e35fdb265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:27:19.636816Z",
     "start_time": "2024-02-09T04:27:18.851615Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustration Purpose Only\n",
    "# 可视化我们的位置嵌入数字并查看模式\n",
    "# 每条垂直线是 0 到 64 之间的维度;每行代表一个字符。这些值介于 -1 和 1 之间，因为它们来自正弦和余弦函数。颜色较暗表示值更接近 -1，颜色较亮表示值更接近 1。绿色表示介于两者之间的值\n",
    "def visualize_pe(pe):\n",
    "    plt.imshow(pe, aspect=\"auto\")\n",
    "    plt.title(\"Positional Encoding\")\n",
    "    plt.xlabel(\"Encoding Dimension\")\n",
    "    plt.ylabel(\"Position Index\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "position_encoding_lookup_table2_np = position_encoding_lookup_table[0].cpu().numpy()\n",
    "visualize_pe(position_encoding_lookup_table2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8bc5355ba995b07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:28:10.855919Z",
     "start_time": "2024-02-09T04:28:10.789425Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
      "0   1.399742 -0.206918  0.327990  0.748870  0.531874  0.751807  0.351937  0.610762  0.129977 -0.207580  ... -0.547752  2.515671  1.223744  1.833670 -0.404194  1.563555  0.591292  0.275255  1.670105  0.758279\n",
      "1   1.407957 -0.561973  2.393893  0.377252  1.083745  0.138066 -0.334590  1.670161  0.329176  2.342795  ...  0.873984  2.226714  0.794747  1.598629  0.884658  1.032520  1.353795  1.059697  1.172643  1.527427\n",
      "2   0.482819  1.301215  0.653670 -0.846175  0.628521  1.126829 -0.102938 -0.636202  0.428573  0.553768  ... -0.934116  2.145729 -1.913517  0.552654  0.597746  2.673483 -1.969119  1.397835 -0.438208  0.437077\n",
      "3   0.851059  0.379319  0.070685  0.910763 -1.117662  0.325378  0.947827  0.472564  0.516017  0.790074  ...  0.072798  0.264451  0.070916 -1.744750  1.088080  0.002188  0.715526 -0.357311  1.604357  1.920290\n",
      "4  -2.720048 -0.354717  0.272903 -0.906938  0.932237 -1.449321 -0.226828 -1.203768  2.488952  2.130765  ...  0.533600  0.432145 -2.389682  0.913403  0.066965  1.655225  0.625080  0.236625 -0.692241  0.992275\n",
      "5  -0.147775  0.718796  0.559903 -0.004127 -0.690035 -0.998508  0.331355 -1.222723  0.836060 -1.353497  ... -0.044886 -0.201054 -0.926252  1.322522  0.587324  1.108183 -1.652407  2.918813  0.942309  1.584331\n",
      "6   0.038134  3.066611 -1.069607  0.424900 -1.142844 -2.948736  0.505219 -0.617680  1.280667 -0.168857  ...  0.202646  1.051821  1.306703  1.517673  0.050767  1.044631  1.347860  0.678610 -0.477987  0.833080\n",
      "7  -1.368717  2.010294 -1.177933  1.943613  0.931115 -2.610583 -0.812628 -1.958096  2.302625  0.242536  ... -2.336618  1.190780 -0.052987  3.281737 -0.415515  0.198294 -1.392472  2.863094 -0.392633  0.868254\n",
      "8   3.896336 -0.053480 -1.065470  1.569054 -1.767442 -0.238040 -1.386770 -0.574759  0.118923 -1.069920  ...  3.366446  0.203255  1.759607  2.526687 -0.652322  2.660683  0.966853  1.618786  1.664013  2.768385\n",
      "9   1.811860 -2.118048  0.777184  0.642304 -0.407949  0.093468 -0.256172 -1.183092  0.421236 -2.164224  ... -0.543957  2.515664  1.226590  1.833666 -0.402060  1.563552  0.592893  0.275254  1.671305  0.758279\n",
      "10 -0.173195 -0.566551 -0.977712  0.379931 -0.595008 -0.661265 -2.512674 -1.641119  2.048560 -0.448827  ... -1.471521  1.216835  0.725749  2.340466 -0.706827  2.097339  0.233203  1.104480 -1.582538  2.481634\n",
      "11 -1.030607  0.309522  0.723369  0.637185 -0.141566  2.726367 -2.486894  0.854962 -0.917957 -2.444283  ...  0.308122  0.597250  0.045924  0.127550  1.359100  2.417307 -1.434337  0.254191 -0.240947  0.542596\n",
      "12 -1.463599  0.860966 -0.461637  0.274124  1.176311  0.329000 -2.790855  1.193104 -1.473960 -2.029624  ... -0.055479  1.198788  0.546739  0.594933  0.364360  2.217009 -0.566188  2.032797  0.175648  2.400138\n",
      "13  1.045454 -0.221277 -2.794333 -0.666396  0.902610  1.350902 -1.489786 -0.188839  0.085725  0.034213  ... -2.388670  0.201825 -0.965607  1.399124 -0.765167  1.607603 -1.889009  2.642380 -0.154335  0.069571\n",
      "14 -0.007462  1.160874 -0.672790 -2.764018  0.996131  0.188258 -0.232188 -0.063762 -0.468037  0.291926  ...  0.152741  1.273031  0.311204 -0.205802 -0.798966  0.182580 -0.989119  1.227733  0.583054  1.285420\n",
      "15  1.162378 -1.163164 -2.252026 -0.785099  1.221151 -0.297189  1.771678  2.388697 -0.672970 -0.417672  ...  1.367740  2.225356 -0.191733 -0.841328 -0.835119  0.627744  0.116416  0.363206  0.752731  1.664508\n",
      "\n",
      "[16 rows x 64 columns]\n",
      "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
      "0   0.566486 -0.102276  1.712332  0.645491  0.550577  0.292057 -0.743899  1.757765  0.018193  2.392380  ...  0.873562  2.226714  0.794431  1.598629  0.884421  1.032520  1.353617  1.059697  1.172510  1.527427\n",
      "1   0.414993  2.257664  0.337751 -0.185363  0.259559  1.541375 -0.440533 -0.388739  0.148429  0.697605  ... -0.934538  2.145729 -1.913833  0.552654  0.597509  2.673483 -1.969297  1.397835 -0.438342  0.437077\n",
      "2   1.619236  0.953164  0.289892  1.609638 -1.208784  0.872807  0.741096  0.836529  0.294495  1.013898  ...  0.072376  0.264451  0.070600 -1.744750  1.087842  0.002188  0.715348 -0.357311  1.604223  1.920290\n",
      "3  -1.822126 -0.691066  0.909637 -0.544932  1.147018 -0.937607 -0.266474 -0.787071  2.348020  2.412382  ...  0.533179  0.432146 -2.389998  0.913404  0.066728  1.655226  0.624902  0.236625 -0.692374  0.992275\n",
      "4   0.054347 -0.218510  1.272569 -0.173198 -0.235499 -0.680109  0.465740 -0.826303  0.789694 -1.042017  ... -0.045308 -0.201053 -0.926568  1.322523  0.587087  1.108184 -1.652585  2.918813  0.942176  1.584331\n",
      "5  -0.641375  2.390103 -0.663338 -0.184546 -0.588541 -2.921712  0.790090 -0.310992  1.333465  0.141597  ...  0.202224  1.051822  1.306387  1.517674  0.050530  1.044631  1.347683  0.678610 -0.478120  0.833080\n",
      "6  -2.305119  2.216562 -1.296015  1.220747  1.414469 -2.883257 -0.427184 -1.794876  2.449352  0.521177  ... -2.337040  1.190781 -0.053303  3.281738 -0.415753  0.198295 -1.392649  2.863094 -0.392766  0.868254\n",
      "7   3.563965  0.845922 -1.644555  1.120570 -1.503901 -0.726434 -0.968285 -0.583603  0.345026 -0.850725  ...  3.366024  0.203256  1.759291  2.526688 -0.652559  2.660683  0.966675  1.618786  1.663879  2.768385\n",
      "8   2.389100 -1.352418  0.047762  0.708803 -0.445387 -0.460229  0.122033 -1.362451  0.704295 -2.026212  ... -0.544379  2.515666  1.226274  1.833667 -0.402297  1.563553  0.592715  0.275254  1.671171  0.758279\n",
      "9   0.782945 -0.638610 -1.466152  0.925738 -0.921894 -1.109737 -2.241015 -1.959568  2.360503 -0.405686  ... -1.471942  1.216836  0.725433  2.340467 -0.707065  2.097339  0.233025  1.104480 -1.582671  2.481634\n",
      "10 -0.574638 -0.533976  0.737949  1.369487 -0.657227  2.521242 -2.369378  0.453218 -0.608066 -2.500290  ...  0.307700  0.597252  0.045608  0.127551  1.358862  2.417307 -1.434515  0.254192 -0.241081  0.542596\n",
      "11 -1.927016  0.021537  0.048141  0.800057  0.630691  0.430396 -2.848071  0.778454 -1.196852 -2.179225  ... -0.055900  1.198790  0.546423  0.594934  0.364123  2.217009 -0.566366  2.032798  0.175515  2.400138\n",
      "12  0.088714 -0.284870 -2.062842 -0.628985  0.495072  1.727591 -1.711710 -0.543744  0.302570 -0.194146  ... -2.389092  0.201828 -0.965923  1.399125 -0.765404  1.607604 -1.889186  2.642380 -0.154468  0.069572\n",
      "13 -0.577903  1.931583 -0.112015 -3.235197  0.852189  0.724227 -0.579936 -0.296741 -0.332961  0.007455  ...  0.152320  1.273033  0.310888 -0.205801 -0.799203  0.182581 -0.989297  1.227734  0.582920  1.285421\n",
      "14  1.502698 -0.266739 -2.162811 -1.512092  1.385136  0.232990  1.359034  2.318464 -0.633057 -0.730043  ...  1.367319  2.225359 -0.192049 -0.841327 -0.835356  0.627745  0.116238  0.363206  0.752598  1.664509\n",
      "15  1.298410 -0.946261 -0.615469 -1.575450  2.106571 -1.518780  0.573318  1.349758 -1.059684  1.534388  ... -0.424336  0.491003 -0.598103  1.668660  0.913921  0.446464 -0.555881  2.340090  1.306710  2.511890\n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# 7，输入准备阶段：将位置信息添加到每个输入嵌入行中，以获得最终输入嵌入矩阵\n",
    "# 由于两个形状相同的矩阵可以相加，因此我们可以将位置信息添加到每个输入嵌入行中，以获得最终输入嵌入矩阵\n",
    "# Add positional encoding into the input embedding vector\n",
    "input_embedding_x = x_batch_embedding + position_encoding_lookup_table # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "input_embedding_y = y_batch_embedding + position_encoding_lookup_table\n",
    "\n",
    "print(pd.DataFrame(input_embedding_x[0].detach().cpu().numpy()))\n",
    "print(pd.DataFrame(input_embedding_y[0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37ba89375b2f44ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:28:52.125568Z",
     "start_time": "2024-02-09T04:28:51.769196Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.843055 -0.550051 -0.127732 -0.844840 -0.819224  0.063522 -0.187879  1.182711 -1.542636  0.582238  0.123997 -0.023149  0.141312  0.812356 -1.628145  0.312126\n",
      "1 -0.350486  0.484634 -0.140094  0.746821  1.481563 -0.266804  0.312059 -0.710153 -0.557109  0.462987 -0.575465  0.824084 -0.910005 -0.060633 -0.851141 -0.696344\n",
      "2 -0.140998 -0.055692 -0.718412 -0.797249  0.366570  1.513673  0.577341 -0.707247 -0.275017  1.270264 -0.846118 -1.158900  1.652279  0.261800 -0.884520 -0.846511\n",
      "3 -0.244500  0.759894  0.067822  0.388658 -0.178537  1.044808 -0.276759 -0.887757 -0.158977 -0.619547 -0.298364 -0.604880 -1.250719  0.388842 -1.088378  0.081781\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.614938 -0.657467  0.080918 -0.701147 -0.332033 -0.185407 -1.033170  0.447318  0.462455  0.497301 -0.083056 -1.593413  0.454453  0.745469 -1.651096  0.451588\n",
      "1 -0.145782  0.062150  0.549227  0.992860  0.473911 -0.393052  0.089704  0.394661 -0.309076 -0.449691 -0.507969  0.825263 -0.800274 -0.936774 -1.369464  0.705076\n",
      "2  0.238615  1.076501  0.382592 -0.691137 -1.052643  0.575740  0.474749 -0.176139  0.206100  1.265228 -0.710662  0.764700  1.243479 -0.662066 -1.538389 -0.075417\n",
      "3  0.319884  0.242967 -0.233653 -0.966210 -0.693450  0.361393 -1.224569  0.128644 -0.085611  0.866901 -0.032113  0.048993 -0.604088 -1.324803 -1.287755  0.321337\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.477557 -0.665898 -0.062522 -0.844529 -0.971296 -0.579648 -0.250835  0.137787  0.971833  0.627744 -0.350412 -1.107923  1.009746  0.223489 -1.007624  0.238047\n",
      "1 -0.157736 -0.379647  0.224449  0.343057  0.675092 -0.883075  0.771069 -0.058893 -0.942641 -0.543024  0.037494 -0.085867 -1.234599  0.592343 -1.158115 -0.058505\n",
      "2 -0.451328  0.773131  0.400444 -0.951134 -0.099844  0.243306  0.205239 -0.197144  0.122609 -0.414965 -1.047807  2.015589  0.876089 -0.899640 -1.242613 -1.861980\n",
      "3  1.023614  0.791576 -0.105600 -0.760460 -0.164955 -1.103956 -1.514201  0.519381 -0.235128  0.016063 -0.164853 -0.578453  0.694984 -0.830543 -0.427421 -0.185568\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.306344  1.002409 -0.558756 -0.368392 -0.373141 -1.728941  0.253534  0.143261  0.430667  0.641169 -1.484428 -0.382139  0.807904  0.482248 -1.512201  1.232279\n",
      "1 -0.550624 -0.022481  0.110000  0.862089  0.557518  0.142512 -0.161860 -0.929796 -0.371089  0.245111  1.033635  0.591592  0.655191  0.273143 -1.534093 -0.017986\n",
      "2 -0.343869  0.463462  0.655731 -1.096172  0.000537  1.985811  0.916595  0.886440  0.575298  0.540631 -0.838614 -0.029521  1.687371 -0.517568 -0.002811 -0.565382\n",
      "3  0.490431  0.417849  0.480311  0.934107 -0.065339 -0.010033  0.094153 -0.461706  0.424854  0.309975 -0.281256  1.245372  0.686575 -1.852055 -0.234020 -0.428271\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.438334 -0.953410 -0.510782 -0.506376  0.031280 -0.456936  0.935150  0.421769 -0.444609  1.587090  1.122377  0.088356 -0.050521 -0.872715 -0.639425  0.572517\n",
      "1  0.295063 -0.300459 -0.087846 -0.854635 -0.029546 -0.896483  0.200802 -1.218089 -0.877742  0.419938 -0.423555  1.188569 -0.862475 -0.562507 -0.539659 -0.153115\n",
      "2  0.086299 -1.197778 -0.423674  0.552355  0.725277  0.213812  0.077010  0.029638  0.246063  1.194938 -0.297176  0.751607  0.912519  0.577433 -1.155185  0.272354\n",
      "3 -0.901110  0.514844 -0.300310  0.770331 -0.456820 -0.062889 -0.767173  0.156300  0.364622 -0.741814 -0.279991 -0.147554 -0.752332 -0.346314 -0.393168 -0.086922\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.098004 -1.362101 -0.880886 -0.048790 -0.831976 -0.919179  0.458893  0.475047 -0.364164  0.514087 -0.198763  0.427323 -0.647461  0.266048 -0.577928  1.007184\n",
      "1 -0.478354 -0.721252  0.784030  0.061834  0.492232 -0.712863 -0.290083 -0.502997  0.520079  0.264797  0.367906  0.900001 -0.784960 -0.213522  0.282126 -0.722758\n",
      "2 -0.036579 -0.290290  0.008043 -0.143929  0.665982  0.520363  0.824292  0.072430  0.679129  1.127481 -0.546705  0.412061  1.899873 -0.461428 -0.429431 -0.858572\n",
      "3 -1.332145 -0.286588 -0.214228  0.020612 -0.352273 -0.049677 -0.492795 -0.235711  0.168299 -0.326701  0.104917 -0.673956 -0.071659  0.481643  0.049477  0.066592\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.433669 -1.239478 -0.150002 -1.126047  0.151928 -0.831388  0.543729  0.171746 -0.161562  0.301515  0.752866  1.031278  0.243581  0.017311 -1.302813 -0.161804\n",
      "1  2.030139  0.848804 -1.016704  0.075851  0.832877 -0.595107 -0.152181 -0.398114  0.323685  0.558464  1.005251  1.297820 -1.275880 -0.522440 -0.065408 -0.632012\n",
      "2  0.335642 -0.023684 -1.246166 -0.892868  1.718963  1.081414  0.382986  0.042498  1.659014 -1.029562 -0.731215  0.100014 -0.983408  0.299675 -0.636578 -0.885547\n",
      "3 -0.060804  1.340595  0.476056  1.011678 -0.739153  0.736712 -0.095485  1.207922  0.523271 -0.370075  0.505100  1.276299  0.144184  1.645972 -0.691074 -0.852811\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.147221 -1.018187 -0.014175 -1.058182 -0.086709 -0.285634  0.605431  1.288928  0.728841  0.193770  0.235218  0.624430 -1.221081  1.286846 -0.811486 -0.250773\n",
      "1 -0.830173 -0.201015  0.375342  1.265234 -0.471125  0.340549 -0.578870 -0.874984 -1.361585 -0.547904  0.919332  0.737287 -0.343580  0.053767  1.055182 -0.393545\n",
      "2 -0.943106 -0.257088  0.703252 -1.483492  0.573838  1.792996  0.816353  0.868351  1.144592  0.029599  0.419578 -0.108692  0.989305  0.400592 -0.742554 -1.176377\n",
      "3 -0.643927 -0.588140 -0.749730 -0.096672 -0.239110 -0.649397  0.518488  0.144483  1.279444 -1.409347 -0.396471 -0.893813  0.444066  1.756736  0.131318 -1.130873\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.799027  0.856980  0.446163  0.499316 -0.239582  0.088171 -0.536068 -0.025151 -0.556649  0.104557  1.030410 -0.996602 -0.416170 -0.349448 -0.417679  0.926921\n",
      "1  0.276474  0.690336 -1.160764  0.339022  1.380022 -2.299778 -0.115019  0.680937  0.577072 -0.210091  0.209844 -0.105638 -0.505048  0.118918 -0.305055 -0.552583\n",
      "2  0.049127  0.937139 -0.036767 -1.256461  0.315426  1.488370  1.166160  1.563194  0.747862  0.093654 -0.987244 -1.068065  0.889559  0.637340  1.055367 -0.694108\n",
      "3 -0.127101  1.703753 -0.780233 -0.362808 -0.211615 -0.016047 -0.382914  0.765831 -0.267362 -0.231050  1.126519 -0.050697 -0.417823  0.072670  0.054113  0.375692\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.807956 -0.080249  0.028504 -0.205558 -0.510860 -0.623191 -0.371553  1.062170 -2.003340  0.103988  0.828823  0.067705  0.314004  0.260999 -1.272962  0.705449\n",
      "1  0.075932  0.159653 -0.375793  0.480969  1.502070 -0.806800  0.496836 -0.671475 -0.481707  0.587015 -0.592886  0.452936 -0.618360 -0.193267 -0.913532 -1.056499\n",
      "2 -0.166165 -0.111579 -0.476513 -0.971685  0.430499  1.906988  0.882759 -0.295891  0.490625  1.036929 -0.382306 -1.500847  1.169874  0.514438 -0.322263 -1.096552\n",
      "3 -0.605010  1.021955  0.607436  0.416600 -0.263974  1.275973 -0.368588 -0.490732  0.345144 -0.508597  0.101237 -0.505797 -1.383213  0.777961 -0.794481  0.200481\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.975871 -0.355146  0.559124  0.062120 -0.388668 -0.141609 -0.105870  1.215286 -0.553361 -0.017332  1.186850  0.127466 -0.455665 -0.147964 -0.403373 -0.231227\n",
      "1 -0.205101 -0.035606 -0.262179  0.460006 -0.181990 -0.047399  1.185728 -0.185939 -0.446449  0.147912 -0.173577  0.529385 -0.016233  0.513281 -0.778501 -0.917174\n",
      "2 -1.216010 -1.284461  1.441692 -0.500132  0.130558  1.589988  0.537305 -0.073081  0.368621 -0.432179  0.292155 -0.420973  0.415114  0.966622 -0.029067 -0.783478\n",
      "3 -1.276993  0.963069 -0.561037  0.083244 -0.130642 -0.090679 -0.371224 -0.047228  0.877646 -0.731854 -0.649757 -1.325310 -0.018268 -0.085761 -1.308902 -0.753023\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.609152  0.293588  0.681168  0.918692 -0.678428 -0.354196  0.642279  0.126239  0.576536 -0.103225  0.383504 -0.529539 -0.000529 -0.494851 -0.752642  0.999774\n",
      "1  0.014018 -0.073415 -0.514388  0.181423  0.368242 -0.733541  1.133217  0.010973  0.346221 -0.306160  0.369413  0.021123 -0.623327  0.922354 -0.711439  0.181036\n",
      "2 -1.416832  0.690725 -0.128975  0.545911  0.819613  0.555542  0.133320  0.751006 -0.056599 -0.231622 -1.088230 -0.624539 -0.307156  0.812401  0.886276 -0.646983\n",
      "3 -0.565820  0.253911 -0.284727  0.483336 -0.032595 -0.357083  0.346250  0.425751  0.231869 -0.779479  0.215294  0.004534 -0.338554 -0.836733 -1.012783 -1.264050\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.246534  0.311532 -0.668713  0.257104  0.410948 -0.281124  0.246150  0.809893 -0.196731  0.002324  0.671286 -0.184460 -1.098380 -0.228603  0.200689  0.926620\n",
      "1  1.258551  0.512766 -0.600787 -0.730703  0.199514 -1.407976  0.178042  0.112705 -1.211821 -1.171478  0.350218  1.014140 -0.667218  0.248370  0.630296 -0.835714\n",
      "2 -0.033908  0.473868 -0.262070  0.831669  0.498441 -0.002046  0.850105 -0.393790  0.640280 -0.812363 -1.168075  0.823002  0.174674  0.574397 -0.715130 -1.762390\n",
      "3  0.124570 -0.550700 -1.080804 -1.263694 -0.748561 -0.488424 -0.637535  1.093251 -0.212878 -1.900913  0.290618 -0.548380 -0.378159  0.075069 -0.142519 -0.604316\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.045093 -0.017782 -0.036026  0.738200  0.535909  0.540001 -0.284811  0.817065  0.161400 -0.418709  0.505471 -0.807120 -1.493106 -0.354721 -0.677752 -1.364141\n",
      "1 -0.553766 -0.643496 -1.064138  0.517799 -0.125831 -0.866458  0.517786 -0.161137  0.205678 -0.741630  0.237892  0.478060  0.579789  0.911892  0.221268 -0.473353\n",
      "2 -0.975538  0.020211  0.270379 -0.424409 -0.191103  2.134469  0.342932  0.477962  0.237305 -0.928211 -0.706870 -0.156639 -0.178393  1.178581 -0.449019 -1.222883\n",
      "3  0.464261 -0.061850  0.318785  0.414360  1.113341  0.312915 -1.301479  0.484767  0.295944 -1.226489 -0.421380 -1.187196 -1.161356 -0.462495 -0.462336 -0.033009\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.450824 -0.462299  0.040806  0.125190 -0.824344  0.226403 -0.032060  0.119213 -0.317265 -0.176777  0.032324 -0.008321 -1.653131  0.684965 -1.229449 -0.189300\n",
      "1  0.128412  0.410048 -0.352405 -0.017320  0.521036 -0.909860  1.139977 -1.093351 -0.397575  0.403441  0.459461  0.752749  0.028293  0.395984  0.351194 -0.202403\n",
      "2 -0.208809  0.352684 -1.591495 -0.094741  0.688446  0.350263  0.301608 -0.364587  0.342892 -0.051749 -1.301603 -0.089152 -0.648856  0.080371 -1.173354 -0.958096\n",
      "3 -0.127549  0.289107 -0.347003  0.595761  0.312818  0.595343  0.013879  0.676382  0.473617 -0.335544 -0.053533  0.074190 -0.820578  0.144357 -0.719646 -0.173205\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.162300  0.810238 -0.950388  0.148364  0.149367  0.110746  0.224680 -1.256394  0.587622  0.771594 -0.404515 -0.575203 -0.644574  0.833663 -0.296932  0.524873\n",
      "1  0.247992  0.218081 -0.906489  0.290906  0.408525  0.238430 -0.589142  0.493921  0.276924 -0.795749  0.255871 -0.493924 -0.131710  1.504040 -0.143763 -1.431793\n",
      "2 -0.738435  1.368045 -0.962342  0.105499  0.967353 -0.287253  0.924607 -0.144282 -0.480593  0.760399 -0.268804 -0.655696  0.737238  0.279763 -0.736466 -0.631314\n",
      "3  1.751726  0.910328 -0.860379  0.358043 -0.686785 -0.679672 -0.331689 -0.186889 -0.187969  0.584522  1.299885 -0.126924 -0.778028 -0.455524  0.194849  0.312603\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.040591 -1.732430  0.535824 -0.954974 -2.603649  0.957650 -0.246722  0.535815  0.109470  0.249254  0.016516 -0.833666  1.394056  0.508778 -0.962780  0.256198\n",
      "1 -0.393435 -1.428576  0.527408  0.083288  0.718811 -1.246769 -0.440604  0.529822 -0.705668 -0.320694  0.520680 -0.457920 -1.183794 -0.457521 -1.093361  1.068063\n",
      "2 -0.241522  1.396123  0.328996 -0.781264  0.504539  0.107320  0.134646 -0.644498 -0.511961  0.427737 -1.987921 -0.009629  0.166767 -0.461718 -0.606908 -0.372021\n",
      "3  0.210581 -0.098175 -0.696049 -0.555823 -0.827056 -0.590123 -1.126034 -1.177567 -0.730200  0.104929 -0.356334 -0.220717  0.136896 -0.590295 -2.865558 -0.424233\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.411827 -2.483540  0.053123 -0.925263  0.360051  0.623676 -0.124993  0.159468  0.198072  0.594068 -0.474884 -1.280501  0.303108 -0.850220 -0.892375 -0.939363\n",
      "1 -0.044534 -0.204753  0.233314  0.749154 -0.218644 -0.693456 -0.457773  1.001401  1.039589 -0.014641 -0.307821  1.190629 -1.070807 -0.807371 -0.881589 -0.527952\n",
      "2 -0.699358 -1.333793 -0.699475 -0.129277  0.059782  0.764894 -0.056004 -1.113134 -0.499442  0.918923 -1.280702 -0.491481  1.078909 -0.093876 -0.641482 -0.118734\n",
      "3 -0.564800  0.499971 -0.642852  0.397720  0.169384  0.624887 -0.979860 -0.546058 -0.446780 -1.158767 -0.091098 -0.327687 -1.669777 -0.101685 -1.274173 -0.141335\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.068883 -1.115615 -0.357018 -0.015936  0.008346  0.085774 -0.354136 -0.656835  0.423317  0.345923  0.836919 -1.179749 -0.554443 -1.097682  0.860269 -0.158142\n",
      "1  0.009750 -0.282828 -0.829414  0.071886  0.169630 -0.810468 -0.203140  0.086831 -0.448596 -0.323520 -0.166213  0.891237  0.018219 -0.639605 -0.662989 -0.733895\n",
      "2 -0.472119 -0.585412 -0.768696  0.632276  0.115810  0.220249  0.413234 -0.142372  0.506677  0.865187 -1.043125  0.052915  1.086173  0.775785 -0.687548  0.561888\n",
      "3 -0.400620  0.729620 -0.715408  0.108888  0.510726 -0.097342 -0.594547  0.713915 -0.543334  0.048470  0.142754  0.394833 -1.054841  0.061452 -0.944729 -0.458728\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.902081 -1.200709 -0.257382  0.646042 -0.182794 -0.184262 -0.027675  0.217132  0.142002  2.054800 -0.394128 -0.465937  0.745165  0.564093 -1.616877  0.484326\n",
      "1 -0.663733 -0.980381  1.089543  1.456469  0.975607 -0.855795  0.499958 -0.500753  0.047210  0.749712 -0.699079  0.198319 -0.639294 -1.052600 -0.938783  0.392807\n",
      "2 -0.306832  0.724243  0.561730 -0.751851  0.319922  0.967821  0.163208  0.383438  0.316170  1.546890 -0.366850  0.070544  2.103587 -0.546666 -0.496985 -0.860911\n",
      "3 -0.743987 -0.726349  0.046074 -0.221491  0.149525  1.097855 -0.537350 -0.875409  0.714005 -0.008699  0.344594 -0.926214 -1.185889 -1.015362 -1.575311 -0.704234\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.848914 -1.111739  0.006205 -0.039756 -0.933432  0.043748  0.154334  0.911096 -0.581574  0.443663 -0.595020 -0.262369 -0.380455  1.255284 -0.666683  0.853622\n",
      "1  0.815517 -0.364623  0.975614 -0.373926 -0.307815 -0.307191 -0.302004 -1.346682 -0.586013  0.702761  0.458446  0.274101 -0.601604  0.523892 -0.964500 -0.109026\n",
      "2  0.973533  0.140913  0.455739 -0.230147  0.263246  0.627085  1.259960 -0.178967  1.052730 -0.874989 -0.313048  0.577313  0.932425  0.900780 -1.945323 -1.009919\n",
      "3 -0.103243  0.544170 -1.326607 -0.637684  0.144137 -0.298935 -0.438171  0.646421  0.243074 -0.110419 -0.587781 -0.322077 -0.514877 -0.507403 -1.168004  0.435127\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.510558 -0.052268 -1.116502 -0.161902 -0.586372  0.105670  0.028299  0.774327 -0.053025  1.081806 -0.170327 -0.089421  0.502616  0.287768 -1.521247  0.794785\n",
      "1 -0.311080 -1.562692  0.837166 -0.634363  0.307649 -1.948408 -0.372313 -0.605613 -0.936221 -0.192276 -0.056637  0.101307 -0.792975 -0.105986 -0.236257  0.015216\n",
      "2  0.017085 -0.507896  0.899571 -0.455941 -0.362668  1.391112 -0.107061  0.238046 -0.315525  0.932544 -0.898798  0.875802  0.983320  0.222729 -0.074483 -0.998997\n",
      "3 -0.385237 -0.218126 -0.137004 -0.461211 -1.078457 -0.409342 -1.257615 -0.549745 -0.321394 -0.854914  0.355592 -0.779808 -0.950686  0.051154 -0.046016  0.221253\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.889798 -1.164731 -0.700482  0.033665  0.390086  0.031297 -0.490637 -0.085371 -0.487951  0.966926 -0.260996  0.527975 -0.205336  0.379985 -0.240390 -0.151295\n",
      "1  0.280330 -0.805784 -0.540771 -0.075365  0.600227 -0.736507  0.075909 -1.635359 -0.503545 -0.522346  0.632772  0.485401 -0.870348 -0.356595 -0.837994 -0.012191\n",
      "2 -0.942716  0.034204  0.100116 -0.430781  0.131741  1.203926  0.480484 -0.452285 -0.342294 -0.279016 -0.535710 -0.099177  0.673115  0.186452 -1.986973 -1.628828\n",
      "3 -0.553349  1.407444  0.062491  0.135272  0.325641  0.257729 -1.722388  0.102674  0.471423 -0.801807 -0.128275 -1.109145 -0.602943  0.500599 -0.271009 -0.016864\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.093965  0.205285 -0.285254 -1.113492 -1.152514  0.298618  0.117928 -0.059249 -0.927987  0.459643  1.681641 -1.200673 -0.799749  0.008045  0.030887 -0.257713\n",
      "1 -0.932471 -0.084573  0.361824  0.234218  0.319488 -1.438632 -0.029715 -0.626279 -0.282511  0.028903  1.048055  0.950885 -0.160759  0.389284  0.627767 -1.276038\n",
      "2 -0.475502  0.312370  0.484566 -0.332308  0.955019  1.655013  0.669675  0.759300  1.482135  0.827720 -0.718860 -0.228196  1.390810  0.481911  0.028049 -0.151780\n",
      "3 -0.218880  1.172321 -1.219250 -0.444534 -0.235476  0.236778 -0.081137  0.352130 -0.074667 -0.132512  0.780349  0.006259 -0.204236  0.361812 -0.265033 -0.688900\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.830415 -0.279750  0.046583 -0.391788 -0.563857 -0.386821 -0.225548  1.137196 -1.963818  0.181007  0.755685  0.229591  0.377930  0.351241 -1.333537  0.596582\n",
      "1  0.185397  0.247017 -0.415759  0.552577  1.578399 -0.754548  0.363617 -0.764385 -0.492507  0.617592 -0.529309  0.495332 -0.758185 -0.161221 -0.736383 -1.138794\n",
      "2 -0.105419 -0.304523 -0.553585 -1.196757  0.522040  2.052510  0.764300 -0.362917  0.456191  1.061300 -0.505191 -1.449561  1.084323  0.408596 -0.402934 -1.207800\n",
      "3 -0.449628  1.182088  0.634406  0.548405 -0.409534  1.220581 -0.390373 -0.595042  0.331115 -0.611581  0.080876 -0.430741 -1.242464  0.818053 -0.714036  0.302046\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.129658 -0.045262  0.570697 -0.255184  0.144432 -0.933445  0.781099  0.685583 -0.498323 -0.430914  0.485012 -0.771712 -0.414206 -0.440227 -0.081936  0.399950\n",
      "1  0.609679 -0.131552  0.411169 -0.882083 -0.518400 -1.112237 -0.255672 -0.659279  0.042839 -0.307857  0.214738  0.506229  0.242895  0.102200 -0.515571  0.586174\n",
      "2  1.080462 -0.214256 -1.082755  0.432201 -0.256774  1.216463  1.270056 -0.025288  0.998846  0.798230 -0.634019  0.690348  1.320518  0.368305 -1.035057 -1.378777\n",
      "3  0.020668 -0.425955 -0.029217 -0.564785 -0.019478  0.283089 -0.785113  0.626727 -0.789826 -0.741861 -0.760041 -0.227345 -0.559742 -0.552307  0.521503 -0.307743\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.017067 -0.132201  0.005626  0.503344  0.283302 -0.019817 -0.375682  0.887630 -0.013207 -0.690582  1.040519 -1.066493 -0.977408 -0.785576 -0.662742 -0.741300\n",
      "1 -0.899417 -1.241653 -0.731693  0.271986 -0.212329 -1.088669  0.407414 -0.006261  0.089814 -0.797364  0.367399  0.209349  0.372228  0.605199 -0.200269 -0.365062\n",
      "2 -0.846173  0.287940  0.647478 -0.366624 -0.408595  2.297336  0.545167  0.798921  0.669671 -0.767625 -0.397918 -0.263753 -0.029016  1.440027  0.004349 -1.003745\n",
      "3  0.165065 -0.109351  0.492917  0.181230  1.139297  0.234374 -1.204153  0.233503  0.183980 -0.872557 -0.557667 -1.474802 -1.108856 -0.114233 -0.701452  0.033226\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  2.639729  0.432699  0.228840  0.328165 -1.619762 -0.143244 -0.148332  1.446185 -0.136884 -0.225079  1.478135 -0.445745 -0.352616  0.028643 -1.034523  0.454969\n",
      "1 -1.186099 -0.324240 -0.791502 -1.016193  1.248315 -0.972243  1.534065 -0.547292 -0.946302  0.431607  0.267453  0.309294 -0.433166  0.201256 -0.104921 -0.838163\n",
      "2  0.106800  0.043554  0.900919  0.428170 -0.247493  0.183408  1.895423  0.936508  0.222470 -0.774004 -0.092559  0.208208  0.731011  0.268177 -0.025636 -0.487947\n",
      "3  0.084313 -0.803575 -0.570023 -0.064708 -1.060818  0.203365 -0.065024  0.202545  0.112469 -0.145246 -0.513070 -0.679315 -0.260319  0.085924 -1.460548 -1.566682\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.557170 -0.069228 -0.252429  1.265160 -0.494375  0.057511 -0.535007  0.295485  0.095640  0.343272  0.357061 -1.167645 -1.233100  0.834950 -1.384201 -0.601127\n",
      "1  0.260277  0.225782 -0.290535  0.660958 -0.063076 -0.393807  0.589620 -0.713500  0.398320 -0.530572 -0.241501  1.107055  0.356750  1.185921 -0.153039  0.215772\n",
      "2 -0.330623  0.452588  0.956771 -0.085573  0.729110  1.394057  0.861968  0.413198  0.140418 -1.303610 -0.348097 -0.386850  0.105175  0.568483 -0.466887 -1.195699\n",
      "3  0.410845  0.677328 -0.645256  0.401230  0.496405 -0.010768  0.305602  1.510773  0.423297  0.059943  0.245373 -0.200643 -0.773368 -0.625001 -0.626335 -0.814548\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.400794  1.674622 -0.415681 -0.043126 -0.117706 -1.618549  0.164571  0.309570  0.279495  0.378610 -1.406251 -0.275353 -0.007751  0.573838 -1.323963  0.907701\n",
      "1 -0.215844  0.564049 -0.409821  0.822482  0.802106 -0.206013  0.197558 -0.683868 -0.381642  0.043122  1.011165  0.687814  1.032968  0.730055 -1.051225 -0.444364\n",
      "2 -0.378914  0.294372  0.494542 -1.077896  0.473817  2.164143  1.176582  0.797308  0.450885 -0.162525 -0.898714 -0.042541  0.997564 -0.456949  0.199125 -0.993319\n",
      "3  0.643288  0.502156  0.548274  1.268326 -0.210520  0.171024  0.186575  0.253788  0.681656 -0.329138 -0.144567  1.465397  0.942959 -1.870249  0.069844 -0.950228\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.124050  0.371224 -0.305690  1.633266  1.128819 -0.565459  0.768883 -0.629083  0.272960  1.084742 -0.375039 -0.911658 -0.139525  0.008518 -0.125098 -0.617350\n",
      "1  0.061303 -0.101059  0.229802  0.550959  0.169034 -0.269751 -0.631112 -0.705359 -0.034888 -0.472547  0.160753  0.327375  0.070442  1.157730 -0.530561 -0.087982\n",
      "2  0.609980  0.256873 -0.059367 -0.192184  0.232327  1.274882  1.126307  0.743813 -0.913144  1.074244 -0.373727 -0.221155  2.028465  0.966211 -0.987181 -0.694455\n",
      "3  1.318778  0.719581  0.533894  0.456643  0.172207  1.013823 -1.328376  0.407472  0.906544 -0.761099  0.555193  0.784875 -1.695383 -1.376251  0.846673  0.197437\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.346843  0.402914 -0.150677  0.686670 -0.074748 -0.799165 -1.057026 -0.457870  0.001222 -0.029633  0.191230 -1.684285  0.309775 -0.031503 -0.056870  0.114892\n",
      "1  0.501270 -0.394868 -0.925444 -0.100002  0.339064 -1.044209  0.320324  0.616051 -0.516770 -0.791340 -0.093174 -0.221809  0.761773  0.443387 -1.108984  0.406432\n",
      "2 -0.031256  0.680584  0.657833 -0.708211 -0.352712  1.367602  1.522030  0.917488 -0.668882 -0.727117 -0.187182 -0.723699  0.793422  0.353857  0.172010 -0.420275\n",
      "3  0.659493  0.942593  0.302045  0.603299 -0.180087  0.333501 -1.532934  0.614952  0.680199  0.657408  0.167041  0.711188 -0.971582 -0.351090 -1.314679 -0.199633\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.089579 -0.973977  0.785349  0.142430 -0.214122  0.011994 -0.505739  0.275072 -0.760688  0.252204  0.822373 -0.967037 -0.197454  0.037943 -0.570538 -0.011652\n",
      "1  0.334160 -0.007269  0.605975  0.523686  0.846121 -0.731225  0.078269  0.136376 -0.141776  0.233214 -0.746309 -0.068632 -0.161884 -0.851085 -1.308490  0.871895\n",
      "2  0.362325  0.289150 -0.313092 -0.715971  0.189923  1.530835 -0.059266 -1.019142 -0.295478  1.257605 -0.598340 -1.377333  1.552617  0.794436 -1.117288 -0.343388\n",
      "3 -0.733659  0.890256 -0.073768  0.246744 -0.061131  0.390819 -0.930806  0.110050  0.317988 -0.263796  0.176278 -1.281914 -0.835195 -0.446496 -1.792402  0.635731\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.257294 -0.695780  0.121141 -0.177667 -0.508855 -0.030935  0.275189  0.017120 -0.159493  0.637873  0.150516 -0.303546  0.428513 -0.441419  0.376588 -0.236788\n",
      "1 -0.910946 -0.198898  0.122654 -0.736171 -0.036079 -0.905127 -0.353397 -0.145026 -0.461541  0.065215  0.192835  0.078685 -0.315833 -1.149468 -1.861958  1.334020\n",
      "2 -0.168921  0.189826  0.158124 -0.004286 -0.458063  0.967991  0.559503 -0.480577 -0.439033  0.417869 -1.184282  0.057145  1.950150  0.222633 -1.324048 -0.784906\n",
      "3 -0.112746 -0.499071 -0.856497  0.014403 -0.251019 -0.796245 -1.281106 -0.721319 -0.473868 -0.149498 -0.647904 -0.931394 -0.289107 -0.584800 -0.534413 -1.095125\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.068883 -1.115615 -0.357018 -0.015936  0.008346  0.085774 -0.354136 -0.656835  0.423317  0.345923  0.836919 -1.179749 -0.554443 -1.097682  0.860269 -0.158142\n",
      "1  0.009750 -0.282828 -0.829414  0.071886  0.169630 -0.810468 -0.203140  0.086831 -0.448596 -0.323520 -0.166213  0.891237  0.018219 -0.639605 -0.662989 -0.733895\n",
      "2 -0.472119 -0.585412 -0.768696  0.632276  0.115810  0.220249  0.413234 -0.142372  0.506677  0.865187 -1.043125  0.052915  1.086173  0.775785 -0.687548  0.561888\n",
      "3 -0.400620  0.729620 -0.715408  0.108888  0.510726 -0.097342 -0.594547  0.713915 -0.543334  0.048470  0.142754  0.394833 -1.054841  0.061452 -0.944729 -0.458728\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.176404  0.531872  0.409214  0.192468 -1.084885  0.238997 -0.798379  0.188346  0.059492 -0.024330  0.924521 -1.091804 -0.326156  0.291749 -0.732185 -0.054935\n",
      "1 -0.615884  0.863978 -0.437723 -0.152925  1.116533 -0.681281  0.415692 -0.764032 -0.687655  0.304295 -0.105089  0.731977  0.626084  0.591653 -0.146139  0.494095\n",
      "2 -0.189808  0.788168  0.899602 -1.132650 -0.081587  0.835441  0.785524  0.663442  0.063898 -0.166113 -0.850259  0.151737  1.317178 -0.699426 -0.841369 -0.484020\n",
      "3  0.683071  1.052087 -1.006277 -0.075180  0.912621 -0.327235 -1.111313  0.549032  0.110626 -0.140059 -0.477161 -0.342185 -0.293553 -0.850334 -0.414416  0.550237\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.073625  0.150127 -0.907866  0.334299 -0.202801 -0.097351  0.876653 -0.385648  0.045448  2.448797  0.102841  1.521410  0.074046  0.981755 -1.376029 -0.471205\n",
      "1  0.400723  0.219498  0.124873 -0.358635 -0.443369 -0.247101 -0.808554 -0.685117  0.247060  0.634415 -0.134543  0.128437 -0.833682  0.816896 -0.164843  0.732479\n",
      "2  0.263923  0.820781 -0.206039  0.285594  0.476259  0.501493 -0.342558 -0.732048 -0.340279  1.199124 -0.888102  0.372415  1.133199 -0.030477 -1.117813 -1.242966\n",
      "3  0.707586  1.043648 -0.785706 -0.182822 -1.052228 -1.017795 -0.886804 -1.194169  0.279976  0.415883 -0.114251 -1.453649 -0.459333 -0.811049  0.461218  0.328624\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.565246 -0.903285  0.252414  0.429647 -0.713902  1.917740  0.362854  0.713402 -0.210875  0.616729  1.185414  0.169722  0.619079  0.519919 -0.158226 -1.496089\n",
      "1  0.888702 -0.035670 -0.126169 -0.364905  0.810807 -0.035866 -0.105929  0.211207 -0.261469 -0.215912 -1.080510  0.501321 -0.848003  0.389684  0.131101 -0.584177\n",
      "2 -0.815836  0.537262  0.411533  0.070018  0.632414 -0.549836  0.097125 -0.331562  0.368747 -1.247963 -0.250107  0.091148  0.837426  0.622526 -0.783870 -0.812427\n",
      "3  0.127784 -0.015729 -1.144995 -0.323159 -0.314417 -0.153439 -1.000394  0.642610  0.306637  0.099248  0.058967 -0.746932 -0.273949  0.180222 -1.573224 -0.010142\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.255211 -0.157632 -0.574581 -0.166197  0.775349 -0.140706  0.687652  0.807035 -0.184444  0.656217  0.474147 -0.447196  0.060639 -0.188234 -1.242352 -0.329322\n",
      "1  0.488382 -0.108822 -0.275919 -0.090759 -0.832167  0.028082 -0.041036 -1.339602  0.134018  0.438956  0.426242  0.013138 -0.717898 -0.010739  0.128523  0.400315\n",
      "2  0.567573 -0.723301  0.512000 -0.188115 -0.571319  0.991571  0.793728  0.696590 -0.254946 -1.087185 -0.457050  0.546511  0.099871  0.519373 -1.122454 -1.564487\n",
      "3  0.835880 -0.165621  0.295777 -0.285468  0.093285  0.132677 -0.379293  0.561905  0.621888 -0.311707 -0.840512 -0.725356 -0.189057  0.202919  0.051076 -0.613800\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.236179 -1.009632 -0.326470 -0.297060 -0.118641  0.074772  0.023244 -0.203720  0.170052  0.070126  1.334358 -0.704639 -0.528730 -1.149122  0.729896  0.029149\n",
      "1  0.322406 -0.267601 -0.963115 -0.098632  0.339756 -1.158210 -0.381949  0.085271 -0.557521 -0.320726  0.213669  0.647582 -0.255545 -0.395816 -0.170296 -1.198567\n",
      "2 -0.267999 -0.983763 -0.655108  0.191769  0.576468  0.802856  0.599541 -0.029581  0.807375  0.281877 -0.961682  0.093886  0.208873  0.895717 -0.360856 -0.000831\n",
      "3 -0.354376  1.055429 -0.356498  0.454247 -0.083929 -0.238622 -0.499768  0.774746 -0.215797 -0.343473  0.161658  0.368649 -0.487320  0.695373 -0.789305 -0.547663\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.038222 -0.280386 -0.843706 -0.616376  0.294691  0.285665 -0.341028  0.934008 -0.646730 -1.280372  0.236532  0.280084 -0.841592  0.712234 -1.591673  0.064760\n",
      "1  0.129691  0.112961 -0.562534  0.463562  0.526540 -0.433082 -1.213341 -0.475485  0.278862 -0.680838  0.434830  1.604740  0.338845  0.895622  0.145595 -0.223971\n",
      "2 -0.329862  0.591885 -1.080422 -1.282320  0.325834  1.647089  1.472171 -0.050331  0.198912 -0.014550 -0.356193 -0.371771  0.288920  0.205029  0.371042 -0.932187\n",
      "3  0.546774 -0.186697  0.209333  0.978451  0.718924  0.990333  0.238876  0.500508 -0.173352 -1.233035 -0.107134 -0.610185 -0.074154  0.489221  0.113718  0.280216\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.306278 -0.150934  0.773143  0.595103 -0.325866 -0.395447 -0.256868  0.796102 -0.280283 -0.303306  1.391956 -1.493502 -0.704255 -0.555533 -1.038595 -0.122333\n",
      "1 -0.061770 -0.297525 -0.457170 -0.168391  0.302301 -1.931828  0.410538  0.788897  0.600575 -0.890034 -0.427509 -0.468240  0.590850  1.146330 -0.039833 -0.277840\n",
      "2 -0.456495  1.298474  0.484908 -0.122877  0.398217  1.352437  1.198192  0.608493  0.803740  0.279618 -0.096277  0.157943 -0.848379  0.831901  0.467421  0.236334\n",
      "3 -0.263925  1.021767  0.642150  0.543245  0.314341 -0.524004 -0.234829  1.024676  0.541816 -1.119100  0.150054  0.552932 -0.928475 -0.283212 -0.447764 -0.602209\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.169938 -0.497667 -0.325872  0.179360  0.028744 -0.469345 -0.364128 -0.387989  0.060916 -0.149403  1.493312 -1.122818 -0.762886 -1.368208  0.920177  0.245207\n",
      "1  0.088814 -0.410940 -0.922852 -0.260886  0.199503 -1.318331 -0.001653  0.331418 -0.532446 -0.429146  0.038133  0.582173  0.127124 -0.449546 -0.571451 -1.034238\n",
      "2 -0.412180 -0.528039 -0.503837  0.738685  0.370084  0.470210  0.903698  0.125189  0.880732  0.189188 -0.687172 -0.031385  0.387593  1.141662 -0.148142  0.251607\n",
      "3 -0.714183  0.683091 -0.422763  0.152439  0.290620 -0.064837 -0.431068  1.105868 -0.195865 -0.142729  0.210017  0.234452 -0.808928  0.553445 -0.942682 -0.842847\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.609152  0.293588  0.681168  0.918692 -0.678428 -0.354196  0.642279  0.126239  0.576536 -0.103225  0.383504 -0.529539 -0.000529 -0.494851 -0.752642  0.999774\n",
      "1  0.014018 -0.073415 -0.514388  0.181423  0.368242 -0.733541  1.133217  0.010973  0.346221 -0.306160  0.369413  0.021123 -0.623327  0.922354 -0.711439  0.181036\n",
      "2 -1.416832  0.690725 -0.128975  0.545911  0.819613  0.555542  0.133320  0.751006 -0.056599 -0.231622 -1.088230 -0.624539 -0.307156  0.812401  0.886276 -0.646983\n",
      "3 -0.565820  0.253911 -0.284727  0.483336 -0.032595 -0.357083  0.346250  0.425751  0.231869 -0.779479  0.215294  0.004534 -0.338554 -0.836733 -1.012783 -1.264050\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.089952  0.067533  0.085976 -0.181908 -0.242819 -0.559609 -0.495977 -0.046075 -0.098181 -0.457995  0.522625 -1.265378 -1.320826 -0.209618  0.640950  0.411471\n",
      "1  1.201720  0.347224 -0.412666 -0.487666 -0.788717 -0.983542  0.471513 -0.094739 -1.259613 -0.879070  0.363518  0.810040  1.047161  0.236667 -0.032963 -0.061229\n",
      "2  0.425411 -0.076583  0.659215  0.843612  0.921593  1.298184  0.342010  0.924739  1.574549 -0.853615 -0.380778  0.607227 -0.442868  1.298039  0.039828  0.212155\n",
      "3 -0.287385  0.249837 -0.788417 -0.484166  0.454116 -0.390626 -0.147465  1.692854 -0.124048 -0.319251 -0.351036  0.625752  0.184101 -0.575827 -0.779026 -0.565858\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.329653  0.529794 -0.331515  0.677400 -0.131367 -0.555976  0.278185 -0.807618 -0.061015  1.134797 -0.002194  0.735074 -0.201729 -0.046618 -0.306439  0.227947\n",
      "1  1.058406  0.809351 -1.344075 -0.246845  0.175689 -0.245918  0.350410 -0.492653 -0.494349 -0.492613  0.239605  0.506383  0.320137  0.692919 -0.615415  0.188230\n",
      "2  0.185587  1.260174  0.250561 -0.299587  1.242047 -0.277770  0.612673  0.279593 -0.020054 -0.857408 -0.524378 -0.238635  0.308315  0.027083  0.397161 -0.839262\n",
      "3  1.383053  0.896572 -0.590994  0.132179 -0.037870 -0.235834 -0.011826  0.426326  0.076197  0.507106 -0.080371 -0.052627  0.483994 -0.217879  0.242060 -0.172769\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.584967  0.036679 -0.297522  0.097387  0.485003 -0.238667  0.350071 -0.361201 -0.267589  0.658243  0.040019 -0.734725 -0.959120 -0.350231  0.195392  0.406448\n",
      "1  0.852087 -0.366939 -0.282480  0.265795 -0.002569 -0.844916 -0.104893 -0.690379 -0.224831 -0.746555  1.011040  1.010993 -0.554456  0.348798  0.947264 -0.397369\n",
      "2 -0.200723 -0.304258 -0.723998  0.231694  0.869864  1.757347  0.229796  0.930889  0.621773  0.163052 -0.344292 -0.835321 -0.158639  0.378007 -0.990519 -1.116215\n",
      "3 -0.100365  0.223015  0.344705 -0.073661 -0.639542  0.149556 -0.734440  0.478714 -0.072013 -0.749257  1.757566  0.081617 -1.066923 -0.352199  0.226350 -0.415477\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.431029 -0.389901 -0.247779  1.057589 -0.214162 -0.377291 -0.831225 -0.075418 -0.452601  0.135656  0.525034 -0.443614 -1.325111  0.056101  0.228472 -0.178676\n",
      "1  1.723565  0.278511 -0.016062 -1.201930  0.733769 -1.793392 -0.558585  0.130216 -0.274069  0.038803 -0.863897  0.813440  0.919791  0.494655  0.573373  0.027652\n",
      "2  0.879813 -0.209875  0.213537  0.656168  0.473445  0.980684 -0.236595 -0.546739  0.569917  1.428658 -0.737947  0.451539  0.750646  0.683225 -0.826336  0.000961\n",
      "3  0.036668  0.604981 -0.078725 -0.471873  0.906546  0.596679 -0.927565  0.293336 -0.211209 -0.397249  0.382137  0.309855 -1.381001 -0.803773 -0.138390  0.917402\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.362992 -0.324788 -0.199897 -0.665556 -0.714832  0.774445  0.666566  0.487252  0.055709  1.294657 -0.629741 -0.634791 -0.727247  0.827464 -0.410294  0.111628\n",
      "1 -0.726221 -1.016504  0.375666 -0.016474 -0.293093 -0.453114  0.620295 -0.496051 -0.858718 -0.680479  0.327625  0.234066 -0.865707  0.594118 -1.148261  0.229093\n",
      "2  0.585169  0.930731 -0.295074  0.362957  0.147258  0.483375  1.154191 -0.452694 -1.047820 -0.006229 -0.810283 -0.388121  0.672026  0.566222 -1.201632 -0.011228\n",
      "3  0.786654  0.380318 -0.605672  0.383441 -0.101051  0.076134 -0.676884 -0.484011  0.056946 -0.020415 -0.457473 -0.753593 -0.350078 -0.780334 -1.821025 -0.593597\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.176822 -2.470878 -0.147898 -0.274015  0.651451  0.796163 -0.673210  0.159244  0.584264  0.825909  0.290970 -0.608795 -0.352897 -0.225098 -0.972970 -0.860101\n",
      "1  1.448978 -0.258498 -1.089631  0.271407  0.727970 -0.072183  0.364827  0.583581 -0.054674  0.277727 -0.891726  1.263470 -0.313823  0.171239 -0.670323 -0.375938\n",
      "2 -0.203743  0.770183  0.648911 -0.115609 -0.092174  1.593996  0.051193 -0.894547  0.432104 -0.469846 -0.743074  0.249705  0.485517  0.563877 -0.916904 -0.435618\n",
      "3  0.720368  0.267263  0.131167  0.399877  0.984353  0.196040 -0.693219  0.515868  0.182996 -0.152200 -0.281154 -0.016874 -0.803759 -0.564830 -0.649379  0.401523\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.308425 -0.539138  0.259385  0.296139 -0.440926  0.288650 -0.458033 -0.505212  0.679706  0.771421  0.926825 -0.857782 -0.079759 -0.741500 -1.539278 -0.015836\n",
      "1 -0.062495  0.018222 -0.693836  0.729388  1.335306 -0.897933 -0.791998 -0.767458 -0.063126  0.372156 -0.413532  0.537594  0.143167 -0.032629 -0.408854  0.525611\n",
      "2 -0.563814  0.754811  0.150471 -0.752292 -0.196089  0.811713  0.596193 -0.068530  0.573009  1.598698 -0.534681  0.204389  1.164141  0.366169 -0.638497  0.336121\n",
      "3  0.018690  0.668563  0.362883 -0.175717  0.777268  0.265388  0.305293  0.003245  0.814834  0.119982  0.105408  0.698817 -0.878940 -1.288623 -0.465074 -0.137698\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.633797 -0.141075  0.139497  0.358901 -0.697078 -0.262024  0.213127  0.320573 -0.346077  1.143742  0.120111 -0.583446  0.061218  0.039754  0.213620 -0.518166\n",
      "1  0.244168 -0.542625  0.169464 -0.562253 -0.065920 -1.249914  0.501507 -0.605107 -0.280301  0.933891 -0.708358  0.235844 -0.427313  0.163051 -0.979141  0.109747\n",
      "2  0.581454  0.309372 -0.450555  0.661134 -0.098451  0.071689  0.438015 -0.642685  0.419394  1.336107 -0.757630  0.287455  2.246995 -0.247059 -0.576627 -0.452991\n",
      "3  0.024407  0.043233 -1.071366 -0.464481 -0.056488  0.196217 -1.077691 -0.171896 -0.610767 -0.282540  0.311062 -1.076181 -0.833077 -0.622530 -0.472522 -0.361622\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.471197  0.088107 -0.383323 -0.223165 -0.469665 -2.372031  0.272508  0.037552  1.050081  1.027021 -0.132061 -0.094782  1.077781  0.878793 -0.239834 -0.230372\n",
      "1  0.029179 -0.375825  0.352634  0.852334  0.187841 -0.277456  0.070253  0.059308 -0.135633  0.185454 -0.571119  0.115326 -0.333283 -0.260872 -0.540465  0.015298\n",
      "2 -0.107907  0.848008 -0.207849  0.075016 -0.292608 -0.287172  0.107562  1.009907  1.871122  0.219246  0.006427  1.198960  1.336066  0.160271 -1.012163 -1.876254\n",
      "3  0.513124  0.250150 -0.362078 -0.456548 -0.820738 -0.722619 -0.163761  0.326078 -0.196407  0.575446  0.202406 -0.401170  0.376073 -0.906169  0.381597 -0.071292\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.467124 -0.984563 -0.257064 -2.412939 -0.639123 -0.345935  0.859396  1.475097 -0.012862  0.375938  0.619530 -0.325561  0.118446  0.458732 -1.207592  0.147781\n",
      "1  1.088158  0.810464  0.807675  0.654824  0.703983 -0.603323  0.043721 -0.270394 -0.900159  0.019439 -0.293723  1.303475 -0.680663  0.589663  0.252987 -0.994002\n",
      "2  0.292755 -0.045952 -0.128179 -0.426168  1.002135  1.233764  0.672043 -0.698935  0.949020 -0.219086  0.124726  0.918181  1.181413 -0.125518 -0.416058 -1.771814\n",
      "3 -0.242158  0.389674 -0.669892 -0.280750 -0.688672  0.363253  0.630064  0.664664  0.136879 -1.421761 -0.212463 -0.382077  0.715042  0.261851 -0.037907  0.047845\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.059226 -0.558207 -0.249727 -0.602460 -0.761082 -0.116131  0.261580  1.301103 -1.791569  0.495411  0.351794  0.675023  0.378143  0.823263 -1.746921  0.506247\n",
      "1  0.274707  0.351110 -0.267733  0.665271  1.453197 -0.418145  0.193962 -1.205970 -0.500388  0.785514 -0.261973  0.550507 -1.111312 -0.008571 -0.504501 -0.966200\n",
      "2 -0.096769 -0.546517 -0.722104 -1.146778  0.559480  1.752102  0.669178 -0.494542  0.156485  0.845991 -0.670795 -1.131252  0.897950  0.253173 -1.034849 -1.355884\n",
      "3 -0.536298  1.018879  0.436880  0.469634 -0.633801  0.954112 -0.318206 -0.859970  0.359073 -0.614701 -0.042932 -0.458950 -1.168430  0.884767 -0.871933  0.298743\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.610655 -1.370669 -0.277214 -0.888282 -0.160174  0.249849  0.726395 -0.309036  0.245068 -0.293404  0.884923 -1.118040 -0.858950 -0.627054 -1.436908 -0.067506\n",
      "1  0.547155 -0.337403 -0.414298 -0.212033 -0.145155  0.207373  0.381346  0.100462  0.709092  0.317103 -0.289168  0.348626 -0.340906  1.027436  0.504305 -0.969035\n",
      "2 -0.273024 -0.651273 -0.300382 -0.134794 -0.397924  1.016576  0.910840 -0.153315  0.340497 -0.124381 -0.059417 -0.479784  0.712562  0.236867 -1.093192 -0.390130\n",
      "3  0.162120  0.305357  0.109529 -0.471995  0.701465 -0.630411  0.013097  0.355996  0.202033  0.387095 -0.159123 -0.428632 -1.473286  0.255073 -0.325924  0.773818\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.858824 -0.177800 -0.352326 -0.744648 -0.964367 -0.167892 -0.971617  1.283839  0.319020  0.239411  0.495465 -0.076289  0.106109  0.246583 -0.913282 -0.194536\n",
      "1  0.298962  0.576147  0.073836 -0.247572  0.533223 -1.038875  0.314859 -0.377403 -0.484762 -0.297288 -0.050916  1.660669  0.520465 -0.142441 -0.137322 -1.251222\n",
      "2 -0.033523 -0.829220 -0.304602 -0.110416 -0.150082  1.594558  0.274587  0.676428  1.243627  0.277391 -0.565361 -0.421763 -0.036186 -0.461395  0.704446 -0.372949\n",
      "3  0.148561  0.996812 -0.108721  0.710904 -1.185101  1.112566 -0.759846  0.024740 -0.183061 -0.134591 -0.500687 -0.171026 -0.172278 -0.232858 -0.280900  0.070325\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.488704 -0.725228  0.173900 -0.830414  0.213070 -0.718127 -0.444598  0.672499 -1.035795  0.015452  0.295015 -0.910450 -0.307357  0.305210 -0.243458  0.940930\n",
      "1  1.042728 -0.129669  0.313524 -1.872151 -0.012815 -0.607495  0.371844 -0.861372 -0.658208 -0.640721  0.625611  0.016249 -0.624924 -0.071195  0.438702 -0.223898\n",
      "2  0.565551  0.743168  0.227358  0.633667  0.500689  1.503283  0.044266  0.352887  0.546992  0.214653  0.130992 -1.011247 -0.438571 -0.165588 -0.744111 -0.907085\n",
      "3  0.286262  0.421306  0.353233 -0.335495 -0.862758 -0.010216 -1.786810  0.390619  0.424009 -0.530579  0.090104 -1.364448 -0.198620 -0.126026  0.187004 -0.110236\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.683050  0.255799  0.749018  0.855960 -0.666782 -0.468342  0.535272  0.089472  0.529222 -0.168698  0.622150 -0.635523  0.240684 -0.716005 -0.665860  1.135421\n",
      "1 -0.067731 -0.270443 -0.465293  0.120904  0.368781 -0.819422  1.016821  0.149845  0.338048 -0.305068  0.320750 -0.081365 -0.627034  0.785185 -0.829301  0.136954\n",
      "2 -1.385310  0.756914  0.017838  0.404852  0.797507  0.749105  0.096549  0.821897  0.128030 -0.020867 -0.998237 -0.762568 -0.179816  0.874281  1.155173 -0.573110\n",
      "3 -0.496411  0.383706 -0.159284  0.536126 -0.037892 -0.341835  0.271312  0.318583  0.227475 -0.699326  0.258262 -0.061990 -0.343047 -0.732064 -0.997765 -1.098356\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.590556 -0.126417 -0.008332 -0.129837 -0.218032  0.165846 -0.376837  0.704665 -0.395220 -0.282819 -0.006145 -0.243167 -1.332027  1.710084 -0.399444  0.197928\n",
      "1  0.804329  0.749667  0.707012 -1.197384  0.487677 -1.247303  0.620496 -0.566080 -0.653210  0.731915 -0.119785 -0.021137  0.388589  0.057226 -0.883790 -0.808471\n",
      "2  0.709311  0.215274 -0.333312  1.049628 -0.046225 -0.605354  1.996748  0.004057  0.490174  0.132027 -0.014141  1.074840  0.392585 -0.311970 -1.730481 -0.528037\n",
      "3 -0.374563 -0.796446 -1.307556 -0.275224 -0.090963  1.311742  0.028888  1.471484  0.982600 -0.028525 -1.000560 -0.926102 -0.157814 -0.307164  0.156203 -0.537111\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  0.401719  0.425182 -0.292917 -0.701397 -0.326698 -0.279441 -0.798718  0.513086  0.320014  0.617820 -0.086840 -0.417280 -1.574415 -0.074147  0.181661 -0.622625\n",
      "1  0.125515  0.108487 -0.512970 -0.430991 -0.051894 -2.067564  1.478227 -0.140694 -0.479147 -0.896122  0.062240  0.174992 -0.011013 -0.770407 -0.188411 -0.085206\n",
      "2  0.193214  0.273297 -0.967667  0.689172  0.477827  0.713157  0.815838  0.416669  0.088426  0.044879 -0.690439 -0.935450  0.204611  0.335932  0.014221  0.127856\n",
      "3 -0.343215  0.089352 -0.411377 -0.590583 -0.172385  0.425989 -1.009057  0.996904 -0.125823 -0.892946  0.006623  0.012985 -1.091260 -0.097457 -0.360760 -0.812595\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0  1.215814 -0.422020  0.019028  0.565088 -0.494546  0.823564 -0.480298  1.053372 -0.271921  0.653942  0.552066 -0.897783 -1.794641  0.332980 -1.265209  1.206126\n",
      "1  0.164707 -0.261773  0.114363  0.238533  0.831699 -0.905116  0.107201 -0.855621 -0.653122 -0.311885 -0.033228  0.752663 -0.481584  0.115888 -0.548276 -0.652191\n",
      "2  0.416088  0.351545  0.118157 -0.563940  0.611703  0.765792  1.143817 -0.260738  0.213900  0.278640 -0.615235  0.565911  0.454788 -0.064289 -0.472896  0.166102\n",
      "3 -0.358502  0.241573 -0.537251 -0.294055  0.697770 -0.024773 -0.013605  1.368784  0.674618 -0.099743 -0.501064 -0.418571  0.113558 -0.607647 -1.413939 -0.550027\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.030005  0.560489  0.692207  0.802441 -1.049383 -1.130618 -0.233082 -0.138613  0.441630  0.025435 -0.203886 -0.245355 -0.877590  0.185146  0.144022 -0.151590\n",
      "1  0.886263  0.015151 -0.640962 -0.522613  0.534007 -2.012024  0.963597 -0.149933 -0.269668  0.051612 -0.241905  0.215621  0.793468  0.193036 -0.299267 -0.301879\n",
      "2 -0.777996 -1.183819 -0.050823  0.102093  0.829035  1.152996  0.128101  1.013317  1.131896  0.165107 -0.910277 -0.281369  0.560279  0.269381 -0.152854 -1.395591\n",
      "3 -0.647185  1.049913 -0.477628  0.617693 -0.168791  0.307628 -0.439353  1.122450 -0.519324 -0.404857  0.109275  1.548271 -1.384960 -0.262460  0.355068  0.332892\n",
      "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
      "0 -0.070381 -0.865201 -1.882663 -0.408366 -0.598919  0.541644  0.638400  0.586414  0.963336  0.899848 -0.811803 -0.428355  0.283824  0.715828 -0.689767 -1.133214\n",
      "1  0.163508  0.148884 -0.098002  0.161132 -0.572743 -0.657556 -0.488363 -0.237395 -0.239877  0.452628 -0.079612 -0.221905 -0.779132  0.257819 -0.168416 -1.584344\n",
      "2  0.159060 -0.572794 -0.154949  0.059427  0.620815 -0.265221 -0.102914 -0.490620 -1.777336  0.243808 -1.639493  0.448959  0.437344  0.562454 -0.772507  0.010446\n",
      "3  0.485404 -0.012398 -1.315031  0.238375 -1.477421 -0.787531 -1.435814 -0.116352  0.708661 -0.874855  0.670505 -0.339437 -0.194216 -0.594283 -0.826917 -0.267774\n",
      "torch.Size([4, 16, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# 8，特征提取与融合阶段-多头注意力机制：准备 Q、K、V \n",
    "# Prepare Query, Key, Value for Multi-head Attention\n",
    "# 转换为 Q、K、V 的原因：\n",
    "# 引入不同的表示视角：将输入X通过不同的线性变换得到Q、K、V，可以让模型从不同的角度对输入信息进行编码和处理。Q主要用于引导模型对信息的查询和关注，K用于提供信息的索引和标识，V则携带了实际的信息内容。这样的设计使得模型能够更灵活地捕捉输入数据中的复杂关系。\n",
    "# 实现注意力机制：Q、K、V是注意力机制的核心组成部分。注意力机制的目的是让模型能够根据不同的任务需求，动态地关注输入序列中的不同部分。通过将输入转换为Q、K、V，模型可以计算出每个位置对于其他位置的注意力权重，从而有针对性地聚合信息，提高模型对重要信息的捕捉能力。\n",
    "# 提高模型的灵活性和泛化能力：将输入转换为Q、K、V并进行注意力计算，使得模型能够自适应地关注输入数据中的不同部分，根据具体的任务和输入内容动态地调整注意力分布。这种灵活性使得模型能够更好地适应各种不同的语言现象和任务需求，提高了模型的泛化能力，能够在多种自然语言处理任务中取得较好的效果，如文本生成、机器翻译、问答系统等。\n",
    "# 虽然模型在一定程度上可以解决输入序列中位置之间的相似度问题，但这只是其功能的一部分，更重要的是通过一系列的设计和计算，实现对输入数据的深度理解、信息融合和高效处理，以解决各种复杂的自然语言处理任务和其他相关领域的问题。\n",
    "# 在 PyTorch 中，view 方法是用于改变张量形状的函数。它可以在不改变张量数据的情况下，将张量重新排列成指定的形状。其语法为 tensor.view(*shape)，其中 *shape 是一个可变参数，表示新的形状。\n",
    "# 例如，如果有一个形状为 [4, 16, 64] 的张量，想要将其重塑为 [4, 16, 4, 16] 的形状，可以使用 view 方法，如下所示：\n",
    "# tensor = torch.randn(4, 16, 64)\n",
    "# reshaped_tensor = tensor.view(4, 16, 4, 16)\n",
    "# 通过 view 函数对张量 Q、K、V 的形状进行了重新组织，以适应后续的计算需求，特别是在多头注意力机制的计算过程中。\n",
    "X = input_embedding_x\n",
    "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "print(query.shape)\n",
    "\n",
    "Wq = nn.Linear(d_model, d_model) # nn.Linear：这是 PyTorch 中 torch.nn 模块里用于创建线性层（也称为全连接层）的类。线性层的作用是对输入数据进行线性变换，其数学公式为 y = x * W + b，其中 x 是输入张量，W 是权重矩阵，b 是偏置向量，y 是输出张量。有两个 d_model 参数。第一个 d_model 表示输入特征的维度，意味着这个线性层期望输入的张量在最后一个维度上有 d_model 个特征。第二个 d_model 表示输出特征的维度，即经过这个线性层变换后，输出张量在最后一个维度上也会有 d_model 个特征。例如，如果 d_model 的值为 64，那么这个线性层的输入和输出特征维度都是 64。\n",
    "Wk = nn.Linear(d_model, d_model) # Wk 是一个自定义的变量名，用于存储创建的线性层对象。后续在模型的计算过程中，可以通过调用 Wk（例如 output = Wk(input)）来对输入数据应用这个线性变换。\n",
    "Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Wq(query) #[4, 16, 64]\n",
    "Q = Q.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "K = Wk(key) #[4, 16, 64]\n",
    "K = K.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "V = Wv(value) #[4, 16, 64]\n",
    "V = V.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "# print(torch.round(Q[0] * 100) / 100)\n",
    "qqq = Q.detach().cpu().numpy()\n",
    "for qs in qqq:\n",
    "    for qss in qs:\n",
    "        print(pd.DataFrame(qss))\n",
    "\n",
    "print(Q.shape) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6b2187b2d310a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:29:30.525962Z",
     "start_time": "2024-02-09T04:29:30.508654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 9，特征提取与融合阶段-多头注意力机制：重塑 Q、K、V \n",
    "# 将 Q、K、V 重塑为 [batch_size、num_heads、context_length、head_size] 以进行进一步计算\n",
    "# 并行计算与高效性：模型的设计还考虑了计算效率。通过将Q、K、V表示为矩阵，并利用矩阵乘法等操作，可以在 GPU 等并行计算设备上进行高效的并行计算。这使得模型能够快速处理大规模的数据，大大提高了训练和推理的速度，使得在实际应用中处理大量文本数据成为可能。\n",
    "# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]\n",
    "# The reason is that treat each batch with \"num_heads\" as its first dimension.\n",
    "Q = Q.transpose(1, 2) # [4, 4, 16, 16]\n",
    "K = K.transpose(1, 2) # [4, 4, 16, 16]\n",
    "V = V.transpose(1, 2) # [4, 4, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6dd5217e3fa43d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:31:11.625621Z",
     "start_time": "2024-02-09T04:31:11.196103Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827912</td>\n",
       "      <td>0.872087</td>\n",
       "      <td>0.805252</td>\n",
       "      <td>0.638937</td>\n",
       "      <td>0.403635</td>\n",
       "      <td>-0.220579</td>\n",
       "      <td>0.074281</td>\n",
       "      <td>0.066307</td>\n",
       "      <td>0.161097</td>\n",
       "      <td>0.776864</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>0.059171</td>\n",
       "      <td>-0.167397</td>\n",
       "      <td>-0.519080</td>\n",
       "      <td>-0.113654</td>\n",
       "      <td>0.370162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123528</td>\n",
       "      <td>0.464093</td>\n",
       "      <td>0.514921</td>\n",
       "      <td>-0.184883</td>\n",
       "      <td>0.292599</td>\n",
       "      <td>-0.127226</td>\n",
       "      <td>0.205937</td>\n",
       "      <td>0.207778</td>\n",
       "      <td>0.602376</td>\n",
       "      <td>1.016719</td>\n",
       "      <td>0.158707</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.460966</td>\n",
       "      <td>0.633765</td>\n",
       "      <td>0.534267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.040624</td>\n",
       "      <td>0.605709</td>\n",
       "      <td>0.119166</td>\n",
       "      <td>-0.133348</td>\n",
       "      <td>-0.169015</td>\n",
       "      <td>-0.146091</td>\n",
       "      <td>0.444743</td>\n",
       "      <td>0.050206</td>\n",
       "      <td>0.774002</td>\n",
       "      <td>0.830742</td>\n",
       "      <td>-0.216136</td>\n",
       "      <td>-0.111672</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>0.496729</td>\n",
       "      <td>0.747770</td>\n",
       "      <td>0.609336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900045</td>\n",
       "      <td>0.596818</td>\n",
       "      <td>0.359724</td>\n",
       "      <td>0.836026</td>\n",
       "      <td>0.074438</td>\n",
       "      <td>0.652813</td>\n",
       "      <td>1.057635</td>\n",
       "      <td>-0.026944</td>\n",
       "      <td>1.124048</td>\n",
       "      <td>0.971823</td>\n",
       "      <td>-0.467323</td>\n",
       "      <td>-0.247706</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>-0.360194</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>0.965252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.061119</td>\n",
       "      <td>1.158660</td>\n",
       "      <td>0.633724</td>\n",
       "      <td>-0.320907</td>\n",
       "      <td>0.383561</td>\n",
       "      <td>-0.448183</td>\n",
       "      <td>-0.238576</td>\n",
       "      <td>-0.078154</td>\n",
       "      <td>-0.033891</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>0.056048</td>\n",
       "      <td>0.533376</td>\n",
       "      <td>0.968321</td>\n",
       "      <td>0.675573</td>\n",
       "      <td>0.506570</td>\n",
       "      <td>0.651593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.273445</td>\n",
       "      <td>0.044782</td>\n",
       "      <td>0.250272</td>\n",
       "      <td>-0.114290</td>\n",
       "      <td>0.121239</td>\n",
       "      <td>0.472959</td>\n",
       "      <td>0.086112</td>\n",
       "      <td>-0.077087</td>\n",
       "      <td>-0.158122</td>\n",
       "      <td>-0.416886</td>\n",
       "      <td>0.212225</td>\n",
       "      <td>0.220158</td>\n",
       "      <td>-0.492304</td>\n",
       "      <td>0.175022</td>\n",
       "      <td>0.505993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897961</td>\n",
       "      <td>1.322749</td>\n",
       "      <td>0.553525</td>\n",
       "      <td>0.079147</td>\n",
       "      <td>0.502846</td>\n",
       "      <td>-0.506932</td>\n",
       "      <td>0.223247</td>\n",
       "      <td>-0.400829</td>\n",
       "      <td>0.203914</td>\n",
       "      <td>0.658943</td>\n",
       "      <td>-0.018270</td>\n",
       "      <td>0.620022</td>\n",
       "      <td>0.600703</td>\n",
       "      <td>0.066074</td>\n",
       "      <td>0.316328</td>\n",
       "      <td>0.332746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025299</td>\n",
       "      <td>0.779429</td>\n",
       "      <td>0.627483</td>\n",
       "      <td>-0.354743</td>\n",
       "      <td>0.218848</td>\n",
       "      <td>-0.055096</td>\n",
       "      <td>0.297431</td>\n",
       "      <td>-0.119230</td>\n",
       "      <td>-0.025749</td>\n",
       "      <td>0.031660</td>\n",
       "      <td>0.079259</td>\n",
       "      <td>0.274235</td>\n",
       "      <td>0.095197</td>\n",
       "      <td>-0.625557</td>\n",
       "      <td>-0.000797</td>\n",
       "      <td>0.226548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.529123</td>\n",
       "      <td>-0.196815</td>\n",
       "      <td>0.232964</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.214278</td>\n",
       "      <td>-0.328799</td>\n",
       "      <td>-0.685732</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.237945</td>\n",
       "      <td>0.642808</td>\n",
       "      <td>0.532076</td>\n",
       "      <td>0.191681</td>\n",
       "      <td>-0.066689</td>\n",
       "      <td>0.383821</td>\n",
       "      <td>-0.080174</td>\n",
       "      <td>0.330104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.935247</td>\n",
       "      <td>0.690427</td>\n",
       "      <td>0.511019</td>\n",
       "      <td>1.063499</td>\n",
       "      <td>0.504674</td>\n",
       "      <td>-0.227777</td>\n",
       "      <td>-0.035515</td>\n",
       "      <td>-0.006277</td>\n",
       "      <td>0.174739</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.184269</td>\n",
       "      <td>0.238904</td>\n",
       "      <td>-0.084467</td>\n",
       "      <td>-0.414827</td>\n",
       "      <td>-0.173580</td>\n",
       "      <td>0.431657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.467166</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.238954</td>\n",
       "      <td>0.085494</td>\n",
       "      <td>0.207894</td>\n",
       "      <td>-0.739952</td>\n",
       "      <td>-0.371635</td>\n",
       "      <td>-0.087244</td>\n",
       "      <td>-0.217037</td>\n",
       "      <td>0.353716</td>\n",
       "      <td>0.276079</td>\n",
       "      <td>0.202890</td>\n",
       "      <td>0.124148</td>\n",
       "      <td>-0.118437</td>\n",
       "      <td>-0.063674</td>\n",
       "      <td>0.186508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016316</td>\n",
       "      <td>-0.238568</td>\n",
       "      <td>-0.610145</td>\n",
       "      <td>-0.418894</td>\n",
       "      <td>-0.609520</td>\n",
       "      <td>-0.072247</td>\n",
       "      <td>-0.179780</td>\n",
       "      <td>-0.085679</td>\n",
       "      <td>0.529486</td>\n",
       "      <td>0.309438</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>-0.154982</td>\n",
       "      <td>0.340535</td>\n",
       "      <td>0.199168</td>\n",
       "      <td>-0.240800</td>\n",
       "      <td>0.511496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.210837</td>\n",
       "      <td>0.166079</td>\n",
       "      <td>0.430884</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.447536</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>-0.069169</td>\n",
       "      <td>0.463648</td>\n",
       "      <td>-0.251515</td>\n",
       "      <td>0.225306</td>\n",
       "      <td>0.302280</td>\n",
       "      <td>0.252891</td>\n",
       "      <td>0.182380</td>\n",
       "      <td>-0.017386</td>\n",
       "      <td>-0.036945</td>\n",
       "      <td>0.483606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.131177</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>0.690766</td>\n",
       "      <td>-0.918174</td>\n",
       "      <td>0.273946</td>\n",
       "      <td>-0.226493</td>\n",
       "      <td>-0.467489</td>\n",
       "      <td>0.787193</td>\n",
       "      <td>-0.527138</td>\n",
       "      <td>-0.107953</td>\n",
       "      <td>0.469084</td>\n",
       "      <td>0.095217</td>\n",
       "      <td>0.443433</td>\n",
       "      <td>0.514454</td>\n",
       "      <td>0.195352</td>\n",
       "      <td>0.647130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.268342</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.530629</td>\n",
       "      <td>-0.301306</td>\n",
       "      <td>0.030230</td>\n",
       "      <td>-0.376372</td>\n",
       "      <td>-0.326081</td>\n",
       "      <td>0.092524</td>\n",
       "      <td>-0.112595</td>\n",
       "      <td>-0.274940</td>\n",
       "      <td>0.107115</td>\n",
       "      <td>0.171046</td>\n",
       "      <td>-0.394139</td>\n",
       "      <td>-0.418468</td>\n",
       "      <td>-0.190215</td>\n",
       "      <td>0.442426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.219957</td>\n",
       "      <td>-0.423728</td>\n",
       "      <td>0.599078</td>\n",
       "      <td>-0.366324</td>\n",
       "      <td>0.452232</td>\n",
       "      <td>0.868501</td>\n",
       "      <td>-0.051142</td>\n",
       "      <td>0.200568</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.050063</td>\n",
       "      <td>0.305444</td>\n",
       "      <td>0.225228</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>0.332432</td>\n",
       "      <td>0.156352</td>\n",
       "      <td>0.471813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
       "0   0.827912  0.872087  0.805252  0.638937  0.403635 -0.220579  0.074281  0.066307  0.161097  0.776864 -0.015753  0.059171 -0.167397 -0.519080 -0.113654  0.370162\n",
       "1   1.123528  0.464093  0.514921 -0.184883  0.292599 -0.127226  0.205937  0.207778  0.602376  1.016719  0.158707  0.074483  0.526718  0.460966  0.633765  0.534267\n",
       "2   1.040624  0.605709  0.119166 -0.133348 -0.169015 -0.146091  0.444743  0.050206  0.774002  0.830742 -0.216136 -0.111672  0.751038  0.496729  0.747770  0.609336\n",
       "3   0.900045  0.596818  0.359724  0.836026  0.074438  0.652813  1.057635 -0.026944  1.124048  0.971823 -0.467323 -0.247706  0.002623 -0.360194  0.017813  0.965252\n",
       "4   1.061119  1.158660  0.633724 -0.320907  0.383561 -0.448183 -0.238576 -0.078154 -0.033891  0.852194  0.056048  0.533376  0.968321  0.675573  0.506570  0.651593\n",
       "5   0.071059  0.273445  0.044782  0.250272 -0.114290  0.121239  0.472959  0.086112 -0.077087 -0.158122 -0.416886  0.212225  0.220158 -0.492304  0.175022  0.505993\n",
       "6   0.897961  1.322749  0.553525  0.079147  0.502846 -0.506932  0.223247 -0.400829  0.203914  0.658943 -0.018270  0.620022  0.600703  0.066074  0.316328  0.332746\n",
       "7   0.025299  0.779429  0.627483 -0.354743  0.218848 -0.055096  0.297431 -0.119230 -0.025749  0.031660  0.079259  0.274235  0.095197 -0.625557 -0.000797  0.226548\n",
       "8   0.529123 -0.196815  0.232964  0.031045  0.214278 -0.328799 -0.685732  0.060671  0.237945  0.642808  0.532076  0.191681 -0.066689  0.383821 -0.080174  0.330104\n",
       "9   0.935247  0.690427  0.511019  1.063499  0.504674 -0.227777 -0.035515 -0.006277  0.174739  0.899135  0.184269  0.238904 -0.084467 -0.414827 -0.173580  0.431657\n",
       "10  0.467166  0.532110  0.238954  0.085494  0.207894 -0.739952 -0.371635 -0.087244 -0.217037  0.353716  0.276079  0.202890  0.124148 -0.118437 -0.063674  0.186508\n",
       "11  0.016316 -0.238568 -0.610145 -0.418894 -0.609520 -0.072247 -0.179780 -0.085679  0.529486  0.309438  0.002346 -0.154982  0.340535  0.199168 -0.240800  0.511496\n",
       "12  0.210837  0.166079  0.430884  0.002882  0.447536  0.287130 -0.069169  0.463648 -0.251515  0.225306  0.302280  0.252891  0.182380 -0.017386 -0.036945  0.483606\n",
       "13 -0.131177  0.249064  0.690766 -0.918174  0.273946 -0.226493 -0.467489  0.787193 -0.527138 -0.107953  0.469084  0.095217  0.443433  0.514454  0.195352  0.647130\n",
       "14 -0.268342  0.041401  0.530629 -0.301306  0.030230 -0.376372 -0.326081  0.092524 -0.112595 -0.274940  0.107115  0.171046 -0.394139 -0.418468 -0.190215  0.442426\n",
       "15 -0.219957 -0.423728  0.599078 -0.366324  0.452232  0.868501 -0.051142  0.200568  0.335000  0.050063  0.305444  0.225228 -0.176068  0.332432  0.156352  0.471813"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10，特征提取与融合阶段-多头注意力机制：计算注意力分数和缩放\n",
    "# Calculate the attention score and scale\n",
    "# Q 和 K 进行乘法的原因：\n",
    "# 计算注意力得分：如前面所说，Q和K的乘法是为了计算注意力得分。具体来说，Q和K转置相乘的结果表示了Q中每个元素与K中每个元素之间的相似度或关联程度。在计算过程中，Q和K的维度通常是匹配的，这样矩阵乘法可以有效地在所有元素之间进行成对的计算，得到一个注意力得分矩阵。这个矩阵中的每个元素反映了相应Q和K元素之间的相关性，为后续确定注意力权重提供了基础。\n",
    "# 捕捉长序列依赖关系：Transformer 等大模型设计的一个重要目标是解决长序列数据中的依赖关系问题。在自然语言处理等任务中，一个句子或文档中的不同位置之间可能存在长期的语义依赖关系。通过注意力机制，模型可以直接计算序列中任意两个位置之间的关联，而不受序列长度的限制，从而有效地捕捉长序列中的依赖关系，提高对文本等长序列数据的理解和处理能力。\n",
    "# 信息筛选与融合：通过Q和K的乘法，模型可以根据计算出的注意力得分来筛选和融合V中的信息。高得分表示对应的Q和K元素之间具有较强的关联，因此在聚合V的信息时，与这些高得分相关的V元素会被赋予更高的权重，从而实现对重要信息的筛选和融合，使得模型能够生成更准确、更有针对性的输出。\n",
    "attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model // num_heads) # [4, 4, 16, 16] # Scale\n",
    "\n",
    "# Illustration only\n",
    "plt.imshow(attention_score[1, 1].detach().cpu().numpy(), \"Accent\", aspect=\"auto\")\n",
    "plt.title(\"Attention(Q @ K)\") #plot attention in the first head of the first batch\n",
    "plt.xlabel(encoding.decode(x_batch[0].tolist()))\n",
    "plt.ylabel(encoding.decode(x_batch[0].tolist()))\n",
    "plt.colorbar()\n",
    "pd.DataFrame(attention_score[0][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9958625c65291f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:31:43.305491Z",
     "start_time": "2024-02-09T04:31:42.713623Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827912</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123528</td>\n",
       "      <td>0.464093</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.040624</td>\n",
       "      <td>0.605709</td>\n",
       "      <td>0.119166</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900045</td>\n",
       "      <td>0.596818</td>\n",
       "      <td>0.359724</td>\n",
       "      <td>0.836026</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.061119</td>\n",
       "      <td>1.158660</td>\n",
       "      <td>0.633724</td>\n",
       "      <td>-0.320907</td>\n",
       "      <td>0.383561</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.273445</td>\n",
       "      <td>0.044782</td>\n",
       "      <td>0.250272</td>\n",
       "      <td>-0.114290</td>\n",
       "      <td>0.121239</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897961</td>\n",
       "      <td>1.322749</td>\n",
       "      <td>0.553525</td>\n",
       "      <td>0.079147</td>\n",
       "      <td>0.502846</td>\n",
       "      <td>-0.506932</td>\n",
       "      <td>0.223247</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025299</td>\n",
       "      <td>0.779429</td>\n",
       "      <td>0.627483</td>\n",
       "      <td>-0.354743</td>\n",
       "      <td>0.218848</td>\n",
       "      <td>-0.055096</td>\n",
       "      <td>0.297431</td>\n",
       "      <td>-0.119230</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.529123</td>\n",
       "      <td>-0.196815</td>\n",
       "      <td>0.232964</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.214278</td>\n",
       "      <td>-0.328799</td>\n",
       "      <td>-0.685732</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.237945</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.935247</td>\n",
       "      <td>0.690427</td>\n",
       "      <td>0.511019</td>\n",
       "      <td>1.063499</td>\n",
       "      <td>0.504674</td>\n",
       "      <td>-0.227777</td>\n",
       "      <td>-0.035515</td>\n",
       "      <td>-0.006277</td>\n",
       "      <td>0.174739</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.467166</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.238954</td>\n",
       "      <td>0.085494</td>\n",
       "      <td>0.207894</td>\n",
       "      <td>-0.739952</td>\n",
       "      <td>-0.371635</td>\n",
       "      <td>-0.087244</td>\n",
       "      <td>-0.217037</td>\n",
       "      <td>0.353716</td>\n",
       "      <td>0.276079</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016316</td>\n",
       "      <td>-0.238568</td>\n",
       "      <td>-0.610145</td>\n",
       "      <td>-0.418894</td>\n",
       "      <td>-0.609520</td>\n",
       "      <td>-0.072247</td>\n",
       "      <td>-0.179780</td>\n",
       "      <td>-0.085679</td>\n",
       "      <td>0.529486</td>\n",
       "      <td>0.309438</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>-0.154982</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.210837</td>\n",
       "      <td>0.166079</td>\n",
       "      <td>0.430884</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.447536</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>-0.069169</td>\n",
       "      <td>0.463648</td>\n",
       "      <td>-0.251515</td>\n",
       "      <td>0.225306</td>\n",
       "      <td>0.302280</td>\n",
       "      <td>0.252891</td>\n",
       "      <td>0.182380</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.131177</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>0.690766</td>\n",
       "      <td>-0.918174</td>\n",
       "      <td>0.273946</td>\n",
       "      <td>-0.226493</td>\n",
       "      <td>-0.467489</td>\n",
       "      <td>0.787193</td>\n",
       "      <td>-0.527138</td>\n",
       "      <td>-0.107953</td>\n",
       "      <td>0.469084</td>\n",
       "      <td>0.095217</td>\n",
       "      <td>0.443433</td>\n",
       "      <td>0.514454</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.268342</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.530629</td>\n",
       "      <td>-0.301306</td>\n",
       "      <td>0.030230</td>\n",
       "      <td>-0.376372</td>\n",
       "      <td>-0.326081</td>\n",
       "      <td>0.092524</td>\n",
       "      <td>-0.112595</td>\n",
       "      <td>-0.274940</td>\n",
       "      <td>0.107115</td>\n",
       "      <td>0.171046</td>\n",
       "      <td>-0.394139</td>\n",
       "      <td>-0.418468</td>\n",
       "      <td>-0.190215</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.219957</td>\n",
       "      <td>-0.423728</td>\n",
       "      <td>0.599078</td>\n",
       "      <td>-0.366324</td>\n",
       "      <td>0.452232</td>\n",
       "      <td>0.868501</td>\n",
       "      <td>-0.051142</td>\n",
       "      <td>0.200568</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.050063</td>\n",
       "      <td>0.305444</td>\n",
       "      <td>0.225228</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>0.332432</td>\n",
       "      <td>0.156352</td>\n",
       "      <td>0.471813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
       "0   0.827912      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "1   1.123528  0.464093      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "2   1.040624  0.605709  0.119166      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "3   0.900045  0.596818  0.359724  0.836026      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "4   1.061119  1.158660  0.633724 -0.320907  0.383561      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "5   0.071059  0.273445  0.044782  0.250272 -0.114290  0.121239      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "6   0.897961  1.322749  0.553525  0.079147  0.502846 -0.506932  0.223247      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "7   0.025299  0.779429  0.627483 -0.354743  0.218848 -0.055096  0.297431 -0.119230      -inf      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "8   0.529123 -0.196815  0.232964  0.031045  0.214278 -0.328799 -0.685732  0.060671  0.237945      -inf      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "9   0.935247  0.690427  0.511019  1.063499  0.504674 -0.227777 -0.035515 -0.006277  0.174739  0.899135      -inf      -inf      -inf      -inf      -inf      -inf\n",
       "10  0.467166  0.532110  0.238954  0.085494  0.207894 -0.739952 -0.371635 -0.087244 -0.217037  0.353716  0.276079      -inf      -inf      -inf      -inf      -inf\n",
       "11  0.016316 -0.238568 -0.610145 -0.418894 -0.609520 -0.072247 -0.179780 -0.085679  0.529486  0.309438  0.002346 -0.154982      -inf      -inf      -inf      -inf\n",
       "12  0.210837  0.166079  0.430884  0.002882  0.447536  0.287130 -0.069169  0.463648 -0.251515  0.225306  0.302280  0.252891  0.182380      -inf      -inf      -inf\n",
       "13 -0.131177  0.249064  0.690766 -0.918174  0.273946 -0.226493 -0.467489  0.787193 -0.527138 -0.107953  0.469084  0.095217  0.443433  0.514454      -inf      -inf\n",
       "14 -0.268342  0.041401  0.530629 -0.301306  0.030230 -0.376372 -0.326081  0.092524 -0.112595 -0.274940  0.107115  0.171046 -0.394139 -0.418468 -0.190215      -inf\n",
       "15 -0.219957 -0.423728  0.599078 -0.366324  0.452232  0.868501 -0.051142  0.200568  0.335000  0.050063  0.305444  0.225228 -0.176068  0.332432  0.156352  0.471813"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11，特征提取与融合阶段-多头注意力机制：掩码 Mask\n",
    "# Apply Mask to attention scores\n",
    "# 防止信息泄露（用于自注意力且输入为序列时）：在处理序列数据（如自然语言文本）的自注意力机制中，我们通常不希望一个位置的信息在计算时依赖于它后面位置的信息。例如在文本生成任务中，在生成第 i 个词时，模型只能基于前面 i-1 个词的信息，而不能提前看到后面的词。通过掩码操作可以实现这一点。\n",
    "attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf'))#[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "\n",
    "# Illustration only\n",
    "plt.imshow(attention_score[1, 1].detach().cpu().numpy(), \"Accent\", aspect=\"auto\")\n",
    "plt.title(\"Attention(Q,K)\")\n",
    "plt.xlabel(encoding.decode(x_batch[0].tolist()))\n",
    "plt.ylabel(encoding.decode(x_batch[0].tolist()))\n",
    "plt.colorbar()\n",
    "pd.DataFrame(attention_score[0][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb3f22c3466b59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:32:08.443877Z",
     "start_time": "2024-02-09T04:32:08.408750Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.659133</td>\n",
       "      <td>0.340867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.488936</td>\n",
       "      <td>0.316498</td>\n",
       "      <td>0.194566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.306845</td>\n",
       "      <td>0.226584</td>\n",
       "      <td>0.178756</td>\n",
       "      <td>0.287816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.284609</td>\n",
       "      <td>0.313769</td>\n",
       "      <td>0.185624</td>\n",
       "      <td>0.071457</td>\n",
       "      <td>0.144541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.159313</td>\n",
       "      <td>0.195051</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.190583</td>\n",
       "      <td>0.132360</td>\n",
       "      <td>0.167512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.299437</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.086341</td>\n",
       "      <td>0.131894</td>\n",
       "      <td>0.048049</td>\n",
       "      <td>0.099724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.100501</td>\n",
       "      <td>0.213640</td>\n",
       "      <td>0.183524</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>0.121962</td>\n",
       "      <td>0.092737</td>\n",
       "      <td>0.131933</td>\n",
       "      <td>0.086976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.176701</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>0.107381</td>\n",
       "      <td>0.128974</td>\n",
       "      <td>0.074929</td>\n",
       "      <td>0.052437</td>\n",
       "      <td>0.110609</td>\n",
       "      <td>0.132063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.148425</td>\n",
       "      <td>0.116194</td>\n",
       "      <td>0.097110</td>\n",
       "      <td>0.168735</td>\n",
       "      <td>0.096496</td>\n",
       "      <td>0.046389</td>\n",
       "      <td>0.056222</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.069378</td>\n",
       "      <td>0.143160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.127447</td>\n",
       "      <td>0.135999</td>\n",
       "      <td>0.101442</td>\n",
       "      <td>0.087011</td>\n",
       "      <td>0.098340</td>\n",
       "      <td>0.038114</td>\n",
       "      <td>0.055086</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>0.064296</td>\n",
       "      <td>0.113778</td>\n",
       "      <td>0.105279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.091145</td>\n",
       "      <td>0.070638</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>0.058983</td>\n",
       "      <td>0.048746</td>\n",
       "      <td>0.083420</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.082307</td>\n",
       "      <td>0.152265</td>\n",
       "      <td>0.122190</td>\n",
       "      <td>0.089881</td>\n",
       "      <td>0.076796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.075991</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.094695</td>\n",
       "      <td>0.061723</td>\n",
       "      <td>0.096286</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.057433</td>\n",
       "      <td>0.097850</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.083268</td>\n",
       "      <td>0.079255</td>\n",
       "      <td>0.073859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.051873</td>\n",
       "      <td>0.075872</td>\n",
       "      <td>0.118007</td>\n",
       "      <td>0.023613</td>\n",
       "      <td>0.077783</td>\n",
       "      <td>0.047157</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>0.129953</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.053092</td>\n",
       "      <td>0.094544</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.098932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.055033</td>\n",
       "      <td>0.075014</td>\n",
       "      <td>0.122352</td>\n",
       "      <td>0.053248</td>\n",
       "      <td>0.074181</td>\n",
       "      <td>0.049398</td>\n",
       "      <td>0.051945</td>\n",
       "      <td>0.078949</td>\n",
       "      <td>0.064308</td>\n",
       "      <td>0.054671</td>\n",
       "      <td>0.080109</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>0.048528</td>\n",
       "      <td>0.047361</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.039804</td>\n",
       "      <td>0.032466</td>\n",
       "      <td>0.090289</td>\n",
       "      <td>0.034385</td>\n",
       "      <td>0.077958</td>\n",
       "      <td>0.118207</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.069334</td>\n",
       "      <td>0.052143</td>\n",
       "      <td>0.067315</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.041590</td>\n",
       "      <td>0.069156</td>\n",
       "      <td>0.057991</td>\n",
       "      <td>0.079499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15\n",
       "0   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "1   0.659133  0.340867  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "2   0.488936  0.316498  0.194566  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "3   0.306845  0.226584  0.178756  0.287816  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "4   0.284609  0.313769  0.185624  0.071457  0.144541  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "5   0.159313  0.195051  0.155182  0.190583  0.132360  0.167512  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "6   0.195804  0.299437  0.138751  0.086341  0.131894  0.048049  0.099724  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "7   0.100501  0.213640  0.183524  0.068726  0.121962  0.092737  0.131933  0.086976  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "8   0.176701  0.085500  0.131407  0.107381  0.128974  0.074929  0.052437  0.110609  0.132063  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "9   0.148425  0.116194  0.097110  0.168735  0.096496  0.046389  0.056222  0.057891  0.069378  0.143160  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "10  0.127447  0.135999  0.101442  0.087011  0.098340  0.038114  0.055086  0.073207  0.064296  0.113778  0.105279  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "11  0.091145  0.070638  0.048715  0.058983  0.048746  0.083420  0.074915  0.082307  0.152265  0.122190  0.089881  0.076796  0.000000  0.000000  0.000000  0.000000\n",
       "12  0.075991  0.072665  0.094695  0.061723  0.096286  0.082016  0.057433  0.097850  0.047859  0.077099  0.083268  0.079255  0.073859  0.000000  0.000000  0.000000\n",
       "13  0.051873  0.075872  0.118007  0.023613  0.077783  0.047157  0.037058  0.129953  0.034912  0.053092  0.094544  0.065053  0.092150  0.098932  0.000000  0.000000\n",
       "14  0.055033  0.075014  0.122352  0.053248  0.074181  0.049398  0.051945  0.078949  0.064308  0.054671  0.080109  0.085398  0.048528  0.047361  0.059505  0.000000\n",
       "15  0.039804  0.032466  0.090289  0.034385  0.077958  0.118207  0.047124  0.060613  0.069334  0.052143  0.067315  0.062126  0.041590  0.069156  0.057991  0.079499"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12，特征提取与融合阶段-多头注意力机制：指数归一化 Softmax\n",
    "# Softmax the attention score\n",
    "# 将得分转换为概率分布：softmax 函数可以将原始的注意力得分转换为一个概率分布，使得每个位置的注意力权重都在 0 到 1 之间，且所有位置的注意力权重之和为 1。这样就可以明确地表示每个位置在当前注意力计算中的相对重要性。例如，在处理文本时，经过 softmax 后，每个词对应的注意力权重表示了该词在当前语境下对于生成特定输出（如生成下一个词或理解句子语义）的重要程度比例。\n",
    "# 便于模型学习和优化：将注意力得分转换为概率分布后，更符合模型的学习和优化过程。模型可以通过调整参数来最小化损失函数，而基于概率分布的表示使得模型能够更有效地学习到不同位置的重要性，从而更好地适应各种任务和输入数据。\n",
    "# 突出重要信息：softmax 函数具有指数运算的特性，会将较大的得分放大，较小的得分缩小。这意味着在原始的注意力得分中，相对较大的得分经过 softmax 后会变得更加突出，对应的位置会获得更高的注意力权重，从而使模型能够更关注输入中的重要信息。例如，在处理图像时，对于包含物体关键特征的区域，其对应的注意力得分经过 softmax 后会得到较高的权重，模型就能更聚焦于这些区域进行特征提取和识别。\n",
    "attention_score = torch.softmax(attention_score, dim=-1) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "pd.DataFrame(attention_score[0][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3a14ab6238d8f340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:32:19.745362Z",
     "start_time": "2024-02-09T04:32:19.729817Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n",
      "torch.Size([4, 4, 16, 16])\n",
      "torch.Size([4, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# 13，特征提取与融合阶段-多头注意力机制：计算 V 注意力\n",
    "# Calculate the V attention output\n",
    "# 结合注意力权重与值信息：通过前面的 q*k 计算得到的是注意力得分，经过 softmax 后得到的是归一化的注意力权重，它表示了输入序列中各个位置对于当前计算的重要性程度。而 V 代表了输入数据的特征表示（值向量）。将注意力权重与 V 相乘，能够根据每个位置的重要性程度对其对应的特征进行加权求和，从而得到综合考虑了注意力分布的输出表示。这样可以使模型在生成输出时，更加关注重要位置的特征信息，有效地融合了注意力机制和原始特征信息。\n",
    "# 实现信息的选择性聚合：不同的位置可能对生成输出具有不同的贡献，通过注意力权重与 V 的相乘，模型能够有选择地聚合来自不同位置的信息。那些具有较高注意力权重的位置对应的 V 中的特征会被更多地保留和传递到输出中，而注意力权重较低的位置对应的特征则会被相对弱化。这种选择性聚合能够帮助模型更好地捕捉输入数据中的关键信息，过滤掉不重要的信息，从而提高模型的性能和泛化能力。\n",
    "# 生成上下文相关的表示：在自然语言处理等任务中，例如文本生成或机器翻译，每个位置的输出都应该依赖于整个输入序列的上下文信息。通过将注意力权重与 V 相乘，可以根据当前位置与其他位置的注意力关系，动态地生成与上下文相关的表示。这样模型能够更好地理解文本中的语义关系，生成更符合上下文逻辑的输出。例如，在生成一个句子中的某个单词时，模型会根据该单词与句子中其他单词的注意力权重，结合其他单词的特征信息来生成更准确的表示，从而提高生成文本的质量。\n",
    "print(attention_score.shape) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(V.shape) #[4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]\n",
    "A = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57894d06f08e7f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:32:38.040254Z",
     "start_time": "2024-02-09T04:32:37.994213Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14，特征提取与融合阶段-多头注意力机制：连接多头的注意力\n",
    "# Concatenate the multi-head attention output\n",
    "# 多头注意力机制中，将每个头的输出结果进行连接，会把输入分别通过不同的线性层得到多个头的查询（Q）、键（K）、值（V），然后分别计算每个头的注意力输出。在经过注意力计算、加权求和等操作得到每个头的输出结果后，就会进行 “Concatenate” 操作。\n",
    "# 假设我们有 num_heads 个注意力头，每个头输出的张量形状为 [batch_size, sequence_length, head_dim]（batch_size 是批次大小，sequence_length 是序列长度，head_dim 是每个头的维度）。在连接这些头的输出时，是沿着最后一个维度（即 head_dim 所在维度）进行拼接。拼接后得到的张量形状变为 [batch_size, sequence_length, num_heads * head_dim] 。\n",
    "A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]\n",
    "A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f4fbfce6f7f330a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:33:01.895820Z",
     "start_time": "2024-02-09T04:33:01.830312Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.064506</td>\n",
       "      <td>0.451883</td>\n",
       "      <td>0.126058</td>\n",
       "      <td>-0.308508</td>\n",
       "      <td>0.429776</td>\n",
       "      <td>-0.321017</td>\n",
       "      <td>0.028085</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>-0.300572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401453</td>\n",
       "      <td>0.703710</td>\n",
       "      <td>-0.791798</td>\n",
       "      <td>0.423244</td>\n",
       "      <td>0.758770</td>\n",
       "      <td>0.434047</td>\n",
       "      <td>-0.165581</td>\n",
       "      <td>-0.510981</td>\n",
       "      <td>-0.012390</td>\n",
       "      <td>0.565687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.319784</td>\n",
       "      <td>0.270716</td>\n",
       "      <td>-0.050118</td>\n",
       "      <td>-0.361192</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>-0.242993</td>\n",
       "      <td>0.135025</td>\n",
       "      <td>0.104678</td>\n",
       "      <td>0.128337</td>\n",
       "      <td>-0.151392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253589</td>\n",
       "      <td>0.731760</td>\n",
       "      <td>-0.598175</td>\n",
       "      <td>0.584505</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>0.657069</td>\n",
       "      <td>-0.114799</td>\n",
       "      <td>-0.277092</td>\n",
       "      <td>0.109785</td>\n",
       "      <td>0.568027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142772</td>\n",
       "      <td>0.471190</td>\n",
       "      <td>-0.135125</td>\n",
       "      <td>-0.391241</td>\n",
       "      <td>0.240273</td>\n",
       "      <td>-0.426438</td>\n",
       "      <td>0.191585</td>\n",
       "      <td>0.249496</td>\n",
       "      <td>0.114119</td>\n",
       "      <td>-0.148854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188330</td>\n",
       "      <td>0.707296</td>\n",
       "      <td>-0.513143</td>\n",
       "      <td>0.675920</td>\n",
       "      <td>0.843663</td>\n",
       "      <td>0.519398</td>\n",
       "      <td>0.241321</td>\n",
       "      <td>-0.254675</td>\n",
       "      <td>-0.067157</td>\n",
       "      <td>0.583508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.112123</td>\n",
       "      <td>0.188070</td>\n",
       "      <td>-0.195295</td>\n",
       "      <td>-0.499538</td>\n",
       "      <td>0.074381</td>\n",
       "      <td>-0.364622</td>\n",
       "      <td>0.126506</td>\n",
       "      <td>0.199517</td>\n",
       "      <td>0.166654</td>\n",
       "      <td>-0.116992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050666</td>\n",
       "      <td>0.629354</td>\n",
       "      <td>-0.457674</td>\n",
       "      <td>0.544949</td>\n",
       "      <td>0.920907</td>\n",
       "      <td>0.465222</td>\n",
       "      <td>0.169629</td>\n",
       "      <td>-0.187357</td>\n",
       "      <td>0.040276</td>\n",
       "      <td>0.363001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020715</td>\n",
       "      <td>0.262572</td>\n",
       "      <td>-0.107539</td>\n",
       "      <td>-0.486144</td>\n",
       "      <td>0.098776</td>\n",
       "      <td>-0.386885</td>\n",
       "      <td>0.132087</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.070903</td>\n",
       "      <td>-0.235981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107817</td>\n",
       "      <td>0.448574</td>\n",
       "      <td>-0.550493</td>\n",
       "      <td>0.264393</td>\n",
       "      <td>0.794209</td>\n",
       "      <td>0.431990</td>\n",
       "      <td>0.282289</td>\n",
       "      <td>-0.231657</td>\n",
       "      <td>-0.063665</td>\n",
       "      <td>0.233252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.399232</td>\n",
       "      <td>-0.110658</td>\n",
       "      <td>-0.406787</td>\n",
       "      <td>0.053074</td>\n",
       "      <td>-0.316901</td>\n",
       "      <td>0.138022</td>\n",
       "      <td>0.376356</td>\n",
       "      <td>0.234606</td>\n",
       "      <td>-0.272379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189062</td>\n",
       "      <td>0.355173</td>\n",
       "      <td>-0.501581</td>\n",
       "      <td>0.171572</td>\n",
       "      <td>0.708473</td>\n",
       "      <td>0.260176</td>\n",
       "      <td>0.336404</td>\n",
       "      <td>-0.285479</td>\n",
       "      <td>-0.020590</td>\n",
       "      <td>0.198434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.124724</td>\n",
       "      <td>0.291068</td>\n",
       "      <td>-0.078329</td>\n",
       "      <td>-0.373831</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>-0.331801</td>\n",
       "      <td>0.086711</td>\n",
       "      <td>0.314733</td>\n",
       "      <td>0.143774</td>\n",
       "      <td>-0.316908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183282</td>\n",
       "      <td>0.388413</td>\n",
       "      <td>-0.533431</td>\n",
       "      <td>0.296272</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.266746</td>\n",
       "      <td>0.207132</td>\n",
       "      <td>-0.252256</td>\n",
       "      <td>-0.067646</td>\n",
       "      <td>0.238893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.095166</td>\n",
       "      <td>0.354915</td>\n",
       "      <td>-0.023955</td>\n",
       "      <td>-0.282569</td>\n",
       "      <td>0.061036</td>\n",
       "      <td>-0.429683</td>\n",
       "      <td>0.220804</td>\n",
       "      <td>0.376843</td>\n",
       "      <td>0.187381</td>\n",
       "      <td>-0.233587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265134</td>\n",
       "      <td>0.267361</td>\n",
       "      <td>-0.426334</td>\n",
       "      <td>0.250288</td>\n",
       "      <td>0.574936</td>\n",
       "      <td>0.035810</td>\n",
       "      <td>0.334888</td>\n",
       "      <td>-0.203362</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>0.152928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.214826</td>\n",
       "      <td>0.306409</td>\n",
       "      <td>-0.111830</td>\n",
       "      <td>-0.316602</td>\n",
       "      <td>0.210473</td>\n",
       "      <td>-0.285486</td>\n",
       "      <td>0.234773</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.178420</td>\n",
       "      <td>-0.231031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293020</td>\n",
       "      <td>0.345010</td>\n",
       "      <td>-0.355211</td>\n",
       "      <td>0.474156</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>0.373845</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>-0.092789</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.254176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.150454</td>\n",
       "      <td>0.300733</td>\n",
       "      <td>-0.084071</td>\n",
       "      <td>-0.347423</td>\n",
       "      <td>0.197580</td>\n",
       "      <td>-0.288124</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.199533</td>\n",
       "      <td>0.170684</td>\n",
       "      <td>-0.402754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265382</td>\n",
       "      <td>0.349139</td>\n",
       "      <td>-0.415729</td>\n",
       "      <td>0.308192</td>\n",
       "      <td>0.549358</td>\n",
       "      <td>0.390737</td>\n",
       "      <td>0.178057</td>\n",
       "      <td>-0.118175</td>\n",
       "      <td>-0.048890</td>\n",
       "      <td>0.205317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.183160</td>\n",
       "      <td>0.410611</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.289932</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.295212</td>\n",
       "      <td>0.119197</td>\n",
       "      <td>0.188429</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>-0.301170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334207</td>\n",
       "      <td>0.335139</td>\n",
       "      <td>-0.470068</td>\n",
       "      <td>0.417071</td>\n",
       "      <td>0.521629</td>\n",
       "      <td>0.421272</td>\n",
       "      <td>0.187883</td>\n",
       "      <td>-0.144798</td>\n",
       "      <td>0.070235</td>\n",
       "      <td>0.236351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.172794</td>\n",
       "      <td>0.400183</td>\n",
       "      <td>-0.066224</td>\n",
       "      <td>-0.222050</td>\n",
       "      <td>0.287892</td>\n",
       "      <td>-0.254785</td>\n",
       "      <td>0.145653</td>\n",
       "      <td>0.185306</td>\n",
       "      <td>0.057948</td>\n",
       "      <td>-0.253772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320497</td>\n",
       "      <td>0.302853</td>\n",
       "      <td>-0.354345</td>\n",
       "      <td>0.398626</td>\n",
       "      <td>0.416117</td>\n",
       "      <td>0.424110</td>\n",
       "      <td>0.158920</td>\n",
       "      <td>-0.182346</td>\n",
       "      <td>0.025160</td>\n",
       "      <td>0.289050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.138616</td>\n",
       "      <td>0.470225</td>\n",
       "      <td>-0.054341</td>\n",
       "      <td>-0.155410</td>\n",
       "      <td>0.200397</td>\n",
       "      <td>-0.344928</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>0.206519</td>\n",
       "      <td>0.046299</td>\n",
       "      <td>-0.360907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346053</td>\n",
       "      <td>0.227064</td>\n",
       "      <td>-0.358268</td>\n",
       "      <td>0.445714</td>\n",
       "      <td>0.284947</td>\n",
       "      <td>0.392696</td>\n",
       "      <td>0.249379</td>\n",
       "      <td>-0.200285</td>\n",
       "      <td>-0.064398</td>\n",
       "      <td>0.272158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.136451</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>-0.058041</td>\n",
       "      <td>-0.134002</td>\n",
       "      <td>0.139564</td>\n",
       "      <td>-0.306150</td>\n",
       "      <td>0.185001</td>\n",
       "      <td>0.164044</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>-0.261845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334006</td>\n",
       "      <td>0.066729</td>\n",
       "      <td>-0.433017</td>\n",
       "      <td>0.403648</td>\n",
       "      <td>0.305393</td>\n",
       "      <td>0.318253</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>-0.088667</td>\n",
       "      <td>-0.013552</td>\n",
       "      <td>0.174142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.107749</td>\n",
       "      <td>0.335819</td>\n",
       "      <td>-0.118002</td>\n",
       "      <td>-0.153278</td>\n",
       "      <td>0.118403</td>\n",
       "      <td>-0.280735</td>\n",
       "      <td>0.077118</td>\n",
       "      <td>0.139404</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>-0.345072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206117</td>\n",
       "      <td>0.047577</td>\n",
       "      <td>-0.357331</td>\n",
       "      <td>0.362311</td>\n",
       "      <td>0.195641</td>\n",
       "      <td>0.355531</td>\n",
       "      <td>0.281613</td>\n",
       "      <td>-0.095845</td>\n",
       "      <td>-0.178509</td>\n",
       "      <td>0.160478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.090324</td>\n",
       "      <td>0.319917</td>\n",
       "      <td>-0.137331</td>\n",
       "      <td>-0.184051</td>\n",
       "      <td>0.176805</td>\n",
       "      <td>-0.253172</td>\n",
       "      <td>0.153882</td>\n",
       "      <td>0.214823</td>\n",
       "      <td>0.040408</td>\n",
       "      <td>-0.244891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212659</td>\n",
       "      <td>0.037718</td>\n",
       "      <td>-0.324449</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.257269</td>\n",
       "      <td>0.335858</td>\n",
       "      <td>0.358707</td>\n",
       "      <td>-0.101721</td>\n",
       "      <td>-0.151703</td>\n",
       "      <td>0.225689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63\n",
       "0  -0.064506  0.451883  0.126058 -0.308508  0.429776 -0.321017  0.028085  0.494516  0.014582 -0.300572  ...  0.401453  0.703710 -0.791798  0.423244  0.758770  0.434047 -0.165581 -0.510981 -0.012390  0.565687\n",
       "1  -0.319784  0.270716 -0.050118 -0.361192  0.337563 -0.242993  0.135025  0.104678  0.128337 -0.151392  ...  0.253589  0.731760 -0.598175  0.584505  0.856153  0.657069 -0.114799 -0.277092  0.109785  0.568027\n",
       "2  -0.142772  0.471190 -0.135125 -0.391241  0.240273 -0.426438  0.191585  0.249496  0.114119 -0.148854  ...  0.188330  0.707296 -0.513143  0.675920  0.843663  0.519398  0.241321 -0.254675 -0.067157  0.583508\n",
       "3  -0.112123  0.188070 -0.195295 -0.499538  0.074381 -0.364622  0.126506  0.199517  0.166654 -0.116992  ...  0.050666  0.629354 -0.457674  0.544949  0.920907  0.465222  0.169629 -0.187357  0.040276  0.363001\n",
       "4  -0.020715  0.262572 -0.107539 -0.486144  0.098776 -0.386885  0.132087  0.430532  0.070903 -0.235981  ...  0.107817  0.448574 -0.550493  0.264393  0.794209  0.431990  0.282289 -0.231657 -0.063665  0.233252\n",
       "5   0.008326  0.399232 -0.110658 -0.406787  0.053074 -0.316901  0.138022  0.376356  0.234606 -0.272379  ...  0.189062  0.355173 -0.501581  0.171572  0.708473  0.260176  0.336404 -0.285479 -0.020590  0.198434\n",
       "6  -0.124724  0.291068 -0.078329 -0.373831  0.082521 -0.331801  0.086711  0.314733  0.143774 -0.316908  ...  0.183282  0.388413 -0.533431  0.296272  0.681265  0.266746  0.207132 -0.252256 -0.067646  0.238893\n",
       "7  -0.095166  0.354915 -0.023955 -0.282569  0.061036 -0.429683  0.220804  0.376843  0.187381 -0.233587  ...  0.265134  0.267361 -0.426334  0.250288  0.574936  0.035810  0.334888 -0.203362 -0.035591  0.152928\n",
       "8  -0.214826  0.306409 -0.111830 -0.316602  0.210473 -0.285486  0.234773  0.146622  0.178420 -0.231031  ...  0.293020  0.345010 -0.355211  0.474156  0.595328  0.373845  0.198910 -0.092789  0.023336  0.254176\n",
       "9  -0.150454  0.300733 -0.084071 -0.347423  0.197580 -0.288124  0.055312  0.199533  0.170684 -0.402754  ...  0.265382  0.349139 -0.415729  0.308192  0.549358  0.390737  0.178057 -0.118175 -0.048890  0.205317\n",
       "10 -0.183160  0.410611 -0.001230 -0.289932  0.188518 -0.295212  0.119197  0.188429  0.064878 -0.301170  ...  0.334207  0.335139 -0.470068  0.417071  0.521629  0.421272  0.187883 -0.144798  0.070235  0.236351\n",
       "11 -0.172794  0.400183 -0.066224 -0.222050  0.287892 -0.254785  0.145653  0.185306  0.057948 -0.253772  ...  0.320497  0.302853 -0.354345  0.398626  0.416117  0.424110  0.158920 -0.182346  0.025160  0.289050\n",
       "12 -0.138616  0.470225 -0.054341 -0.155410  0.200397 -0.344928  0.082469  0.206519  0.046299 -0.360907  ...  0.346053  0.227064 -0.358268  0.445714  0.284947  0.392696  0.249379 -0.200285 -0.064398  0.272158\n",
       "13 -0.136451  0.428663 -0.058041 -0.134002  0.139564 -0.306150  0.185001  0.164044  0.008917 -0.261845  ...  0.334006  0.066729 -0.433017  0.403648  0.305393  0.318253  0.263525 -0.088667 -0.013552  0.174142\n",
       "14 -0.107749  0.335819 -0.118002 -0.153278  0.118403 -0.280735  0.077118  0.139404  0.003165 -0.345072  ...  0.206117  0.047577 -0.357331  0.362311  0.195641  0.355531  0.281613 -0.095845 -0.178509  0.160478\n",
       "15 -0.090324  0.319917 -0.137331 -0.184051  0.176805 -0.253172  0.153882  0.214823  0.040408 -0.244891  ...  0.212659  0.037718 -0.324449  0.442058  0.257269  0.335858  0.358707 -0.101721 -0.151703  0.225689\n",
       "\n",
       "[16 rows x 64 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15，特征提取与融合阶段-多头注意力机制：定义多头注意力的最终输出（定义输出权重矩阵）\n",
    "# Define the output weight matrix\n",
    "# 在计算注意力机制的输出时乘以 Wo（一个线性变换矩阵）而不是直接使用 A（注意力机制的中间输出）用于后续计算，主要有以下几个原因：\n",
    "# 特征变换与融合：\n",
    "# A 的输出是经过注意力计算后得到的结果，但它可能还没有被映射到适合后续任务或模型结构的特征空间中。通过乘以 Wo，可以对 A 进行线性变换，将其特征映射到更有利于后续计算和模型学习的空间中。\n",
    "# 这种线性变换可以融合不同维度的信息。d_model 通常是模型的隐藏层维度，在注意力机制中，不同位置和不同头的信息可能具有不同的特征表示。Wo 可以将这些信息进行加权组合和变换，使得输出能够更好地捕捉到各种语义和句法关系，为后续的任务（如文本生成、分类等）提供更有价值的特征。\n",
    "# 模型灵活性与拟合能力：\n",
    "# 引入 Wo 增加了模型的参数数量和灵活性。模型可以通过调整 Wo 的参数来更好地适应不同的任务和数据特点，提高模型的拟合能力。\n",
    "# 不同的任务可能需要不同的特征表示和变换方式。Wo 可以学习到特定任务所需的特征映射，使得模型能够更准确地完成各种自然语言处理任务，如在情感分类任务中，Wo 可以将注意力输出变换为能够准确反映情感倾向的特征表示。\n",
    "# 与模型其他部分的兼容性：\n",
    "# 模型的后续部分可能对输入有特定的要求，例如特定的维度、数据分布等。乘以 Wo 可以将注意力输出调整为与后续层兼容的形式，确保模型能够顺利进行前向传播和反向传播。\n",
    "# 在 Transformer 等模型架构中，通常会有多个不同的模块和层，Wo 有助于将注意力机制的输出与其他模块（如前馈神经网络层、层归一化层等）进行无缝连接，使整个模型能够协同工作，有效地学习和处理自然语言数据。\n",
    "Wo = nn.Linear(d_model, d_model)\n",
    "output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(output.shape)\n",
    "pd.DataFrame(output[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2835a23754fafc42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:34:34.038171Z",
     "start_time": "2024-02-09T04:34:34.029896Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 16，特征提取与融合阶段-残差连接和层归一化：残差连接\n",
    "# 在 Transformer 等模型中使用残差连接（output = output + X）主要有以下几个重要原因：\n",
    "# 解决梯度消失问题：在深层神经网络中，随着网络层数的增加，梯度在反向传播过程中可能会逐渐变小，导致前面的层难以学习到有效的参数，这就是梯度消失问题。残差连接提供了一条直接的梯度传播路径，使得梯度可以更容易地从后面的层传递到前面的层，从而缓解梯度消失问题，让模型能够更有效地训练深层网络。\n",
    "# 加快训练速度：由于残差连接有助于梯度的传播，模型在训练过程中可以更快地收敛，减少训练时间和计算资源的消耗。它使得模型能够更容易地学习到恒等映射，即当模型已经学习到较好的特征表示时，残差连接可以让模型直接传递这些特征，而不需要重新学习，从而加快了训练速度。\n",
    "# 提高模型性能：通过残差连接，模型可以更好地利用输入信息，保留原始输入中的重要特征。这有助于模型学习到更复杂的函数，提高模型的表达能力和泛化能力，从而在各种自然语言处理任务中取得更好的性能，如机器翻译、文本分类、问答系统等。\n",
    "# 防止过拟合：残差连接可以在一定程度上增加模型的鲁棒性，减少过拟合的风险。因为它允许模型在训练过程中更容易地保留一些原始信息，避免模型过度拟合训练数据中的噪声和细节，使得模型能够更好地适应新的未知数据。#\n",
    "# Add residual connection\n",
    "output = output + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7761664159de5538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:34:34.636527Z",
     "start_time": "2024-02-09T04:34:34.608438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 17，特征提取与融合阶段--残差连接和层归一化：层归一化\n",
    "# Add Layer Normalization\n",
    "# 在 Transformer 等模型中，LayerNorm（层归一化）具有重要意义，主要体现在以下几个方面：\n",
    "# 加速模型收敛：在神经网络训练过程中，每层输入的分布会随着训练的进行而发生变化，这会导致模型训练难度增加，收敛速度变慢，这种现象被称为 “内部协变量偏移”。层归一化通过对每个样本的特征维度进行归一化，将其均值调整为 0，方差调整为 1，使得每层的输入分布保持相对稳定，从而减少了内部协变量偏移的影响，加快了模型的收敛速度。\n",
    "# 提高模型泛化能力：层归一化可以使模型对输入数据的变化更加鲁棒，减少模型对特定数据分布的依赖。它能够在一定程度上避免模型过拟合，提高模型在不同数据集上的泛化能力。因为归一化操作使得模型在训练过程中能够看到更广泛的数据分布，从而学习到更具通用性的特征表示。\n",
    "# 缓解梯度消失和爆炸问题：通过归一化输入数据，层归一化可以使数据的分布更加合理，避免某些特征维度的值过大或过小，从而使得梯度在反向传播过程中更加稳定，缓解了梯度消失和爆炸问题。这有助于模型更好地学习到深层的特征，提高模型的性能。\n",
    "# 允许使用更大的学习率：由于层归一化能够稳定输入数据的分布，使得模型在训练过程中对学习率的变化更加不敏感，因此可以使用更大的学习率进行训练，进一步加快模型的收敛速度，减少训练时间和计算资源的消耗。\n",
    "# 对于 d_model 维度的层归一化，它是针对模型中每个样本的 d_model 维特征向量进行操作的，确保这些特征向量在每个维度上都具有相似的分布，从而为后续的计算提供稳定、标准化的输入，有助于提高整个模型的性能和训练效率。\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output_layernorm = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ce7c56ac3de91a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:34:35.139721Z",
     "start_time": "2024-02-09T04:34:35.110122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 18，特征提取与融合阶段-前馈神经网络\n",
    "# Define Feed Forward Network\n",
    "# 在 Transformer 等模型中设置前馈神经网络（Feed Forward Network，FFN）有以下重要原因：\n",
    "# 增强模型的表达能力：\n",
    "# 前馈神经网络可以对输入进行非线性变换，能够学习到更复杂的函数关系。尽管注意力机制能够有效地捕捉文本中的长期依赖关系，但它本身主要是对输入进行加权求和等线性操作，对于一些复杂的语义和句法关系的建模能力有限。前馈神经网络通过引入非线性激活函数（如 ReLU 等），可以将注意力机制输出的特征进行进一步的加工和组合，从而增强模型对各种语言现象的表达能力，例如可以更好地处理句子中的嵌套结构、语义角色标注等复杂任务。\n",
    "# 融合局部信息：\n",
    "# 注意力机制通常关注的是全局的依赖关系，而前馈神经网络可以在局部范围内对特征进行融合和处理。它可以将每个位置的特征与周围的局部特征进行组合，捕捉到一些局部的上下文信息，这对于理解文本中的一些局部语义和语法结构非常有帮助。例如，在处理一个短语或一个句子片段时，前馈神经网络可以利用局部的词序和词汇信息来更好地理解其含义，从而与注意力机制捕捉的全局信息相互补充，使模型对文本的理解更加全面和准确。\n",
    "output = nn.Linear(d_model, d_model * 4)(output_layernorm)\n",
    "output = nn.ReLU()(output)\n",
    "output = nn.Linear(d_model * 4, d_model)(output)\n",
    "output = torch.dropout(output, p=dropout, train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35cce4d92eeb74ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:35:25.104704Z",
     "start_time": "2024-02-09T04:35:25.076394Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# 19，特征提取与融合阶段-残差连接和层归一化：添加最后一个残差连接和层规一化\n",
    "# Add residual connection & layerNorm (last time in a Transformer block)\n",
    "# 在 Transformer 等模型中，添加最后一个残差连接和层规范化有以下重要作用：\n",
    "# 残差连接的作用：\n",
    "# 信息传递与保留：残差连接能够将前一层的输入直接传递到后面的层，确保在模型的深层结构中，原始输入信息不会在传播过程中丢失或被过度变换。在经过多个层的复杂计算后，原始输入中的一些关键信息可能会被弱化，而残差连接提供了一条 “捷径”，让这些信息可以直接参与到后续的计算中，有助于模型更好地利用输入信息，提高模型的性能。\n",
    "# 缓解梯度消失和爆炸：与前面提到的残差连接作用类似，最后一个残差连接也有助于在反向传播过程中更好地传递梯度。当模型层数较多时，如果没有残差连接，梯度在传播过程中可能会逐渐消失或爆炸，导致模型难以训练。残差连接提供了直接的梯度传播路径，使得梯度能够更顺畅地从输出层传递到输入层，从而缓解了梯度消失和爆炸问题，让模型能够更有效地进行训练。\n",
    "# 层规范化的作用：\n",
    "# 数据标准化：层规范化对输入数据进行标准化处理，将其均值调整为 0，方差调整为 1。这有助于将数据分布在一个合理的范围内，避免数据的某些维度过大或过小，使得模型对不同特征维度的敏感度更加一致。在经过多个层的计算后，数据的分布可能会变得不稳定，层规范化可以将数据重新拉回到一个相对稳定的分布状态，为后续的计算提供更稳定的输入。\n",
    "# 提高模型泛化能力：通过对数据进行规范化，层规范化可以减少模型对数据分布变化的敏感性，提高模型的鲁棒性和泛化能力。它使得模型在面对不同的输入数据时，能够更加稳定地输出结果，不容易受到数据微小变化的影响。同时，层规范化也有助于防止模型过拟合，因为它可以减少数据中的噪声和异常值对模型训练的干扰，让模型学习到更具一般性的特征表示。\n",
    "# 添加最后一个残差连接和层规范化可以进一步优化模型的性能，提高模型的稳定性和泛化能力，使得模型能够更好地处理各种自然语言处理任务。\n",
    "output = output + output_layernorm\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a0de0adb842207e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:35:25.618791Z",
     "start_time": "2024-02-09T04:35:25.608096Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 20，特征提取与融合阶段：重复多头注意力机制、残差连接和层归一化、前馈神经网络\n",
    "# Until here, we finished a Transformer block,\n",
    "# We actually should pack the above Transformer block code into a call and \n",
    "# repeat the steps for num_layers times\n",
    "# but this jupyter notebook is purely for illustration purpose, so we'll skip it:\n",
    "# for _ in range(num_layers):\n",
    "#   do loop for each transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d4b434e547757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:35:26.555049Z",
     "start_time": "2024-02-09T04:35:26.440498Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>100060</th>\n",
       "      <th>100061</th>\n",
       "      <th>100062</th>\n",
       "      <th>100063</th>\n",
       "      <th>100064</th>\n",
       "      <th>100065</th>\n",
       "      <th>100066</th>\n",
       "      <th>100067</th>\n",
       "      <th>100068</th>\n",
       "      <th>100069</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.151764</td>\n",
       "      <td>-0.138867</td>\n",
       "      <td>-0.942958</td>\n",
       "      <td>-0.355769</td>\n",
       "      <td>0.120958</td>\n",
       "      <td>-0.019493</td>\n",
       "      <td>-0.985039</td>\n",
       "      <td>-0.595256</td>\n",
       "      <td>-0.274413</td>\n",
       "      <td>-0.415326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.195056</td>\n",
       "      <td>0.714029</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.845246</td>\n",
       "      <td>-0.640783</td>\n",
       "      <td>0.091546</td>\n",
       "      <td>-0.408806</td>\n",
       "      <td>-0.191840</td>\n",
       "      <td>-0.015684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.521729</td>\n",
       "      <td>-1.121775</td>\n",
       "      <td>-0.537063</td>\n",
       "      <td>-0.250971</td>\n",
       "      <td>-0.099949</td>\n",
       "      <td>-0.275567</td>\n",
       "      <td>-1.067943</td>\n",
       "      <td>0.374539</td>\n",
       "      <td>-0.539804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624783</td>\n",
       "      <td>0.048131</td>\n",
       "      <td>-0.694008</td>\n",
       "      <td>-0.895768</td>\n",
       "      <td>0.732693</td>\n",
       "      <td>-0.710512</td>\n",
       "      <td>0.545159</td>\n",
       "      <td>0.595501</td>\n",
       "      <td>-0.359785</td>\n",
       "      <td>0.808579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.163774</td>\n",
       "      <td>0.329005</td>\n",
       "      <td>-0.335850</td>\n",
       "      <td>-0.324920</td>\n",
       "      <td>-0.111924</td>\n",
       "      <td>-0.604700</td>\n",
       "      <td>0.194806</td>\n",
       "      <td>-0.727899</td>\n",
       "      <td>0.285325</td>\n",
       "      <td>-1.056179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188514</td>\n",
       "      <td>0.313076</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>-0.482292</td>\n",
       "      <td>0.652267</td>\n",
       "      <td>-0.295093</td>\n",
       "      <td>-0.194476</td>\n",
       "      <td>-0.445205</td>\n",
       "      <td>-0.628694</td>\n",
       "      <td>0.259318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059175</td>\n",
       "      <td>0.317208</td>\n",
       "      <td>0.151746</td>\n",
       "      <td>0.198112</td>\n",
       "      <td>-0.475986</td>\n",
       "      <td>0.184620</td>\n",
       "      <td>-0.923758</td>\n",
       "      <td>-0.826276</td>\n",
       "      <td>-1.389871</td>\n",
       "      <td>-1.488968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829640</td>\n",
       "      <td>0.911820</td>\n",
       "      <td>-0.049544</td>\n",
       "      <td>-0.765915</td>\n",
       "      <td>1.190440</td>\n",
       "      <td>-0.410919</td>\n",
       "      <td>0.457360</td>\n",
       "      <td>1.183785</td>\n",
       "      <td>-0.142738</td>\n",
       "      <td>0.731109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.203493</td>\n",
       "      <td>-0.297255</td>\n",
       "      <td>-0.255880</td>\n",
       "      <td>-0.881310</td>\n",
       "      <td>-0.029223</td>\n",
       "      <td>-0.673268</td>\n",
       "      <td>0.062755</td>\n",
       "      <td>-0.512842</td>\n",
       "      <td>0.194643</td>\n",
       "      <td>-0.216917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166174</td>\n",
       "      <td>0.046733</td>\n",
       "      <td>-0.812436</td>\n",
       "      <td>1.617657</td>\n",
       "      <td>0.356186</td>\n",
       "      <td>-0.563219</td>\n",
       "      <td>1.082209</td>\n",
       "      <td>-1.476452</td>\n",
       "      <td>-1.098764</td>\n",
       "      <td>0.508759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.532292</td>\n",
       "      <td>-0.151126</td>\n",
       "      <td>-1.302914</td>\n",
       "      <td>0.088134</td>\n",
       "      <td>-0.593736</td>\n",
       "      <td>-0.514214</td>\n",
       "      <td>-0.746215</td>\n",
       "      <td>-0.234466</td>\n",
       "      <td>0.356030</td>\n",
       "      <td>-0.717981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649706</td>\n",
       "      <td>0.552178</td>\n",
       "      <td>0.513879</td>\n",
       "      <td>0.171119</td>\n",
       "      <td>-0.232306</td>\n",
       "      <td>-0.202599</td>\n",
       "      <td>1.271086</td>\n",
       "      <td>-0.397143</td>\n",
       "      <td>0.241443</td>\n",
       "      <td>0.621223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.061822</td>\n",
       "      <td>-0.780232</td>\n",
       "      <td>0.033775</td>\n",
       "      <td>0.493742</td>\n",
       "      <td>-0.188903</td>\n",
       "      <td>-1.241413</td>\n",
       "      <td>-0.740801</td>\n",
       "      <td>-0.637103</td>\n",
       "      <td>-0.528444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468431</td>\n",
       "      <td>0.478140</td>\n",
       "      <td>0.426217</td>\n",
       "      <td>-0.507422</td>\n",
       "      <td>0.130201</td>\n",
       "      <td>-0.550591</td>\n",
       "      <td>0.389341</td>\n",
       "      <td>-0.341393</td>\n",
       "      <td>0.475995</td>\n",
       "      <td>-0.193076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.656042</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.133958</td>\n",
       "      <td>0.173233</td>\n",
       "      <td>-0.281520</td>\n",
       "      <td>-0.073458</td>\n",
       "      <td>-0.582138</td>\n",
       "      <td>0.420799</td>\n",
       "      <td>0.526054</td>\n",
       "      <td>0.233135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244255</td>\n",
       "      <td>0.253873</td>\n",
       "      <td>-0.222831</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.250771</td>\n",
       "      <td>-0.162268</td>\n",
       "      <td>1.170713</td>\n",
       "      <td>0.707391</td>\n",
       "      <td>0.074375</td>\n",
       "      <td>0.437575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.241402</td>\n",
       "      <td>-0.129030</td>\n",
       "      <td>-0.521567</td>\n",
       "      <td>0.174383</td>\n",
       "      <td>0.592734</td>\n",
       "      <td>-0.216198</td>\n",
       "      <td>-0.862083</td>\n",
       "      <td>0.077348</td>\n",
       "      <td>-0.410971</td>\n",
       "      <td>-0.838186</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.227130</td>\n",
       "      <td>-0.084385</td>\n",
       "      <td>0.661095</td>\n",
       "      <td>-0.146697</td>\n",
       "      <td>0.385814</td>\n",
       "      <td>-0.346445</td>\n",
       "      <td>0.140659</td>\n",
       "      <td>-0.355870</td>\n",
       "      <td>1.617523</td>\n",
       "      <td>0.017401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.148204</td>\n",
       "      <td>-0.088879</td>\n",
       "      <td>-0.644473</td>\n",
       "      <td>-0.505158</td>\n",
       "      <td>0.598318</td>\n",
       "      <td>-0.625066</td>\n",
       "      <td>-0.978184</td>\n",
       "      <td>-0.081975</td>\n",
       "      <td>-0.418040</td>\n",
       "      <td>-0.484153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025702</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.739608</td>\n",
       "      <td>0.510721</td>\n",
       "      <td>0.704796</td>\n",
       "      <td>-0.762244</td>\n",
       "      <td>0.193956</td>\n",
       "      <td>-0.841684</td>\n",
       "      <td>-0.158762</td>\n",
       "      <td>-0.145742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.042756</td>\n",
       "      <td>-0.544063</td>\n",
       "      <td>0.361202</td>\n",
       "      <td>-0.876234</td>\n",
       "      <td>0.244055</td>\n",
       "      <td>-0.727155</td>\n",
       "      <td>-0.126128</td>\n",
       "      <td>0.082008</td>\n",
       "      <td>0.921980</td>\n",
       "      <td>-0.250840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182311</td>\n",
       "      <td>0.611349</td>\n",
       "      <td>-0.155162</td>\n",
       "      <td>1.257192</td>\n",
       "      <td>-0.003148</td>\n",
       "      <td>-0.914570</td>\n",
       "      <td>0.766338</td>\n",
       "      <td>-0.837840</td>\n",
       "      <td>0.133556</td>\n",
       "      <td>-0.149257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055674</td>\n",
       "      <td>-0.363830</td>\n",
       "      <td>0.030649</td>\n",
       "      <td>-0.543012</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.537952</td>\n",
       "      <td>0.784556</td>\n",
       "      <td>-0.032552</td>\n",
       "      <td>-0.054321</td>\n",
       "      <td>-0.995863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.480730</td>\n",
       "      <td>0.305459</td>\n",
       "      <td>-0.434508</td>\n",
       "      <td>-0.684696</td>\n",
       "      <td>-0.703986</td>\n",
       "      <td>0.175240</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>-1.186731</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>0.385154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.501703</td>\n",
       "      <td>-1.228627</td>\n",
       "      <td>-1.299516</td>\n",
       "      <td>-0.611446</td>\n",
       "      <td>1.106983</td>\n",
       "      <td>-0.995603</td>\n",
       "      <td>-0.320499</td>\n",
       "      <td>-1.062071</td>\n",
       "      <td>0.479815</td>\n",
       "      <td>0.738366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332079</td>\n",
       "      <td>0.881173</td>\n",
       "      <td>0.094492</td>\n",
       "      <td>-0.284838</td>\n",
       "      <td>-1.137060</td>\n",
       "      <td>-0.699440</td>\n",
       "      <td>0.539316</td>\n",
       "      <td>-1.508222</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>-0.538308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.690691</td>\n",
       "      <td>-0.339400</td>\n",
       "      <td>-0.283244</td>\n",
       "      <td>-0.483147</td>\n",
       "      <td>0.985240</td>\n",
       "      <td>-0.703993</td>\n",
       "      <td>-0.188205</td>\n",
       "      <td>0.258862</td>\n",
       "      <td>0.471739</td>\n",
       "      <td>0.193974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219407</td>\n",
       "      <td>0.678628</td>\n",
       "      <td>-0.752596</td>\n",
       "      <td>0.156606</td>\n",
       "      <td>-0.730574</td>\n",
       "      <td>-0.291162</td>\n",
       "      <td>0.482393</td>\n",
       "      <td>-1.234015</td>\n",
       "      <td>0.199265</td>\n",
       "      <td>0.306926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.766174</td>\n",
       "      <td>-1.270677</td>\n",
       "      <td>-0.908114</td>\n",
       "      <td>-0.192428</td>\n",
       "      <td>1.097895</td>\n",
       "      <td>-0.788984</td>\n",
       "      <td>-0.176648</td>\n",
       "      <td>0.563787</td>\n",
       "      <td>0.124659</td>\n",
       "      <td>-0.347454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040482</td>\n",
       "      <td>0.429994</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>0.345786</td>\n",
       "      <td>-0.212589</td>\n",
       "      <td>-1.009258</td>\n",
       "      <td>-0.413055</td>\n",
       "      <td>-1.563507</td>\n",
       "      <td>-0.458435</td>\n",
       "      <td>-0.656041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.046377</td>\n",
       "      <td>-0.572218</td>\n",
       "      <td>-1.237437</td>\n",
       "      <td>-0.710016</td>\n",
       "      <td>0.403219</td>\n",
       "      <td>-0.400054</td>\n",
       "      <td>0.304650</td>\n",
       "      <td>-0.584162</td>\n",
       "      <td>0.465965</td>\n",
       "      <td>-0.446406</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424351</td>\n",
       "      <td>-0.253294</td>\n",
       "      <td>0.256185</td>\n",
       "      <td>-0.494801</td>\n",
       "      <td>-0.198137</td>\n",
       "      <td>-0.118498</td>\n",
       "      <td>-0.296561</td>\n",
       "      <td>-0.421304</td>\n",
       "      <td>-0.035740</td>\n",
       "      <td>-0.558273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 100070 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6         7         8         9       ...    100060    100061    100062    100063    100064    100065    100066    100067    100068    100069\n",
       "0  -0.151764 -0.138867 -0.942958 -0.355769  0.120958 -0.019493 -0.985039 -0.595256 -0.274413 -0.415326  ...  0.035935  0.195056  0.714029  0.057430  0.845246 -0.640783  0.091546 -0.408806 -0.191840 -0.015684\n",
       "1  -0.152344  0.521729 -1.121775 -0.537063 -0.250971 -0.099949 -0.275567 -1.067943  0.374539 -0.539804  ...  0.624783  0.048131 -0.694008 -0.895768  0.732693 -0.710512  0.545159  0.595501 -0.359785  0.808579\n",
       "2  -0.163774  0.329005 -0.335850 -0.324920 -0.111924 -0.604700  0.194806 -0.727899  0.285325 -1.056179  ...  0.188514  0.313076 -0.325933 -0.482292  0.652267 -0.295093 -0.194476 -0.445205 -0.628694  0.259318\n",
       "3  -0.059175  0.317208  0.151746  0.198112 -0.475986  0.184620 -0.923758 -0.826276 -1.389871 -1.488968  ... -0.829640  0.911820 -0.049544 -0.765915  1.190440 -0.410919  0.457360  1.183785 -0.142738  0.731109\n",
       "4  -0.203493 -0.297255 -0.255880 -0.881310 -0.029223 -0.673268  0.062755 -0.512842  0.194643 -0.216917  ... -0.166174  0.046733 -0.812436  1.617657  0.356186 -0.563219  1.082209 -1.476452 -1.098764  0.508759\n",
       "5  -0.532292 -0.151126 -1.302914  0.088134 -0.593736 -0.514214 -0.746215 -0.234466  0.356030 -0.717981  ... -0.649706  0.552178  0.513879  0.171119 -0.232306 -0.202599  1.271086 -0.397143  0.241443  0.621223\n",
       "6   0.061069  0.061822 -0.780232  0.033775  0.493742 -0.188903 -1.241413 -0.740801 -0.637103 -0.528444  ... -0.468431  0.478140  0.426217 -0.507422  0.130201 -0.550591  0.389341 -0.341393  0.475995 -0.193076\n",
       "7   0.656042  0.297650  0.133958  0.173233 -0.281520 -0.073458 -0.582138  0.420799  0.526054  0.233135  ...  0.244255  0.253873 -0.222831  0.014127  0.250771 -0.162268  1.170713  0.707391  0.074375  0.437575\n",
       "8   0.241402 -0.129030 -0.521567  0.174383  0.592734 -0.216198 -0.862083  0.077348 -0.410971 -0.838186  ... -1.227130 -0.084385  0.661095 -0.146697  0.385814 -0.346445  0.140659 -0.355870  1.617523  0.017401\n",
       "9   0.148204 -0.088879 -0.644473 -0.505158  0.598318 -0.625066 -0.978184 -0.081975 -0.418040 -0.484153  ...  0.025702  0.224775  0.739608  0.510721  0.704796 -0.762244  0.193956 -0.841684 -0.158762 -0.145742\n",
       "10  0.042756 -0.544063  0.361202 -0.876234  0.244055 -0.727155 -0.126128  0.082008  0.921980 -0.250840  ... -0.182311  0.611349 -0.155162  1.257192 -0.003148 -0.914570  0.766338 -0.837840  0.133556 -0.149257\n",
       "11  0.055674 -0.363830  0.030649 -0.543012 -0.004213 -0.537952  0.784556 -0.032552 -0.054321 -0.995863  ... -0.480730  0.305459 -0.434508 -0.684696 -0.703986  0.175240  0.132505 -1.186731  0.130202  0.385154\n",
       "12  0.501703 -1.228627 -1.299516 -0.611446  1.106983 -0.995603 -0.320499 -1.062071  0.479815  0.738366  ... -0.332079  0.881173  0.094492 -0.284838 -1.137060 -0.699440  0.539316 -1.508222  0.802151 -0.538308\n",
       "13  0.690691 -0.339400 -0.283244 -0.483147  0.985240 -0.703993 -0.188205  0.258862  0.471739  0.193974  ... -0.219407  0.678628 -0.752596  0.156606 -0.730574 -0.291162  0.482393 -1.234015  0.199265  0.306926\n",
       "14 -0.766174 -1.270677 -0.908114 -0.192428  1.097895 -0.788984 -0.176648  0.563787  0.124659 -0.347454  ... -0.040482  0.429994  0.560528  0.345786 -0.212589 -1.009258 -0.413055 -1.563507 -0.458435 -0.656041\n",
       "15  0.046377 -0.572218 -1.237437 -0.710016  0.403219 -0.400054  0.304650 -0.584162  0.465965 -0.446406  ...  1.424351 -0.253294  0.256185 -0.494801 -0.198137 -0.118498 -0.296561 -0.421304 -0.035740 -0.558273\n",
       "\n",
       "[16 rows x 100070 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 21，输出生成阶段：应用最后一个线性层来得到我们的 logits\n",
    "# Apply the final linear layer to get the logits\n",
    "# 在自然语言处理模型中，通过nn.Linear(d_model, max_token_value)(output)得到logits主要有以下原因：\n",
    "# 维度变换与适配：\n",
    "# d_model 通常是模型中特征向量的维度，而 max_token_value 表示词汇表的大小或可能的输出类别数量。线性变换 nn.Linear(d_model, max_token_value) 将模型输出的 d_model 维特征向量映射到 max_token_value 维的空间中，使得模型的输出维度与词汇表或输出类别的数量相匹配，为后续的分类或生成任务做准备。例如，在文本生成任务中，需要为每个可能的生成 token 计算一个得分，通过这种线性变换就可以将模型的特征表示转换为与词汇表大小一致的维度，每个维度对应一个 token 的得分。\n",
    "# 生成预测分数：\n",
    "# 得到的 logits 可以看作是对每个类别（token）的未归一化预测分数。这些分数反映了模型对于输入数据属于每个类别的可能性大小。在分类任务中，可以根据这些分数进行进一步的处理，如使用 softmax 函数将其转换为概率分布，从而得到每个类别被预测的概率。在生成任务中，也可以根据 logits 来选择生成的 token，例如选择得分最高的 token 作为生成结果，或者按照一定的概率分布进行采样来生成 token。\n",
    "# 模型训练与优化：\n",
    "# 在训练过程中，logits 是计算损失函数的重要输入。通过将模型预测的 logits 与真实标签进行比较，可以计算出损失值，如交叉熵损失等，然后利用反向传播算法来更新模型的参数，使得模型能够不断调整预测结果，以最小化损失函数，从而提高模型的准确性和性能。\n",
    "# logits 的计算是模型中非常重要的一步，它将模型的特征表示转换为可用于分类、生成和模型训练的预测分数，是连接模型内部计算和实际任务需求的关键环节。\n",
    "logits = nn.Linear(d_model, max_token_value+1)(output)\n",
    "pd.DataFrame(logits[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a857abd1cceecb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:35:46.352818Z",
     "start_time": "2024-02-09T04:35:46.256747Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>100060</th>\n",
       "      <th>100061</th>\n",
       "      <th>100062</th>\n",
       "      <th>100063</th>\n",
       "      <th>100064</th>\n",
       "      <th>100065</th>\n",
       "      <th>100066</th>\n",
       "      <th>100067</th>\n",
       "      <th>100068</th>\n",
       "      <th>100069</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 100070 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6         7         8         9       ...    100060    100061    100062    100063    100064    100065    100066    100067    100068    100069\n",
       "0   0.000007  0.000007  0.000003  0.000006  0.000010  0.000008  0.000003  0.000005  0.000006  0.000006  ...  0.000009  0.000010  0.000017  0.000009  0.000020  0.000004  0.000009  0.000006  0.000007  0.000008\n",
       "1   0.000007  0.000014  0.000003  0.000005  0.000007  0.000008  0.000006  0.000003  0.000012  0.000005  ...  0.000016  0.000009  0.000004  0.000003  0.000018  0.000004  0.000015  0.000015  0.000006  0.000019\n",
       "2   0.000007  0.000012  0.000006  0.000006  0.000008  0.000005  0.000010  0.000004  0.000011  0.000003  ...  0.000010  0.000012  0.000006  0.000005  0.000016  0.000006  0.000007  0.000005  0.000004  0.000011\n",
       "3   0.000008  0.000012  0.000010  0.000010  0.000005  0.000010  0.000003  0.000004  0.000002  0.000002  ...  0.000004  0.000021  0.000008  0.000004  0.000028  0.000006  0.000013  0.000028  0.000007  0.000018\n",
       "4   0.000007  0.000006  0.000007  0.000004  0.000008  0.000004  0.000009  0.000005  0.000010  0.000007  ...  0.000007  0.000009  0.000004  0.000043  0.000012  0.000005  0.000025  0.000002  0.000003  0.000014\n",
       "5   0.000005  0.000007  0.000002  0.000009  0.000005  0.000005  0.000004  0.000007  0.000012  0.000004  ...  0.000004  0.000015  0.000014  0.000010  0.000007  0.000007  0.000030  0.000006  0.000011  0.000016\n",
       "6   0.000009  0.000009  0.000004  0.000009  0.000014  0.000007  0.000002  0.000004  0.000004  0.000005  ...  0.000005  0.000014  0.000013  0.000005  0.000010  0.000005  0.000012  0.000006  0.000014  0.000007\n",
       "7   0.000016  0.000011  0.000010  0.000010  0.000006  0.000008  0.000005  0.000013  0.000014  0.000011  ...  0.000011  0.000011  0.000007  0.000009  0.000011  0.000007  0.000027  0.000017  0.000009  0.000013\n",
       "8   0.000011  0.000007  0.000005  0.000010  0.000015  0.000007  0.000004  0.000009  0.000006  0.000004  ...  0.000002  0.000008  0.000016  0.000007  0.000012  0.000006  0.000010  0.000006  0.000042  0.000009\n",
       "9   0.000010  0.000008  0.000004  0.000005  0.000015  0.000005  0.000003  0.000008  0.000006  0.000005  ...  0.000009  0.000011  0.000018  0.000014  0.000017  0.000004  0.000010  0.000004  0.000007  0.000007\n",
       "10  0.000009  0.000005  0.000012  0.000004  0.000011  0.000004  0.000007  0.000009  0.000021  0.000007  ...  0.000007  0.000016  0.000007  0.000030  0.000008  0.000003  0.000018  0.000004  0.000010  0.000007\n",
       "11  0.000009  0.000006  0.000009  0.000005  0.000008  0.000005  0.000019  0.000008  0.000008  0.000003  ...  0.000005  0.000011  0.000005  0.000004  0.000004  0.000010  0.000010  0.000003  0.000010  0.000012\n",
       "12  0.000014  0.000002  0.000002  0.000005  0.000026  0.000003  0.000006  0.000003  0.000014  0.000018  ...  0.000006  0.000020  0.000009  0.000006  0.000003  0.000004  0.000014  0.000002  0.000019  0.000005\n",
       "13  0.000017  0.000006  0.000006  0.000005  0.000023  0.000004  0.000007  0.000011  0.000014  0.000010  ...  0.000007  0.000017  0.000004  0.000010  0.000004  0.000006  0.000014  0.000002  0.000010  0.000012\n",
       "14  0.000004  0.000002  0.000003  0.000007  0.000025  0.000004  0.000007  0.000015  0.000010  0.000006  ...  0.000008  0.000013  0.000015  0.000012  0.000007  0.000003  0.000006  0.000002  0.000005  0.000004\n",
       "15  0.000009  0.000005  0.000002  0.000004  0.000013  0.000006  0.000011  0.000005  0.000013  0.000005  ...  0.000035  0.000007  0.000011  0.000005  0.000007  0.000008  0.000006  0.000006  0.000008  0.000005\n",
       "\n",
       "[16 rows x 100070 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 22，输出生成阶段：softmax logits 以获得每个 token 的概率\n",
    "# Get the probabilities \n",
    "# 我们在这里得到的是一个巨大的矩阵，形状为 [16， 100069]，它是整个词汇表中每个标记的概率\n",
    "# 在自然语言处理任务中，使用torch.softmax(logits, dim=-1)将logits转换为概率分布有以下几个重要原因：\n",
    "# 表示预测的不确定性：\n",
    "# 提供概率信息：通过softmax函数得到的概率分布可以明确地表示模型对每个可能结果的预测概率。例如在文本分类任务中，每个类别都有一个对应的概率值，这让我们不仅能知道模型预测的最可能类别，还能了解到其他类别的可能性大小，从而更全面地评估模型的预测结果，量化模型对于不同输出的置信程度。\n",
    "# 处理多类别问题：在许多自然语言处理任务中，输出往往是多类别分类或多选项生成问题。softmax函数将logits转换为一个概率分布，使得所有可能的类别概率之和为 1，这样就可以方便地处理多个类别之间的关系，模型可以根据这个概率分布来做出决策，例如选择概率最高的类别作为最终输出，或者按照概率进行采样来生成文本等。\n",
    "# 便于计算损失函数：\n",
    "# 在模型训练过程中，通常使用基于概率的损失函数，如交叉熵损失。将logits转换为概率分布后，可以很方便地与真实标签的概率分布（通常是独热编码形式）进行比较，计算出损失值，以此来衡量模型预测结果与真实结果之间的差异，进而通过反向传播算法更新模型参数，使模型的预测结果逐渐接近真实结果，优化模型的性能。\n",
    "# 符合概率模型的假设：\n",
    "#许多自然语言处理模型在理论上是基于概率模型构建的，将输出表示为概率分布符合这些模型的数学基础和假设。例如，在语言生成任务中，我们希望模型生成的文本是基于一定的概率分布，符合语言的自然规律，即某些词语或句子在特定语境下出现的概率更高，通过softmax得到的概率分布可以更好地模拟这种自然语言的概率特性。\n",
    "# torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss\n",
    "# but for illustration purpose, we'll use torch.softmax here\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "pd.DataFrame(probabilities[0].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5de9e0455613f790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:36:08.064535Z",
     "start_time": "2024-02-09T04:36:08.045119Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Lane'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the predicted token and it's original English word\n",
    "predicted_index = torch.argmax(logits[0,0]).item()\n",
    "encoding.decode([predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "329ff8f3529b7309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:39:04.969636Z",
     "start_time": "2024-02-09T04:39:04.925038Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the sales process. The ability to effectively communicate the value and benefits of our products'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the original input sentence\n",
    "encoding.decode(x_batch[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "111989a98903f4dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks like the predicted token \"Catholics\" is not the correct prediction token to the original sentence, because we only did one training loop and barely trained nothing.\n",
    "# But this is the basic idea of how the Transformer works.\n",
    "# We'll continue to train the model in the next notebook and wrap the above code into a class.\n",
    "# https://waylandzhang.github.io/en/let-s-code-llm.html\n",
    "# 在实践中，多个 transformer 块将堆叠在一起以执行一个解码交易。在训练期间，输出 Token 将与 Ground Truth Token 进行比较以计算损失。然后对超参数中定义的 max_iters 时间重复该过程。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
